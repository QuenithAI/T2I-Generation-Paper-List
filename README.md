<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Text-to-Image Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Text-to-Image Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (应达学术)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of Text-to-Image generation.
>
> Your contributions are also vital—feel free to [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> 本仓库由 **「应达学术」(QuenithAI)** 的一线科研导师团队倾力打造并持续维护，旨在为您呈现文生图领域最全面、最前沿的论文。
>
> 您的贡献对我们和社区来说至关重要——我们诚邀有志之士通过 [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) 或 [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) 来成为这个项目的合作者之一，期待您的加入！
> 
> 如果您在冲刺科研顶会的道路上需要专业的1V1指导，欢迎**通过[微信](assets/wechat.jpg)或[邮件](mailto:christzhaung@gmail.com)联系我们**。


<details>
<summary><strong>⚡ Latest Updates</strong></summary>

- **(Sep 21th, 2025)**: Add a new direction: [🔄 Unified Generation and Understanding](#-unified-generation-and-understanding).
- **(Aug 21th, 2025)**: Add a new direction: [🎨 Personalized Image Generation](#personalized).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

## <span id="contents">📚 Table of Contents</span>
- [📚 Table of Contents](#-table-of-contents)
- [📜 Papers \& Models](#-papers--models)
  - [✍️ Survey Papers](#️-survey-papers)
  - [🖼️ Text-to-Image Generation](#️-text-to-image-generation)
  - [🕹️ Conditional Image Generation](#️-conditional-image-generation)
  - [🎨 Personalized Image Generation](#-personalized-image-generation)
  - [✂️ Image Editing](#️-image-editing)
  - [🔄 Unified Generation and Understanding](#-unified-generation-and-understanding)
- [🗂️ Datasets](#️-datasets)
- [🎓 About Us](#-about-us)
- [🤝 Contributing](#-contributing)

---

## <span id="papers">📜 Papers & Models</span>

### <span id="survey">✍️ Survey Papers</span>



[<small>⇧ Back to ToC</small>](#contents)

### <span id="t2i">🖼️ Text-to-Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>


<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***PreciseCam:*** *Precise Camera Control for Text-to-Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.12910)
  [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://graphics.unizar.es/projects/PreciseCam2024/)
  [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/edurnebernal/PreciseCam)
  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/edurnebb/PreciseCam)

* **[CVPR 2025]** ***Type‑R:*** *Automatically Retouching Typos for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18159) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CyberAgentAILab/Type-R) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cyberagent/type-r)

* **[CVPR 2025]** ***Compass Control:*** *Multi Object Orientation Control for Text‑to‑Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.06752) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/CompassControl/)

* **[CVPR 2025]** ***Generative Photography:*** *Scene‑Consistent Camera Control for Realistic Text‑to‑Image Synthesis*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://generative-photography.github.io/project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pandayuanyu/generative-photography) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/pandaphd/generative_photography)

* **[CVPR 2025]** ***One‑Way Ticket:*** *Time‑Independent Unified Encoder for Distilling Text‑to‑Image Diffusion Models*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/32579) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/Loopfree) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/senmaonk/loopfree-sd2.1-base)

* **[CVPR 2025]** ***Text Embedding is Not All You Need:*** *Attention Control for Text‑to‑Image Semantic Alignment with Text Self‑Attention Maps*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.15236) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://t-sam-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/t-sam-diffusion/code)

* **[CVPR 2025]** ***Towards Uncertainty:*** *Understanding and Quantifying Uncertainty for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03178) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ENSTA-U2IS-AI/Uncertainty_diffusion)

* **[CVPR 2025]** ***Responsible Diffusion:*** *Plug‑and‑Play Interpretable Responsible Text‑to‑Image Generation via Dual‑Space Multi‑faceted Concept Control*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.18324) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://basim-azam.github.io/responsiblediffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/basim-azam/responsiblediffusion)

* **[CVPR 2025]** ***Make It Count:*** *Text‑to‑Image Generation with an Accurate Number of Objects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.10210) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://make-it-count-paper.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Litalby1/make-it-count)

* **[CVPR 2025]** ***MCCD:*** *Multi‑Agent Collaboration‑based Compositional Diffusion for Complex Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2505.02648)

* **[CVPR 2025]** ***Debias‑SD:*** *Rethinking Training for De‑biasing Text‑to‑Image Generation: Unlocking the Potential of Stable Diffusion*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.12692)

* **[CVPR 2025]** ***ShapeWords:*** *Guiding Text‑to‑Image Synthesis with 3D Shape‑Aware Prompts*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02912) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lodurality.github.io/shapewords/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lodurality/shapewords_paper_code) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/dmpetrov/shapewords)

* **[CVPR 2025]** ***SnapGen:*** *Taming High‑Resolution Text‑to‑Image Models for Mobile Devices with Efficient Architectures and Training*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.09619) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapgen/)

* **[CVPR 2025]** ***STORM:*** *Spatial Transport Optimization by Repositioning Attention Map for Training‑Free Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.22168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://micv-yonsei.github.io/storm2025/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MICV-yonsei/STORM)

* **[CVPR 2025]** ***Focus‑N‑Fix:*** *Region‑Aware Fine‑Tuning for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.06481)

* **[CVPR 2025]** ***SILMM:*** *Self‑Improving Large Multimodal Models for Compositional Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05818) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silmm.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/SILMM)

* **[CVPR 2025]** ***GLoCE:*** *Localized Concept Erasure for Text‑to‑Image Diffusion Models Using Training‑Free Gated Low‑Rank Adaptation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.12356) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gl-oce.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/GLoCE)

* **[CVPR 2025]** ***Self‑Cross Guidance:*** *Self‑Cross Diffusion Guidance for Text‑to‑Image Synthesis of Similar Subjects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.18936) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://selfcross-guidance.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mengtang-lab/selfcross-guidance)

* **[CVPR 2025]** ***Noise Diffusion:*** *Enhancing Semantic Faithfulness in Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bomingmiao/NoiseDiffusion)

* **[CVPR 2025]** ***PromptSampler:*** *Learning to Sample Effective and Diverse Prompts for Text‑to‑Image Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.07838) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dbsxodud-11/PAG)

* **[CVPR 2025]** ***STEREO:*** *A Two‑Stage Framework for Adversarially Robust Concept Erasing from Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.16807) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/koushiksrivats/robust-concept-erasing)

* **[CVPR 2025]** ***MinorityPrompt:*** *Minority‑Focused Text‑to‑Image Generation via Prompt Optimization*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/soobin-um/MinorityPrompt)

* **[CVPR 2025]** ***DistillT5:*** *Scaling Down Text Encoders of Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19897) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LifuWang-66/DistillT5)

* **[CVPR 2025]** ***TIU:*** *The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text‑to‑Image Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NGK2110/TIU)

* **[CVPR 2025]** ***Fuse‑DiT:*** *Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text‑to‑Image Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tang-bd/fuse-dit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ooutlierr/fuse-dit)

* **[CVPR 2025]** **Detect‑and‑Guide:** *Self‑regulation of Diffusion Models for Safe Text‑to‑Image Generation via Guideline Token Optimization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.15197)

* **[CVPR 2025]** **Multi‑Group T2I:** *Multi‑Group Proportional Representations for Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sangwon-jung94/mpr-t2i)

* **[CVPR 2025]** **VODiff:** *Controlling Object Visibility Order in Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dliang293/VODiff)

* **[CVPR 2025]** *Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Large-Scale_Text-to-Image_Model_with_Inpainting_is_a_Zero-Shot_Subject-Driven_Image_CVPR_2025_paper.html) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://diptychprompting.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chaehunshin/DiptychPrompting) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta)

* **[CVPR 2025]** *Six‑CD: Benchmarking Concept Removals for Text-to-image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Artanisax/Six-CD)

* **[CVPR 2025]** *ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ConceptGuard_Continual_Personalized_Text-to-Image_Generation_with_Forgetting_and_Confusion_Mitigation_CVPR_2025_paper.html)

* **[CVPR 2025]** *ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Jia_ChatGen_Automatic_Text-to-Image_Generation_From_FreeStyle_Chatting_CVPR_2025_paper.html) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengyou-jia.github.io/ChatGen-Home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengyou-jia/ChatGen) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ChengyouJia/ChatGen-Base-8B)

* **[CVPR 2025]** **CoSER:** *Towards Consistent Dense Multiview Text‑to‑Image Generator for 3D Creation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation_CVPR_2025_paper.html)

* **[CVPR 2025]** **PQPP:** *A Joint Benchmark for Text‑to‑Image Prompt and Query Performance Prediction*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Poesina_PQPP_A_Joint_Benchmark_for_Text-to-Image_Prompt_and_Query_Performance_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Eduard6421/PQPP)

* **[CVPR 2025]** **STEPS:** *Sequential Probability Tensor Estimation for Text‑to‑Image Hard Prompt Search*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEPS_Sequential_Probability_Tensor_Estimation_for_Text-to-Image_Hard_Prompt_Search_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ynqiu/STEPS) 

* **[CVPR 2025]** **ACE:** *Anti‑Editing Concept Erasure in Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_ACE_Anti-Editing_Concept_Erasure_in_Text-to-Image_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/120L020904/ACE)

* **[ICLR 2025]** **Improving Long‑Text Alignment:** *Improving Long‑Text Alignment for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2ZK8zyIt7o) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/luping-liu/LongAlign)

* **[ICLR 2025]** **ITTA:** *Information Theoretic Text‑to‑Image Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Ugs2W5XFFo) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Chao0511/mitune)

* **[ICLR 2025]** **Meissonic:** *Revitalizing Masked Generative Transformers for Efficient High‑Resolution Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=GJsuYHhAga) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/meissonic/home) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/viiika/Meissonic) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/MeissonFlow/Meissonic)

* **[ICLR 2025]** **PaRa:** *Personalizing Text‑to‑Image Diffusion via Parameter Rank Reduction*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=KZgo2YQbhc)

* **[ICLR 2025]** **Prompt‑Pruning:** *Not All Prompts Are Made Equal – Prompt‑based Pruning of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=3BhZCfJ73Y) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rezashkv/diffusion_pruning) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rezashkv/diffusion_pruning)

* **[ICLR 2025]** **Denoising AR Transformers:** *Denoising Autoregressive Transformers for Scalable Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=amDkNPVWcn)

* **[ICLR 2025]** **Progressive Compositionality:** *Progressive Compositionality in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S85PP4xjFD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evansh666.github.io/EvoGen_Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/evansh666/EvoGen)

* **[ICLR 2025]** **Classifier Scores:** *Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hUdLs6TqZL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://srvcodes.github.io/continual_personalization)

* **[ICLR 2025]** **Engagement:** *Measuring and Improving Engagement of Text‑to‑Image Generation Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=TmCcNuo03f) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://behavior-in-the-wild.github.io/image-engagement)

* **[ICLR 2025]** **Residual Gate Eraser:** *Concept Pinpoint Eraser for Text‑to-image Diffusion Models via Residual Attention Gate*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZRDhBwKs7l) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/CPE)

* **[ICLR 2025]** **Random Seeds:** *Enhancing Compositional Text‑to‑Image Generation with Reliable Random Seeds*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=5BSlakturs) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/doub7e/Reliable-Random-Seeds)

* **[ICLR 2025]** **One‑Prompt‑One‑Story:** *Free‑Lunch Consistent Text‑to‑Image Generation Using a Single Prompt*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=cD1kl2QKv1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byliutao.github.io/1Prompt1Story.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/byliutao/1Prompt1Story)

* **[ICLR 2025]** **You Only Sample Once:** *Taming One‑Step Text‑to‑Image Synthesis by Self‑Cooperative Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=T7bmHkwzS6) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yoso-t2i.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Luo-Yihong/YOSO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Luo-Yihong/yoso_pixart512)

* **[ICLR 2025]** **Copyright Revisiting:** *Rethinking Artistic Copyright Infringements in the Era of Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=0OTVNEm9N4)

* **[ICLR 2025]** **Concept Combination Erasing:** *Erasing Concept Combination from Text‑to‑Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=OBjF5I4PWg)

* **[ICLR 2025]** **Cross‑Attention Patterns:** *Cross‑Attention Head Position Patterns Can Align with Human Visual Concepts in Text‑to‑Image Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=1vggIT5vvj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SNU-DRL/HRV)

* **[ICLR 2025]** **TIGeR:** *Unifying Text‑to‑Image Generation and Retrieval with Large Multimodal Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mr2icR6dpD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/TIGeR)

* **[ICLR 2025]** **DGQ:** *Distribution‑Aware Group Quantization for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZyNEr7Xw5L) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ugonfor/DGQ)

* **[ICLR 2025]** **Jacobi Decoding:** *Accelerating Auto‑regressive Text‑to‑Image Generation with Training‑free Speculative Jacobi Decoding*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LZfjxvqw0N) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD)

* **[ICLR 2025]** **PT‑T2I/V:** *An Efficient Proxy‑Tokenized Diffusion Transformer for Text‑to‑Image/Video Task*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=lTrrnNdkOX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://360cvgroup.github.io/Qihoo-T2X/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/360CVGroup/Qihoo-T2X)

* **[ICLR 2025]** **Gecko Evaluation:** *Revisiting Text‑to‑Image Evaluation with Gecko: on Metrics, Prompts, and Human Rating*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Im2neAMlre) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google-deepmind/gecko_benchmark_t2i)

* **[ICLR 2025]** **SANA:** *Efficient High‑Resolution Text‑to‑Image Synthesis with Linear Diffusion Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=N8Oj1XhtYZ) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/Sana) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NVlabs/Sana) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_diffusers)

* **[ICLR 2025]** **Rectified Flow:** *Text‑to‑Image Rectified Flow as Plug‑and‑Play Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=SzPZK856iI) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yangxiaofeng/rectified_flow_prior)

* **[ICLR 2025]** **Human Feedback Filtering:** *Automated Filtering of Human Feedback Data for Aligning Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=8jvVNPHtVJ)

* **[ICLR 2025]** **SAFREE:** *Training‑Free and Adaptive Guard for Safe Text‑to‑Image and Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hgTFotBRKl) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://safree-safe-t2i-t2v.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jaehong31/SAFREE)

* **[ICLR 2025]** **IterComp:** *Iterative Composition‑Aware Feedback Learning from Model Gallery for Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=4w99NAikOE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangLing0818/IterComp) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/comin/IterComp)

* **[ICLR 2025]** **ScImage:** *How good are multimodal large language models at scientific text‑to‑image generation?*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ugyqNEOjoU) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/casszhao/ScImage)

* **[ICLR 2025]** **Score Distillation:** *Guided Score Identity Distillation for Data‑Free One‑Step Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=HMVDiaWMwM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mingyuanzhou/SiD-LSG) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/UT-Austin-PML/SiD-LSG)

* **[ICLR 2025]** **Causal Variation:** *Evaluating Semantic Variation in Text‑to‑Image Synthesis: A Causal Perspective*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=NWb128pSCb) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhuxiangru/SemVarBench)

* **[NeurIPS 2025]** ***LABridge:*** *A Text–Image Latent Alignment Framework via OU Diffusion Bridge*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119312)

* **[NeurIPS 2025]** *Compositional Discrete Latent Code for High-Fidelity, Productive Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/120203) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lavoiems/DiscreteLatentCode) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/lavoies/DLC_512x256)

* **[NeurIPS 2025]** ***STARFlow:*** *Scaling Latent Normalizing Flows for High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/120027)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>


- [Plot'n Polish: Zero‑shot Story Visualization and Disentangled Editing with Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2509.04446v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://plotnpolish.github.io)
- [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](http://arxiv.org/abs/2509.04548v1) [![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/UniPic?style=social)](https://github.com/SkyworkAI/UniPic) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://unipic-v2.github.io/)
- [PromptEnhancer: A Simple Approach to Enhance Text‑to‑Image Models via Chain‑of‑Thought Prompt Rewriting](http://arxiv.org/abs/2509.04545v1)
- [From Editor to Dense Geometry Estimator](http://arxiv.org/abs/2509.04338v1)
- [Noisy Label Refinement with Semantically Reliable Synthetic Images](http://arxiv.org/abs/2509.04298v1)
- [MEPG:Multi‑Expert Planning and Generation for Compositionally‑Rich Image Generation](http://arxiv.org/abs/2509.04126v1)
- [Easier Painting Than Thinking: Can Text‑to‑Image Models Set the Stage, but Not Direct the Play?](http://arxiv.org/abs/2509.03516v1)
- [Fidelity‑preserving enhancement of ptychography with foundational text‑to‑image models](http://arxiv.org/abs/2509.04513v1)
- [Exploring Diffusion Models for Generative Forecasting of Financial Charts](http://arxiv.org/abs/2509.02308v1)
- [Data‑Driven Loss Functions for Inference‑Time Optimization in Text‑to‑Image Generation](http://arxiv.org/abs/2509.02295v1)
- [Palette Aligned Image Diffusion](http://arxiv.org/abs/2509.02000v1)
- [Draw‑In‑Mind: Learning Precise Image Editing via Chain‑of‑Thought Imagination](http://arxiv.org/abs/2509.01986v1) [![GitHub Stars](https://img.shields.io/github/stars/showlab/DIM?style=social)](https://github.com/showlab/DIM)
- [Discrete Noise Inversion for Next‑scale Autoregressive Text‑based Image Editing](http://arxiv.org/abs/2509.01984v2)
- [Q‑Sched: Pushing the Boundaries of Few‑Step Diffusion Models with Quantization‑Aware Scheduling](http://arxiv.org/abs/2509.01624v1)
- [RealMat: Realistic Materials with Diffusion and Reinforcement Learning](http://arxiv.org/abs/2509.01134v1)
- [CompSlider: Compositional Slider for Disentangled Multiple‑Attribute Image Generation](http://arxiv.org/abs/2509.01028v2)
- [Prompting Away Stereotypes? Evaluating Bias in Text‑to‑Image Models for Occupations](http://arxiv.org/abs/2509.00849v1)
- [Multi‑Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification](http://arxiv.org/abs/2509.00752v1)
- [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text‑to‑Image Generation](http://arxiv.org/abs/2509.00642v1)
- [AMCR: A Framework for Assessing and Mitigating Copyright Risks in Generative Models](http://arxiv.org/abs/2509.00641v1)
- [Reusing Computation in Text‑to‑Image Diffusion for Efficient Generation of Image Sets](http://arxiv.org/abs/2508.21032v1)
- [Understanding and evaluating computer vision models through the lens of counterfactuals](http://arxiv.org/abs/2508.20881v1)
- [Pref‑GRPO: Pairwise Preference Reward‑based GRPO for Stable Text‑to‑Image Reinforcement Learning](http://arxiv.org/abs/2508.20751v1)
- [Persode: Personalized Visual Journaling with Episodic Memory‑Aware AI Agent](http://arxiv.org/abs/2508.20585v1)
- [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](http://arxiv.org/abs/2508.20505v1)
- [Safe‑Control: A Safety Patch for Mitigating Unsafe Content in Text‑to‑Image Generation Models](http://arxiv.org/abs/2508.21099v1)
- [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text‑to‑Image Models](http://arxiv.org/abs/2508.19791v1)
- [MonoRelief V2: Leveraging Real Data for High‑Fidelity Monocular Relief Recovery](http://arxiv.org/abs/2508.19555v1)
- [All‑in‑One Slider for Attribute Manipulation in Diffusion Models](http://arxiv.org/abs/2508.19195v1) 
- [Visual‑CoG: Stage‑Aware Reinforcement Learning with Chain of Guidance for Text‑to‑Image Generation](http://arxiv.org/abs/2508.18032v2)
- [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text‑to‑Image Generation](http://arxiv.org/abs/2508.17760v1)
- [Instant Preference Alignment for Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.17718v1)
- [T2I‑ReasonBench: Benchmarking Reasoning‑Informed Text‑to‑Image Generation](http://arxiv.org/abs/2508.17472v1)
- [Bias Amplification in Stable Diffusion's Representation of Stigma Through Skin Tones and Their Homogeneity](http://arxiv.org/abs/2508.17465v1)
- [An LLM‑LVLM Driven Agent for Iterative and Fine‑Grained Image Editing](http://arxiv.org/abs/2508.17435v1)
- [HiCache: Training‑free Acceleration of Diffusion Models via Hermite Polynomial‑based Feature Caching](http://arxiv.org/abs/2508.16984v1)
- [Delta‑SVD: Efficient Compression for Personalized Text‑to‑Image Models](http://arxiv.org/abs/2508.16863v1)
- [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely‑Controllable Synthetic Data](http://arxiv.org/abs/2508.16783v1)
- [A Framework for Benchmarking Fairness‑Utility Trade‑offs in Text‑to‑Image Models via Pareto Frontiers](http://arxiv.org/abs/2508.16752v1)
- [A‑FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](http://arxiv.org/abs/2509.00036v1)
- [UniEM‑3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](http://arxiv.org/abs/2508.16239v1)
- [RAGSR: Regional Attention Guided Diffusion for Image Super‑Resolution](http://arxiv.org/abs/2508.16158v1)
- [Scaling Group Inference for Diverse and High‑Quality Generation](http://arxiv.org/abs/2508.15773v1)
- [Waver: Wave Your Way to Lifelike Video Generation](http://arxiv.org/abs/2508.15761v2)
- [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](http://arxiv.org/abs/2508.15227v1)
- [Side Effects of Erasing Concepts from Diffusion Models](http://arxiv.org/abs/2508.15124v2)
- [CurveFlow: Curvature‑Guided Flow Matching for Image Generation](http://arxiv.org/abs/2508.15093v2)
- [SATURN: Autoregressive Image Generation Guided by Scene Graphs](http://arxiv.org/abs/2508.14502v1)
- [MUSE: Multi‑Subject Unified Synthesis via Explicit Layout Semantic Expansion](http://arxiv.org/abs/2508.14440v1)
- [CTA‑Flux: Integrating Chinese Cultural Semantics into High‑Quality English Text‑to‑Image Communities](http://arxiv.org/abs/2508.14405v1)
- [Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation](http://arxiv.org/abs/2508.18235v1)
- [Inference Time Debiasing Concepts in Diffusion Models](http://arxiv.org/abs/2508.14933v1)
- [Pixels Under Pressure: Exploring Fine‑Tuning Paradigms for Foundation Models in High‑Resolution Medical Imaging](http://arxiv.org/abs/2508.14931v1)
- [SAGA: Learning Signal‑Aligned Distributions for Improved Text‑to‑Image Generation](http://arxiv.org/abs/2508.13866v1)
- [UniECS: Unified Multimodal E‑Commerce Search Framework with Gated Cross‑modal Fusion](http://arxiv.org/abs/2508.13843v1)
- [DiffIER: Optimizing Diffusion Models with Iterative Error Reduction](http://arxiv.org/abs/2508.13628v2)
- [7Bench: a Comprehensive Benchmark for Layout‑guided Text‑to‑image Models](http://arxiv.org/abs/2508.12919v1)
- [S²‑Guidance: Stochastic Self Guidance for Training‑Free Enhancement of Diffusion Models](http://arxiv.org/abs/2508.12880v1)
- [Single‑Reference Text‑to‑Image Manipulation with Dual Contrastive Denoising Score](http://arxiv.org/abs/2508.12718v1)
- [DeCoT: Decomposing Complex Instructions for Enhanced Text‑to‑Image Generation with Large Language Models](http://arxiv.org/abs/2508.12396v1)
- [Navigating the Exploration‑Exploitation Tradeoff in Inference‑Time Scaling of Diffusion Models](http://arxiv.org/abs/2508.12361v1)
- [SafeCtrl: Region‑Based Safety Control for Text‑to‑Image Diffusion via Detect‑Then‑Suppress](http://arxiv.org/abs/2508.11904v1)
- [LoRAtorio: An intrinsic approach to LoRA Skill Composition](http://arxiv.org/abs/2508.11624v1)
- [SPG: Style‑Prompting Guidance for Style‑Specific Content Creation](http://arxiv.org/abs/2508.11476v1)
- [Match & Choose: Model Selection Framework for Fine‑tuning Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.10993v1)
- [NextStep‑1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](http://arxiv.org/abs/2508.10711v2)
- [CountCluster: Training‑Free Object Quantity Guidance with Cross‑Attention Map Clustering for Text‑to‑Image Generation](http://arxiv.org/abs/2508.10710v1)
- [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](http://arxiv.org/abs/2508.10424v1)
- [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.10407v2)
- [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](http://arxiv.org/abs/2508.10280v1)
- [Echo‑4o: Harnessing the Power of GPT‑4o Synthetic Images for Improved Image Generation](http://arxiv.org/abs/2508.09987v1)
- [WeDesign: Generative AI‑Facilitated Community Consultations for Urban Public Space Design](http://arxiv.org/abs/2508.19256v1)
- [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](http://arxiv.org/abs/2508.09598v1)
- [Dual Recursive Feedback on Generation and Appearance Latents for Pose‑Robust Text‑to‑Image Diffusion](http://arxiv.org/abs/2508.09575v1)
- [Understanding Dementia Speech Alignment with Diffusion‑Based Image Generation](http://arxiv.org/abs/2508.09385v1)
- [Per‑Query Visual Concept Learning](http://arxiv.org/abs/2508.09045v1)
- [TARA: Token‑Aware LoRA for Composable Personalization in Diffusion Models](http://arxiv.org/abs/2508.08812v1)
- [Exploring Palette based Color Guidance in Diffusion Models](http://arxiv.org/abs/2508.08754v1)
- [SafeFix: Targeted Model Repair via Controlled Image Generation](http://arxiv.org/abs/2508.08701v1)
- [CLUE: Leveraging Low‑Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](http://arxiv.org/abs/2508.07413v1)
- [CoAR: Concept Injection into Autoregressive Models for Personalized Text‑to‑Image Generation](http://arxiv.org/abs/2508.07341v1)
- [Multi‑task Adversarial Attacks against Black‑box Model with Few‑shot Queries](http://arxiv.org/abs/2508.10039v1)
- [Explainability‑in‑Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](http://arxiv.org/abs/2508.07183v1)
- [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](http://arxiv.org/abs/2508.07031v1)
- [HiMat: DiT‑based Ultra‑High Resolution SVBRDF Generation](http://arxiv.org/abs/2508.07011v2)
- [CannyEdit: Selective Canny Control and Dual‑Prompt Guidance for Training‑Free Image Editing](http://arxiv.org/abs/2508.06937v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vaynexie.github.io/CannyEdit)
- [AR‑GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](http://arxiv.org/abs/2508.06924v1)
- [Talk2Image: A Multi‑Agent System for Multi‑Turn Image Generation and Editing](http://arxiv.org/abs/2508.06916v1)
- [Towards Effective Prompt Stealing Attack against Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2508.06837v1)
- [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](http://arxiv.org/abs/2508.06715v1)
- [VISTAR: A User‑Centric and Role‑Driven Benchmark for Text‑to‑Image Evaluation](http://arxiv.org/abs/2508.06152v1)
- [NEP: Autoregressive Image Editing via Next Editing Token Prediction](http://arxiv.org/abs/2508.06044v1)
- [Learning 3D Texture‑Aware Representations for Parsing Diverse Human Clothing and Body Parts](http://arxiv.org/abs/2508.06032v1)
- [UnGuide: Learning to Forget with LoRA‑Guided Diffusion Models](http://arxiv.org/abs/2508.05755v1)
- [Whose Truth? Pluralistic Geo‑Alignment for (Agentic) AI](http://arxiv.org/abs/2508.05432v1)
- [UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text‑to‑Image Generation](http://arxiv.org/abs/2508.05399v1)
- [Textual Inversion for Efficient Adaptation of Open‑Vocabulary Object Detectors Without Forgetting](http://arxiv.org/abs/2508.05323v1)
- [ACM Multimedia Grand Challenge on ENT Endoscopy Analysis](http://arxiv.org/abs/2508.04801v1)



</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***DistriFusion:*** *Distributed Parallel Inference for High-Resolution Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.19481.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mit-han-lab/distrifuser)

* **[CVPR 2024]** ***InstanceDiffusion:*** *Instance-level Control for Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.03290.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/frank-xwang/InstanceDiffusion)

* **[CVPR 2024]** ***ECLIPSE:*** *A Resource-Efficient Text-to-Image Prior for Image Generations*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04655.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/eclipse-t2i/eclipse-inference) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://eclipse-t2i.vercel.app/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2)

* **[CVPR 2024]** ***Instruct-Imagen:*** *Image Generation with Multi-modal Instruction*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.01952.pdf)

* **[CVPR 2024]** ***Continuous 3D Words:*** *Learning Continuous 3D Words for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.08654.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ttchengab/continuous_3d_words_code/)

* **[CVPR 2024]** ***Rich Human Feedback:*** *Rich Human Feedback for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10240.pdf)

* **[CVPR 2024]** ***MarkovGen:*** *Structured Prediction for Efficient Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10997.pdf)

* **[CVPR 2024]** ***Customization Assistant:*** *Customization Assistant for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.03045.pdf)

* **[CVPR 2024]** ***ADI:*** *Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.15841.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adi-t2i.github.io/ADI/)

* **[CVPR 2024]** ***UFOGen:*** *You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.09257.pdf)

* **[CVPR 2024]** ***Interpret Diffusion:*** *Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17216.pdf)

* **[CVPR 2024]** ***Tailored Visions:*** *Enhancing Text-to-Image Generation with Personalized Prompt Rewriting*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.08129.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zzjchen/Tailored-Visions)

* **[CVPR 2024]** ***CoDi:*** *Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.01407.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fast-codi.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fast-codi/CoDi) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/MKFMIKU/CoDi)

* **[CVPR 2024]** ***Arbitrary‑Scale Diffusion:*** *Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.10255.pdf)

* **[CVPR 2024]** ***Human-Centric Priors:*** *Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05239)

* **[CVPR 2024]** ***ElasticDiffusion:*** *Training-free Arbitrary Size Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18822) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://elasticdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MoayedHajiAli/ElasticDiffusion-official)

* **[CVPR 2024]** ***CosmicMan:*** *A Text-to-Image Foundation Model for Humans*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01294) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cosmicman-cvpr2024.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cosmicman-cvpr2024/CosmicMan)

* **[CVPR 2024]** ***PanFusion:*** *Taming Stable Diffusion for Text to 360° Panorama Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07949) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengzhag.github.io/publication/panfusion) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengzhag/PanFusion)

* **[CVPR 2024]** ***Intelligent Grimm:*** *Open-ended Visual Storytelling via Latent Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.00973) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://haoningwu3639.github.io/StoryGen_Webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/haoningwu3639/StoryGen)

* **[CVPR 2024]** ***Scalability:*** *On the Scalability of Diffusion-based Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02883)

* **[CVPR 2024]** ***MuLAn:*** *A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02790) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mulan-dataset.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/mulan-dataset/v1.0)

* **[CVPR 2024]** ***Multi-dimensional Preferences:*** *Learning Multi-dimensional Human Preference for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.14705)

* **[CVPR 2024]** ***Dynamic Prompts:*** *Dynamic Prompt Optimizing for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.04095)

* **[CVPR 2024]** ***Reinforcement Diversification:*** *Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***HypercGAN:*** *Adversarial Text to Continuous Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kilichbek.github.io/webpage/hypercgan/)

* **[CVPR 2024]** ***EmoGen:*** *Emotional Image Content Generation with Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/JingyuanYY/EmoGen)

* **[CVPR 2024]** ***Fake‑Inversion:*** *Learning to Detect Images from Unseen Text‑to‑Image Models by Inverting Stable Diffusion*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01023) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fake-inversion.github.io)

* **[CVPR 2024]** ***ViewDiff:*** *3D‑Consistent Image Generation with Text‑to‑Image Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00482) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lukashoel.github.io/ViewDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/facebookresearch/ViewDiff) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[CVPR 2024]** ***LeftRefill:*** *Filling Right Canvas based on Left Reference through Generalized Text‑to‑Image Diffusion Model*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00736) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ewrfcas.github.io/LeftRefill/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ewrfcas/LeftRefill)

* **[CVPR 2024]** ***OpenBias:*** *Open‑Set Bias Detection in Text‑to‑Image Generative Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01162) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/OpenBias)

* **[CVPR 2024]** ***InitNO:*** *Boosting Text‑to‑Image Diffusion Models via Initial Noise Optimization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00896) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://xiefan-guo.github.io/initno/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/xiefan-guo/initno)


* **[CVPR 2024]** ***DreamMatcher:*** *Appearance Matching Self‑Attention for Semantically‑Consistent Text‑to‑Image Personalization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00774) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cvlab-kaist.github.io/DreamMatcher/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cvlab-kaist/DreamMatcher)

* **[CVPR 2024]** ***HanDiffuser:*** *Text‑to‑Image Generation with Realistic Hand Appearances*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00239) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://supreethn.github.io/research/handiffuser/)


* **[CVPR 2024]** ***Discriminative Probing and Tuning:*** *for Text‑to‑Image Generation (DPT‑T2I)*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00710) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dpt-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/DPT-T2I) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/leigangqu/DPT-T2I)


* **[CVPR 2024]** ***DiffAgent:*** *Fast and Accurate Text‑to‑Image API Selection with Large Language Model*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00611) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OpenGVLab/DiffAgent)

* **[CVPR 2024]** ***FlashEval:*** *Towards Fast and Accurate Evaluation of Text‑to‑Image Diffusion Generative Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01526) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thu-nics.github.io/FlashEval/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-nics/FlashEval) 


* **[ECCV 2024]** ***LaVi‑Bridge:*** *Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.07860) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/LaVi-Bridge/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/LaVi-Bridge)

* **[ECCV 2024]** ***DiffPNG:*** *Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.05352v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nini0919/DiffPNG)

* **[ECCV 2024]** ***SPRIGHT:*** *Getting it Right: Improving Spatial Consistency in Text-to-Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01197) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://spright-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SPRIGHT-T2I/SPRIGHT)

* **[ECCV 2024]** ***IndicTTI:*** *Navigating Text-to-Image Generative Bias across Indic Languages*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.00283v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://iab-rubric.org/resources/other-databases/indictti)

* **[ECCV 2024]** ***Safeguard T2I:*** *Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.21032)

* **[ECCV 2024]** ***Reality-and-Fantasy:*** *The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12579) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leo81005.github.io/Reality-and-Fantasy/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://leo81005.github.io/Reality-and-Fantasy/)

* **[ECCV 2024]** ***RECE:*** *Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12383v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CharlesGong12/RECE)

* **[ECCV 2024]** ***StyleTokenizer:*** *Defining Image Style by a Single Instance for Controlling Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2409.02543) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alipay/style-tokenizer)

* **[ECCV 2024]** ***PEA-Diffusion:*** *Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08492.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/PEA-Diffusion)

* **[ECCV 2024]** ***Skewed Relations T2I:*** *Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11936.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zdxdsw/skewed_relations_T2I)

* **[ECCV 2024]** ***Parrot:*** *Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05562.pdf)

* **[ECCV 2024]** ***MobileDiffusion:*** *Instant Text-to-Image Generation on Mobile Devices*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07923.pdf)

* **[ECCV 2024]** ***PixArt-Σ:*** *Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04692) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/PixArt-sigma-project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-sigma)

* **[ECCV 2024]** ***CogView3:*** *Finer and Faster Text-to-Image Generation via Relay Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05121) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogView)

* **[ICLR 2024]** ***Patched Diffusion Models:*** *Patched Denoising Diffusion Models For High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.01316.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mlpc-ucsd/patch-dm)

* **[ICLR 2024]** ***Relay Diffusion:*** *Unifying diffusion process across resolutions for image synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.03350.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/RelayDiffusion)

* **[ICLR 2024]** ***SDXL:*** *Improving Latent Diffusion Models for High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.01952.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Stability-AI/generative-models)

* **[ICLR 2024]** ***Compose and Conquer:*** *Diffusion-Based 3D Depth Aware Composable Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.09048.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tomtom1103/compose-and-conquer)

* **[ICLR 2024]** ***PixArt-α:*** *Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.00426.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-alpha) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha)

* **[ICLR 2024]** ***Image Content Suppression:*** *Get What You Want, Not What You Don’t: Image Content Suppression for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=zpVPhvVKXk) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](N/A) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/SuppressEOT) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](N/A)

* **[ICLR 2024]** *Confidence‑aware Reward Optimization for Fine‑tuning Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=Let8OMe20n) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/kykim0/TextNorm)

* **[ICLR 2024]** ***Würstchen:*** *An Efficient Architecture for Large‑Scale Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=gU58d5QeGv) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dome272/Wuerstchen) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/warp-ai/wuerstchen)


* **[SIGGRAPH 2024]** ***RGB↔X:*** *Image Decomposition and Synthesis Using Material- and Lighting-aware Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://zheng95z.github.io/assets/files/sig24-rgbx.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zheng95z.github.io/publications/rgbx24)

* **[AAAI 2024]** ***Semantic-aware Augmentation:*** *Semantic-aware Data Augmentation for Text-to-image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07951.pdf)

* **[AAAI 2024]** ***Abstract Concepts:*** *Text-to-Image Generation for Abstract Concepts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28122)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Text-to-Image GAN with Pretrained Representations](http://arxiv.org/abs/2501.00116v1)
- [VMix: Improving Text-to-Image Diffusion Model with Cross-Attention Mixing Control](http://arxiv.org/abs/2412.20800v1) [![GitHub Stars](https://img.shields.io/github/stars/fenfenfenfan/VMix?style=social)](https://github.com/fenfenfenfan/VMix)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vmix-diffusion.github.io/VMix/)
- [INFELM: In-depth Fairness Evaluation of Large Text-To-Image Models](http://arxiv.org/abs/2501.01973v3)
- [Is Your Text-to-Image Model Robust to Caption Noise?](http://arxiv.org/abs/2412.19531v1)
- [DebiasDiff: Debiasing Text-to-image Diffusion Models with Self-discovering Latent Attribute Directions](http://arxiv.org/abs/2412.18810v1) [![GitHub Stars](https://img.shields.io/github/stars/leigest519/DebiasDiff?style=social)](https://github.com/leigest519/DebiasDiff)
- [Explaining in Diffusion: Explaining a Classifier Through Hierarchical Semantics with Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.18604v1)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://explain-in-diffusion.github.io/)
- [FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models](http://arxiv.org/abs/2412.18302v1)
- [EvalMuse-40K: A Reliable and Fine-Grained Benchmark with Comprehensive Human Annotations for Text-to-Image Generation Model Evaluation](http://arxiv.org/abs/2412.18150v2)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://shh-han.github.io/EvalMuse-project/)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/DY-Evalab/EvalMuse)
- [AEIOU: A Unified Defense Framework against NSFW Prompts in Text-to-Image Models](http://arxiv.org/abs/2412.18123v1)
- [Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation](http://arxiv.org/abs/2412.16906v2)
- [PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.16257v2)
- [GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation](http://arxiv.org/abs/2412.16227v1)
- [What makes a good metric? Evaluating automatic metrics for text-to-image consistency](http://arxiv.org/abs/2412.13989v1)
- [Maybe you are looking for CroQS: Cross-modal Query Suggestion for Text-to-Image Retrieval](http://arxiv.org/abs/2412.13834v1)
- [CoMPaSS: Enhancing Spatial Understanding in Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.13195v2) [![GitHub Stars](https://img.shields.io/github/stars/blurgyy/CoMPaSS?style=social)](https://github.com/blurgyy/CoMPaSS)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/blurgy/CoMPaSS-FLUX.1)
- [ArtAug: Enhancing Text-to-Image Generation through Synthesis-Understanding Interaction](http://arxiv.org/abs/2412.12888v2)  [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
- [A Framework for Critical Evaluation of Text-to-Image Models: Integrating Art Historical Analysis, Artistic Exploration, and Critical Prompt Engineering](http://arxiv.org/abs/2412.12774v1)
- [Efficient Scaling of Diffusion Transformers for Text-to-Image Generation](http://arxiv.org/abs/2412.12391v1)
- [VersaGen: Unleashing Versatile Visual Control for Text-to-Image Synthesis](http://arxiv.org/abs/2412.11594v3) [![GitHub Stars](https://img.shields.io/github/stars/FelixChan9527/VersaGen_official?style=social)](https://github.com/FelixChan9527/VersaGen_official)
- [Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization](http://arxiv.org/abs/2412.12212v1)
- [AlignGuard: Scalable Safety Alignment for Text-to-Image Generation](http://arxiv.org/abs/2412.10493v2) [![GitHub Stars](https://img.shields.io/github/stars/Visualignment/SafetyDPO?style=social)](https://github.com/Visualignment/SafetyDPO)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://alignguard.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Visualignment/safe-stable-diffusion-v1-5)
- [SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training](http://arxiv.org/abs/2412.09619v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/snapgen/)
- [Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG](http://arxiv.org/abs/2412.09614v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://context-canvas.github.io/)
- [DECOR:Decomposition and Projection of Text Embeddings for Text-to-Image Customization](http://arxiv.org/abs/2412.09169v1)
- [Fast Prompt Alignment for Text-to-Image Generation](http://arxiv.org/abs/2412.08639v1) [![GitHub Stars](https://img.shields.io/github/stars/tiktok/fast_prompt_alignment?style=social)](https://github.com/tiktok/fast_prompt_alignment)
- [FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.07674v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://fiva-dataset.github.io/)
- [Preference Adaptive and Sequential Text-to-Image Generation](http://arxiv.org/abs/2412.10419v2)
- [Boosting Alignment for Post-Unlearning Text-to-Image Generative Models](http://arxiv.org/abs/2412.07808v2) [![GitHub Stars](https://img.shields.io/github/stars/reds-lab/Restricted_gradient_diversity_unlearning?style=social)](https://github.com/reds-lab/Restricted_gradient_diversity_unlearning)
- [Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty](http://arxiv.org/abs/2412.06771v2) [![GitHub Stars](https://img.shields.io/github/stars/google-deepmind/proactive_t2i_agents?style=social)](https://github.com/google-deepmind/proactive_t2i_agents)
- [SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation](http://arxiv.org/abs/2412.05818v2) [![GitHub Stars](https://img.shields.io/github/stars/LgQu/SILMM?style=social)](https://github.com/LgQu/SILMM)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://silmm.github.io/)
- [Evaluating Hallucination in Text-to-Image Diffusion Models with Scene-Graph based Question-Answering Agent](http://arxiv.org/abs/2412.05722v1)
- [SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models](http://arxiv.org/abs/2412.04852v2) [![GitHub Stars](https://img.shields.io/github/stars/taco-group/SleeperMark?style=social)](https://github.com/taco-group/SleeperMark)
- [LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors](http://arxiv.org/abs/2412.04460v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://layerfusion.github.io/)
- [T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts](http://arxiv.org/abs/2412.04300v3)
- [BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation](http://arxiv.org/abs/2412.04086v2)
- [Safeguarding Text-to-Image Generation via Inference-Time Prompt-Noise Optimization](http://arxiv.org/abs/2412.03876v1)
- [DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation](http://arxiv.org/abs/2412.03255v2) [![GitHub Stars](https://img.shields.io/github/stars/hithqd/DynamicControl?style=social)](https://github.com/hithqd/DynamicControl)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hithqd.github.io/projects/Dynamiccontrol/)
- [The Role of Text-to-Image Models in Advanced Style Transfer Applications: A Case Study with DALL-E 3](http://arxiv.org/abs/2412.05325v1)
- [Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation](http://arxiv.org/abs/2412.03178v1)
- [ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts](http://arxiv.org/abs/2412.02912v1)
- [ScImage: How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?](http://arxiv.org/abs/2412.02368v1)
- [Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models](http://arxiv.org/abs/2412.02237v3)
- [Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis](http://arxiv.org/abs/2412.02168v3)
- [Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis](http://arxiv.org/abs/2412.01819v4)
- [Continuous Concepts Removal in Text-to-image Diffusion Models](http://arxiv.org/abs/2412.00580v2)
- [Blind Inverse Problem Solving Made Easy by Text-to-Image Latent Diffusion](http://arxiv.org/abs/2412.00557v1)
- [Safety Alignment Backfires: Preventing the Re-emergence of Suppressed Concepts in Fine-tuned Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.00357v1)
- [Sparrow: Data-Efficient Video-LLM with Text-to-Image Augmentation](http://arxiv.org/abs/2411.19951v5)
- [QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain](http://arxiv.org/abs/2411.19534v1)
- [DreamBlend: Advancing Personalized Fine-tuning of Text-to-Image Diffusion Models](http://arxiv.org/abs/2411.19390v1)
- [EFSA: Episodic Few-Shot Adaptation for Text-to-Image Retrieval](http://arxiv.org/abs/2412.00139v2)
- [Bridging the Gap: Aligning Text-to-Image Diffusion Models with Specific Feedback](http://arxiv.org/abs/2412.00122v1)
- [Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar Subjects](http://arxiv.org/abs/2411.18936v2)
- [All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds](http://arxiv.org/abs/2411.18810v5)
- [An indicator for effectiveness of text-to-image guardrails utilizing the Single-Turn Crescendo Attack (STCA)](http://arxiv.org/abs/2411.18699v1)
- [Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation](http://arxiv.org/abs/2411.18301v1)
- [Type-R: Automatically Retouching Typos for Text-to-Image Generation](http://arxiv.org/abs/2411.18159v2)
- [Reward Incremental Learning in Text-to-Image Generation](http://arxiv.org/abs/2411.17310v1)
- [ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting](http://arxiv.org/abs/2411.17176v1)
- [Relations, Negations, and Numbers: Looking for Logic in Generative Text-to-Image Models](http://arxiv.org/abs/2411.17066v1)
- [Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis](http://arxiv.org/abs/2411.16503v1)
- [Unlocking the Potential of Text-to-Image Diffusion with PAC-Bayesian Theory](http://arxiv.org/abs/2411.17472v1)
- [CoCoNO: Attention Contrast-and-Complete for Initial Noise Optimization in Text-to-Image Synthesis](http://arxiv.org/abs/2411.16783v1)
- [Text-to-Image Synthesis: A Decade Survey](http://arxiv.org/abs/2411.16164v1)
- [In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models](http://arxiv.org/abs/2411.16769v2)


</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GigaGAN:*** *Scaling Up GANs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mingukkang.github.io/GigaGAN/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/gigagan-pytorch)

* **[CVPR 2023]** ***ERNIE-ViLG 2.0:*** *Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***Shifted Diffusion:*** *Shifted Diffusion for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/drboog/Shifted_Diffusion)

* **[CVPR 2023]** ***GALIP:*** *Generative Adversarial CLIPs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tobran/GALIP)

* **[CVPR 2023]** ***Specialist Diffusion:*** *Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)

* **[CVPR 2023]** ***Verifiable Evaluation:*** *Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***RIATIG:*** *Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WUSTL-CSPL/RIATIG)

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion)

* **[ICCV 2023]** ***DiffFit:*** *Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/DiffFit-pytorch)

* **[ICCV 2023]** *Inspecting the Geographical Representativeness of Images from Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00474)

* **[ICCV 2023]** *Expressive Text‑to‑Image Generation with Rich Text*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00694) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://songweig.github.io/rich-text-to-image/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/songweige/rich-text-to-image) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/songweig/rich-text-to-image)

* **[ICCV 2023]** *Text‑Conditioned Sampling Framework for Text‑to‑Image Generation with Masked Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02125) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hello3196.github.io/TCTS_FAS/) 

* **[ICCV 2023]** ***DALL‑EVAL:*** *Probing the Reasoning Skills and Social Biases of Text‑to‑Image Generation Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00283) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/j-min/DallEval) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/j-min/PaintSkills)

* **[ICCV 2023]** ***HRS‑Bench:*** *Holistic, Reliable and Scalable Benchmark for Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.01834) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://eslambakr.github.io/hrsbench.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/eslambakr/hrsbench.github.io)

* **[ICCV 2023]** *Ablating Concepts in Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02074) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~concept-ablation/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nupurkmr9/concept-ablation) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/nupurkmr9/concept-ablation)

* **[ICCV 2023]** *Editing Implicit Assumptions in Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00649) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://time-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bahjat-kawar/time-diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/bahjat-kawar/time-diffusion)

* **[ICCV 2023]** *Discriminative Class Tokens for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02077) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://vesteinn.github.io/discriminative_class_tokens/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/idansc/discriminative_class_tokens)

* **[ICCV 2023]** *Harnessing the Spatial‑Temporal Attention of Diffusion Models for High‑Fidelity Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00714) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn)

* **[ICCV 2023]** ***Human Preference Score:*** *Better Aligning Text‑to‑Image Models with Human Preference*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00200) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tgxs002.github.io/align_sd_web/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tgxs002/align_sd) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/xswu/align_sd)

* **[ICCV 2023]** *Unleashing Text‑to‑Image Diffusion Models for Visual Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.00527) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://vpd.ivg-research.xyz/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wl-zhao/VPD)

* **[ICCV 2023]** ***MagicFusion:*** *Boosting Text‑to‑Image Generation Performance by Fusing Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02065) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://magicfusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MagicFusion/MagicFusion.github.io)

* **[NeurIPS 2023]** ***ImageReward:*** *Learning and Evaluating Human Preferences for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=JVzeOYEx6d) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/ImageReward)

* **[NeurIPS 2023]** ***RAPHAEL:*** *Text-to-Image Generation via Large Mixture of Diffusion Paths*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.18295) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://raphael-painter.github.io/)

* **[NeurIPS 2023]** ***Linguistic Binding:*** *Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=AOKU4nRw1W) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RoyiRa/Linguistic-Binding-in-Diffusion-Models)

* **[NeurIPS 2023]** ***DenseDiffusion:*** *Dense Text-to-Image Generation with Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/naver-ai/densediffusion)

* **[ICLR 2023]** ***Structured Diffusion Guidance:*** *Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=PUIqjT4rzq7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/weixi-feng/Structured-Diffusion-Guidance)

* **[ICML 2023]** ***StyleGAN-T:*** *Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/stylegan-t/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/autonomousvision/stylegan-t)

* **[ICML 2023]** ***Muse:*** *Text-To-Image Generation via Masked Generative Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/chang23b/chang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://muse-icml.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/muse-maskgit-pytorch)

* **[ICML 2023]** ***UniDiffusers:*** *One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2303.06555) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/unidiffuser)

* **[ACM MM 2023]** ***SUR-adapter:*** *Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.05189.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qrange-group/SUR-adapter)

* **[ACM MM 2023]** ***ControlStyle:*** *Text-Driven Stylized Image Generation Using Diffusion Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.05463.pdf)

* **[SIGGRAPH 2023]** ***Attend-and-Excite:*** *Attention-Based Semantic Guidance for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2301.13826.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuval-alaluf.github.io/Attend-and-Excite/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuval-alaluf/Attend-and-Excite) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite)



</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [LaCon: Late-Constraint Diffusion for Steerable Guided Image Synthesis](https://arxiv.org/abs/2305.11520) [![GitHub Stars](https://img.shields.io/github/stars/AlonzoLeeeooo/LCDG?style=social)]((https://github.com/AlonzoLeeeooo/LCDG))   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/AlonzoLeeeooo/LaCon)
- [RenAIssance: A Survey into AI Text‑to‑Image Generation in the Era of Large Model](http://arxiv.org/abs/2309.00810v1)
- [Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text‑to‑Image Generative Models](http://arxiv.org/abs/2308.15692v2)
- [Dense Text‑to‑Image Generation with Attention Modulation](http://arxiv.org/abs/2308.12964v1) [![GitHub Stars](https://img.shields.io/github/stars/naver-ai/DenseDiffusion?style=social)](https://github.com/naver-ai/DenseDiffusion)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/naver-ai/DenseDiffusion)
- [AltDiffusion: A Multilingual Text‑to‑Image Diffusion Model](http://arxiv.org/abs/2308.09991v2) [![GitHub Stars](https://img.shields.io/github/stars/superhero-7/AltDiffusion?style=social)](https://github.com/superhero-7/AltDiffusion)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/BAAI/AltDiffusion-m18)
- [Noisy‑Correspondence Learning for Text‑to‑Image Person Re‑identification](http://arxiv.org/abs/2308.09911v3)
- [Likelihood‑Based Text‑to‑Image Evaluation with Patch‑Level Perceptual and Semantic Credit Assignment](http://arxiv.org/abs/2308.08525v1)
- [Learning to Generate Semantic Layouts for Higher Text‑Image Correspondence in Text‑to‑Image Synthesis](http://arxiv.org/abs/2308.08157v1)
- [MarkovGen: Structured Prediction for Efficient Text‑to‑Image Generation](http://arxiv.org/abs/2308.10997v3)
- [IP‑Adapter: Text Compatible Image Prompt Adapter for Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2308.06721v1) [![GitHub Stars](https://img.shields.io/github/stars/tencent-ailab/IP-Adapter?style=social)](https://github.com/tencent-ailab/IP-Adapter)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ip-adapter.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/h94/IP-Adapter)
- [Masked‑Attention Diffusion Guidance for Spatially Controlling Text‑to‑Image Generation](http://arxiv.org/abs/2308.06027v2) [![GitHub Stars](https://img.shields.io/github/stars/endo-yuki-t/MAG?style=social)](https://github.com/endo-yuki-t/MAG)
- [PromptPaint: Steering Text‑to‑Image Generation Through Paint Medium‑like Interactions](http://arxiv.org/abs/2308.05184v1)
- [LayoutLLM‑T2I: Eliciting Layout Guidance from LLM for Text‑to‑Image Generation](http://arxiv.org/abs/2308.05095v2) [![GitHub Stars](https://img.shields.io/github/stars/LayoutLLM-T2I/LayoutLLM-T2I?style=social)](https://github.com/LayoutLLM-T2I/LayoutLLM-T2I)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://layoutllm-t2i.github.io/)
- [Circumventing Concept Erasure Methods for Text‑to‑Image Generative Models](http://arxiv.org/abs/2308.01508v2) [![GitHub Stars](https://img.shields.io/github/stars/NYU-DICE-Lab/circumventing-concept-erasure?style=social)](https://github.com/NYU-DICE-Lab/circumventing-concept-erasure)
- [The Bias Amplification Paradox in Text‑to‑Image Generation](http://arxiv.org/abs/2308.00755v2)
- [BAGM: A Backdoor Attack for Manipulating Text‑to‑Image Generative Models](http://arxiv.org/abs/2307.16489v2) [![GitHub Stars](https://img.shields.io/github/stars/JJ-Vice/BAGM?style=social)](https://github.com/JJ-Vice/BAGM)
- [Subject‑Diffusion: Open Domain Personalized Text‑to‑Image Generation without Test‑time Fine‑tuning](http://arxiv.org/abs/2307.11410v2) [![GitHub Stars](https://img.shields.io/github/stars/OPPO-Mente-Lab/Subject-Diffusion?style=social)](https://github.com/OPPO-Mente-Lab/Subject-Diffusion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://oppo-mente-lab.github.io/subject_diffusion/)
- [BoxDiff: Text‑to‑Image Synthesis with Training‑Free Box‑Constrained Diffusion](http://arxiv.org/abs/2307.10816v4) [![GitHub Stars](https://img.shields.io/github/stars/showlab/BoxDiff?style=social)](https://github.com/showlab/BoxDiff)
- [Beyond the ML Model: Applying Safety Engineering Frameworks to Text‑to‑Image Development](http://arxiv.org/abs/2307.10312v1)
- [Distilling Knowledge from Text‑to‑Image Generative Models Improves Visio‑Linguistic Reasoning in CLIP](http://arxiv.org/abs/2307.09233v3)
- [Text‑guided Image Restoration and Semantic Enhancement for Text‑to‑Image Person Retrieval](http://arxiv.org/abs/2307.09059v4)
- [PromptMagician: Interactive Prompt Engineering for Text‑to‑Image Creation](http://arxiv.org/abs/2307.09036v2)
- [PromptCrafter: Crafting Text‑to‑Image Prompt through Mixed‑Initiative Dialogue with LLM](http://arxiv.org/abs/2307.08985v1)
- [Image Captions are Natural Prompts for Text‑to‑Image Models](http://arxiv.org/abs/2307.08526v2)
- [Analysing Gender Bias in Text‑to‑Image Models using Object Detection](http://arxiv.org/abs/2307.08025v1)
- [Can Pre‑Trained Text‑to‑Image Models Generate Visual Goals for Reinforcement Learning?](http://arxiv.org/abs/2307.07837v1)
- [Fast Adaptation with Bradley‑Terry Preference Models in Text‑To‑Image Classification and Generation](http://arxiv.org/abs/2308.07929v2)
- [HyperDreamBooth: HyperNetworks for Fast Personalization of Text‑to‑Image Models](http://arxiv.org/abs/2307.06949v2) [![GitHub Stars](https://img.shields.io/github/stars/JiauZhang/hyperdreambooth?style=social)](https://github.com/JiauZhang/hyperdreambooth)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hyperdreambooth.github.io/)
- [Domain‑Agnostic Tuning‑Encoder for Fast Personalization of Text‑To‑Image Models](http://arxiv.org/abs/2307.06925v1)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://datencoder.github.io/)
- [T2I‑CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text‑to‑image Generation](http://arxiv.org/abs/2307.06350v3) [![GitHub Stars](https://img.shields.io/github/stars/Karine-Huang/T2I-CompBench?style=social)](https://github.com/Karine-Huang/T2I-CompBench)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://karine-h.github.io/T2I-CompBench/)
- [Towards Safe Self‑Distillation of Internet‑Scale Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2307.05977v1)
- [TIAM – A Metric for Evaluating Alignment in Text‑to‑Image Generation](http://arxiv.org/abs/2307.05134v2) [![GitHub Stars](https://img.shields.io/github/stars/CEA-LIST/TIAMv2?style=social)](https://github.com/CEA-LIST/TIAMv2)
- [Articulated 3D Head Avatar Generation using Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2307.04859v1)
- [Divide, Evaluate, and Refine: Evaluating and Improving Text‑to‑Image Alignment with Iterative VQA Feedback](http://arxiv.org/abs/2307.04749v2) [![GitHub Stars](https://img.shields.io/github/stars/1jsingh/Divide-Evaluate-and-Refine?style=social)](https://github.com/1jsingh/Divide-Evaluate-and-Refine)
- [AnimateDiff: Animate Your Personalized Text‑to‑Image Diffusion Models without Specific Tuning](http://arxiv.org/abs/2307.04725v2) [![GitHub Stars](https://img.shields.io/github/stars/guoyww/AnimateDiff?style=social)](https://github.com/guoyww/AnimateDiff)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://animatediff.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/guoyww/animatediff)
- [Augmenters at SemEval‑2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero‑Shot Visual WSD through Prompt Augmentation and Text‑To‑Image Diffusion](http://arxiv.org/abs/2307.05564v1)
- [Typology of Risks of Generative Text‑to‑Image Models](http://arxiv.org/abs/2307.05543v1)
- [DIAGNOSIS: Detecting Unauthorized Data Usages in Text‑to‑image Diffusion Models](http://arxiv.org/abs/2307.03108v3)
- [On the Cultural Gap in Text‑to‑Image Generation](http://arxiv.org/abs/2307.02971v1)
- [Counting Guidance for High Fidelity Text‑to‑Image Synthesis](http://arxiv.org/abs/2306.17567v3) [![GitHub Stars](https://img.shields.io/github/stars/furiosa-ai/counting-guidance?style=social)](https://github.com/furiosa-ai/counting-guidance)
- [CLIPAG: Towards Generator‑Free Text‑to‑Image Generation](http://arxiv.org/abs/2306.16805v2) [![GitHub Stars](https://img.shields.io/github/stars/royg27/CLIPAG?style=social)](https://github.com/royg27/CLIPAG)
- [Localized Text‑to‑Image Generation for Free via Cross Attention Control](http://arxiv.org/abs/2306.14636v1)
- [A‑STAR: Test‑time Attention Segregation and Retention for Text‑to‑image Synthesis](http://arxiv.org/abs/2306.14544v1)
- [Text‑Anchored Score Composition: Tackling Condition Misalignment in Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2306.14408v3)
- [Zero‑shot spatial layout conditioning for text‑to‑image diffusion models](http://arxiv.org/abs/2306.13754v1)
- [The Cultivated Practices of Text‑to‑Image Generation](http://arxiv.org/abs/2306.11393v3)
- [Point‑Cloud Completion with Pretrained Text‑to‑image Diffusion Models](http://arxiv.org/abs/2306.10533v1)
- [Energy‑Efficient Downlink Semantic Generative Communication with Text‑to‑Image Generators](http://arxiv.org/abs/2306.05041v1)
- [WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2306.04744v3) [![GitHub Stars](https://img.shields.io/github/stars/kylemin/WOUAF?style=social)](https://github.com/kylemin/WOUAF)
- [ConceptBed: Evaluating Concept Learning Abilities of Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2306.04695v2) [![GitHub Stars](https://img.shields.io/github/stars/ConceptBed/evaluations?style=social)](https://github.com/ConceptBed/evaluations)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://conceptbed.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/conceptbed/resultsexplorer)
- [Composition and Deformance: Measuring Imageability with a Text‑to‑Image Model](http://arxiv.org/abs/2306.03168v1)
- [Detector Guidance for Multi‑Object Text‑to‑Image Generation](http://arxiv.org/abs/2306.02236v1)
- [Word‑Level Explanations for Analyzing Bias in Text‑to‑Image Models](http://arxiv.org/abs/2306.05500v1)
- [Multilingual Conceptual Coverage in Text‑to‑Image Models](http://arxiv.org/abs/2306.01735v1)
- [Video Colorization with Pre‑trained Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2306.01732v1)
- [StyleDrop: Text‑to‑Image Generation in Any Style](http://arxiv.org/abs/2306.00983v1)
- [StableRep: Synthetic Images from Text‑to‑Image Models Make Strong Visual Representation Learners](http://arxiv.org/abs/2306.00984v2)
- [SnapFusion: Text‑to‑Image Diffusion Model on Mobile Devices within Two Seconds](http://arxiv.org/abs/2306.00980v3)
- [ViCo: Plug‑and‑play Visual Condition for Personalized Text‑to‑image Generation](http://arxiv.org/abs/2306.00971v2)
- [T2IAT: Measuring Valence and Stereotypical Biases in Text‑to‑Image Generation](http://arxiv.org/abs/2306.00905v1)
- [ReFACT: Updating Text‑to‑Image Models by Editing the Text Encoder](http://arxiv.org/abs/2306.00738v2) [![GitHub Stars](https://img.shields.io/github/stars/Technion-CSNLP/ReFACT?style=social)](https://github.com/technion-cs-nlp/ReFACT)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://technion-cs-nlp.github.io/ReFACT/)
- [Wuerstchen: An Efficient Architecture for Large‑Scale Text‑to‑Image Diffusion Models](http://arxiv.org/abs/2306.00637v2) [![GitHub Stars](https://img.shields.io/github/stars/dome272/Wuerstchen?style=social)](https://github.com/dome272/Wuerstchen)
- [RealignDiff: Boosting Text‑to‑Image Diffusion Model with Coarse‑to‑fine Semantic Re‑alignment](http://arxiv.org/abs/2305.19599v5)
- [Translation‑Enhanced Multilingual Text‑to‑Image Generation](http://arxiv.org/abs/2305.19216v1)
- [Controllable Text‑to‑Image Generation with GPT‑4](http://arxiv.org/abs/2305.18583v1)
- [RAPHAEL: Text‑to‑Image Generation via Large Mixture of Diffusion Paths](http://arxiv.org/abs/2305.18295v5)
- [VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text‑to‑Image Generative Models](http://arxiv.org/abs/2312.00057v2)


</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="conditional">🕹️ Conditional Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[ICCV 2025]** ***UNO:*** A Universal Customization Method for Both Single and Multi‑Subject Conditioning<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.02160) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://bytedance.github.io/UNO) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/UNO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/bytedance-research/UNO)

* **[ICCV 2025]** ***CoMPaSS:*** Enhancing Spatial Understanding in Text‑to‑Image Diffusion Models<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.13195) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://compass.blurgy.xyz) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/blurgyy/CoMPaSS) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/blurgy/CoMPaSS-FLUX.1)

* **[ICCV 2025]** ***SP‑Ctrl:*** Rethink Sparse Signals for Pose‑Guided Text‑to‑Image Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2506.20983) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/DREAMXFAR/SP-Ctrl)

* **[ICCV 2025]** ***CompCon:*** Discovering Divergent Representations Between Text‑to‑Image Models<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2509.08940) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/CompCon)

* **[ICCV 2025]** ***C2OT:*** The Curse of Conditions: Analyzing and Improving Optimal Transport for Conditional Flow‑Based Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.10636) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hkchengrex.com/C2OT) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hkchengrex/C2OT) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hkchengrex/c2ot-cifar10-fm)

* **[ICCV 2025]** ***RAG‑Diffusion:*** Region‑Aware Text‑to‑Image Generation via Hard Binding and Soft Refinement<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.06558) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://huggingface.co/spaces/NJU/RAG-Diffusion) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/RAG-Diffusion)

* **[ICCV 2025]** ***CharaConsist:*** Fine‑Grained Consistent Character Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2507.11533) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://murray-wang.github.io/CharaConsist) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Murray-Wang/CharaConsist) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/black-forest-labs/FLUX.1)

* **[ICCV 2025]** ***Shadow Director:*** Parametric Shadow Control for Portrait Generation in Text‑to‑Image Diffusion Models (ICCV 2025)<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.21943) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hm-cai.com/ShadowDirector)

* **[ICCV 2025]** ***ImageGen‑CoT:*** Enhancing Text‑to‑Image In‑Context Learning with Chain‑of‑Thought Reasoning (ICCV 2025)<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.19312) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://imagegen-cot.github.io)

* **[NeurIPS 2025]** ***SoftREPA:*** Aligning Text to Image in Diffusion Models is Easier Than You Think (NeurIPS 2025)<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.08250) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://softrepa.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/softrepa/SoftREPA)

* **[AAAI 2025]** *Simple-ControlNet: Simplifying Control Mechanism in Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/32309) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/feng-zhida/Simple-ControlNet) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/fzd/SimpleControlNet) 

* **[AAAI 2025]** *EMControl: Adding Conditional Control to Text-to-Image Diffusion Models via EM*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/32828) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](-) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](-) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](-)

* **[AAAI 2025]** *Local Conditional Controlling for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/33139)

* **[AAAI 2025]** *VersaGen: Versatile Visual Control for Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/download/32240/34395) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/FelixChan9527/VersaGen_official)

* **[AAAI 2025]** *Fair Text-to-Image Diffusion via Fair Mapping*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/34823)

* **[ICLR 2025]** *IFAdapter: Instance Feature Control for Grounded T2I*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=openreview)](https://openreview.net/forum?id=25l4SWH2eS) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ifadapter.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WUyinwei-hah/IFAdapter) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/WuYW/IFAdapter)

* **[ICLR 2025]** *LayerFusion / Harmonized Multi-Layer T2I (Foreground+Background)*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=openreview)](https://openreview.net/forum?id=OE2T7AgQFN) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://layerfusion.github.io/)

* **[ICLR 2025]** *Enhancing Compositional T2I with Reliable Random Seeds*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=openreview)](https://openreview.net/forum?id=5BSlakturs) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/doub7e/Reliable-Random-Seeds)

* **[NeurIPS 2025]** *Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/118199)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation](http://arxiv.org/abs/2508.17364v1) [![GitHub Stars](https://img.shields.io/github/stars/gavin-gqzhang/UniGen?style=social)](https://github.com/gavin-gqzhang/UniGen)
- [SafeFix: Targeted Model Repair via Controlled Image Generation](http://arxiv.org/abs/2508.08701v1) [![GitHub Stars](https://img.shields.io/github/stars/oxu2/SafeFix?style=social)](https://github.com/oxu2/SafeFix)
- [MultiRef: Controllable Image Generation with Multiple Visual References](http://arxiv.org/abs/2508.06905v3) [![GitHub Stars](https://img.shields.io/github/stars/Dipsy0830/MultiRef-code?style=social)](https://github.com/Dipsy0830/MultiRef-code) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://multiref.github.io/)
- [DivControl: Knowledge Diversion for Controllable Image Generation](http://arxiv.org/abs/2507.23620v1)
- [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](http://arxiv.org/abs/2507.15724v1)
- [ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning](http://arxiv.org/abs/2506.03596v1) [![GitHub Stars](https://img.shields.io/github/stars/Maplebb/ControlThinker?style=social)](https://github.com/Maplebb/ControlThinker) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/maplebb/ControlThinker)
- [Dual‑Process Image Generation](http://arxiv.org/abs/2506.01955v1) [![GitHub Stars](https://img.shields.io/github/stars/g-luo/dual_process?style=social)](https://github.com/g-luo/dual_process) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://dual-process.github.io/)
- [AP‑CAP: Advancing High‑Quality Data Synthesis for Animal Pose Estimation via a Controllable Image Generation Pipeline](http://arxiv.org/abs/2504.00394v1)
- [STAY Diffusion: Styled Layout Diffusion Model for Diverse Layout‑to‑Image Generation](http://arxiv.org/abs/2503.12213v1)
- [Contract‑Inspired Contest Theory for Controllable Image Generation in Mobile Edge Metaverse](http://arxiv.org/abs/2501.09391v1)
- [Grounding Text‑to‑Image Diffusion Models for Controlled High‑Quality Image Generation](http://arxiv.org/abs/2501.09194v2)
- [Test‑time Controllable Image Generation by Explicit Spatial Constraint Enforcement](http://arxiv.org/abs/2501.01368v1)
- [EliGen: Entity‑Level Controlled Image Generation with Regional Attention](http://arxiv.org/abs/2501.01097v3) [![GitHub Stars](https://img.shields.io/github/stars/modelscope/DiffSynth-Studio?style=social)](https://github.com/modelscope/DiffSynth-Studio) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/modelscope/EliGen)
- [TIDE: Achieving Balanced Subject‑Driven Image Generation via Target‑Instructed Diffusion Enhancement](http://arxiv.org/abs/2509.06499v1) [![GitHub Stars](https://img.shields.io/github/stars/KomJay520/TIDE?style=social)](https://github.com/KomJay520/TIDE)
- [LEARN: A Story‑Driven Layout‑to‑Image Generation Framework for STEM Instruction](http://arxiv.org/abs/2508.11153v1)
- [Locality‑aware Parallel Decoding for Efficient Autoregressive Image Generation](http://arxiv.org/abs/2507.01957v1) [![GitHub Stars](https://img.shields.io/github/stars/mit-han-lab/lpd?style=social)](https://github.com/mit-han-lab/lpd) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/mit-han-lab/lpd_l_512)
- [Hyperspectral Image Generation with Unmixing Guided Diffusion Model](http://arxiv.org/abs/2506.02601v3)
- [Conditional Panoramic Image Generation via Masked Autoregressive Modeling](http://arxiv.org/abs/2505.16862v1) [![GitHub Stars](https://img.shields.io/github/stars/zhuqiangLu/AOG-NET-360?style=social)](https://github.com/zhuqiangLu/AOG-NET-360)
- [Context‑Aware Autoregressive Models for Multi‑Conditional Image Generation](http://arxiv.org/abs/2505.12274v1)


</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‑Semantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‑Shot Structure‑Aware Stylized Image Synthesis:*** *One‑Shot Structure‑Aware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‑to‑Image Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‑to‑Fine Latent Diffusion for Pose‑Guided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304) 

* **[CVPR 2024]** ***CAN:*** *Condition‑Aware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‑Painter:*** *Training‑Free Layout Control for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Zero-Painter)

* **[CVPR 2024]** ***MIGC:*** *Multi‑Instance Generation Controller for Text‑to‑Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‑Free Spatial Control of Any Text‑to‑Image Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‑To‑Image Diffusion Models with Fine‑Grained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl) 

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl) 

* **[NeurIPS 2024]** ***Ctrl‑X:*** *Controlling Structure and Appearance for Text‑To‑Image Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x) 

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‑Guided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs) 

* **[ICLR 2024]** ***R & B:*** *Region and Boundary Aware Zero‑shot Grounded Text‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=8Q4uVOJ5bX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sagileo.github.io/Region-and-Boundary/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/StevenShaw1999/RnB)

* **[WACV 2024]** ***Layout Control with Cross‑Attention Guidance:*** *Training‑Free Layout Control with Cross‑Attention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‑Semantic Map Guided Diffusion Model for Free‑form Layout‑to‑image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‑to‑Image Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [UNIC‑Adapter: Unified Image‑instruction Adapter with Multi‑modal Transformer for Image Generation](http://arxiv.org/abs/2412.18928v1)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/AIDC-AI/UNIC-Adapter)
- [Steering Rectified Flow Models in the Vector Field for Controlled Image Generation](http://arxiv.org/abs/2412.00100v1) [![GitHub Stars](https://img.shields.io/github/stars/FlowChef/flowchef?style=social)](https://github.com/FlowChef/flowchef) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://flowchef.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/FlowChef)
- [Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation](http://arxiv.org/abs/2411.03551v1)
- [CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation](http://arxiv.org/abs/2410.09400v2) [![GitHub Stars](https://img.shields.io/github/stars/xyfJASON/ctrlora?style=social)](https://github.com/xyfJASON/ctrlora) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/xyfJASON/ctrlora)
- [ControlAR: Controllable Image Generation with Autoregressive Models](http://arxiv.org/abs/2410.02705v3) [![GitHub Stars](https://img.shields.io/github/stars/hustvl/ControlAR?style=social)](https://github.com/hustvl/ControlAR) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/wondervictor/ControlAR)
- [BrainDreamer: Reasoning‑Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance](http://arxiv.org/abs/2409.14021v1)
- [CSGO: Content‑Style Composition in Text‑to‑Image Generation](http://arxiv.org/abs/2408.16766v2) [![GitHub Stars](https://img.shields.io/github/stars/InstantX-research/CSGO?style=social)](https://github.com/InstantX‑research/CSGO) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://csgo-gen.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/InstantX/CSGO)
- [MUSES: 3D‑Controllable Image Generation via Multi‑Modal Agent Collaboration](http://arxiv.org/abs/2408.10605v5) [![GitHub Stars](https://img.shields.io/github/stars/DINGYANB/MUSES?style=social)](https://github.com/DINGYANB/MUSES) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/yanboding/MUSES)
- [Prompt‑Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models](http://arxiv.org/abs/2406.16333v1) [![GitHub Stars](https://img.shields.io/github/stars/TruthAI-Lab/PCIG?style=social)](https://github.com/TruthAI-Lab/PCIG)
- [Controllable Image Generation With Composed Parallel Token Prediction](http://arxiv.org/abs/2405.06535v1)
- [Condition‑Aware Neural Network for Controlled Image Generation](http://arxiv.org/abs/2404.01143v1) [![GitHub Stars](https://img.shields.io/github/stars/mit-han-lab/efficientvit?style=social)](https://github.com/mit-han-lab/efficientvit)
- [Refining Text‑to‑Image Generation: Towards Accurate Training‑Free Glyph‑Enhanced Image Generation](http://arxiv.org/abs/2403.16422v2)
- [GazeFusion: Saliency‑Guided Image Generation](http://arxiv.org/abs/2407.04191v2) [![GitHub Stars](https://img.shields.io/github/stars/NYU-ICL/saliency-guided-image-generation?style=social)](https://github.com/NYU-ICL/saliency-guided-image-generation)
- [TCIG: Two‑Stage Controlled Image Generation with Quality Enhancement through Diffusion](http://arxiv.org/abs/2403.01212v1)
- [Text2Street: Controllable Text‑to‑Image Generation for Street Views](http://arxiv.org/abs/2402.04504v1)
- [Spatial‑Aware Latent Initialization for Controllable Image Generation](http://arxiv.org/abs/2401.16157v1)
- [PIXART‑δ: Fast and Controllable Image Generation with Latent Consistency Models](http://arxiv.org/abs/2401.05252v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pixart-alpha.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/collections/PixArt-alpha/pixart-delta-lcm)
- [OmniControlNet: Dual‑stage Integration for Conditional Image Generation](http://arxiv.org/abs/2406.05871v1) [![GitHub Stars](https://img.shields.io/github/stars/Yuanshi9815/OminiControl?style=social)](https://github.com/Yuanshi9815/OminiControl) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Yuanshi/OminiControl)



</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***GLIGEN:*** *Open-Set Grounded Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gligen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/gligen/GLIGEN) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/gligen/demo)

* **[CVPR 2022]** ***Autoregressive Image Generation:*** *Using Residual Quantization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/kakaobrain/rq-vae-transformer)

* **[CVPR 2023]** ***SpaText:*** *Spatio-Textual Representation for Controllable Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/spatext/)

* **[CVPR 2022]** ***Text to Image Generation with Semantic-Spatial Aware GAN:*** *Text to Image Generation with Semantic-Spatial Aware GAN*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf)

* **[CVPR 2023]** ***ReCo:*** *Region-Controlled Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/ReCo)

* **[CVPR 2023]** ***LayoutDiffusion:*** *Controllable Diffusion Model for Layout-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/LayoutDiffusion)

* **[ICLR 2023]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grenoble-zhang.github.io/Ctrl-U-Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/grenoble-zhang/Ctrl-U)

* **[ICCV 2023]** ***ControlNet:*** *Adding Conditional Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/ControlNet) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/lllyasviel/ControlNet)

* **[ICCV 2023]** ***SceneGenie:*** *Scene Graph Guided Diffusion Models for Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf)

* **[ICCV 2023]** ***ZestGuide:*** *Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)

* **[ICML 2023]** ***Composer:*** *Creative and Controllable Image Synthesis with Composable Conditions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/huang23b/huang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/composer-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/composer)

* **[ICML 2023]** ***MultiDiffusion:*** *Fusing Diffusion Paths for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://multidiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/omerbt/MultiDiffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/weizmannscience/MultiDiffusion)

* **[SIGGRAPH 2023]** ***Sketch-Guided Text-to-Image Diffusion Models:*** *Sketch-Guided Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3588432.3591560) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sketch-guided-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ogkalu2/Sketch-Guided-Stable-Diffusion)

* **[NeurIPS 2023]** ***Uni-ControlNet:*** *All-in-One Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16322.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/unicontrolnet/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/Uni-ControlNet)

* **[NeurIPS 2023]** ***Prompt Diffusion:*** *In-Context Learning Unlocked for Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=6BZS2EAkns) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhendong-wang.github.io/prompt-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Zhendong-Wang/Prompt-Diffusion)

* **[WACV 2023]** ***More Control for Free!:*** *Image Synthesis with Semantic Diffusion Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf)

* **[ACM MM 2023]** ***LayoutLLM-T2I:*** *Eliciting Layout Guidance from LLM for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.05095.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Diffusion Self‑Guidance for Controllable Image Generation](http://arxiv.org/abs/2306.00986v3) [![GitHub Stars](https://img.shields.io/github/stars/superolly/self-guidance?style=social)](https://github.com/superolly/self-guidance)
- [Robust Image Ordinal Regression with Controllable Image Generation](http://arxiv.org/abs/2305.04213v3) [![GitHub Stars](https://img.shields.io/github/stars/Ch3ngY1/Controllable-Image-Generation?style=social)](https://github.com/Ch3ngY1/Controllable-Image-Generation)
- [Controllable Image Generation via Collage Representations](http://arxiv.org/abs/2304.13722v1)
- [Diagnostic Benchmark and Iterative Inpainting for Layout‑Guided Image Generation](http://arxiv.org/abs/2304.06671v3) [![GitHub Stars](https://img.shields.io/github/stars/j-min/IterInpaint?style=social)](https://github.com/j-min/IterInpaint) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://layoutbench.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/j-min/IterInpaint-CLEVR)
- [NoisyTwins: Class‑Consistent and Diverse Image Generation through StyleGANs](http://arxiv.org/abs/2304.05866v1) [![GitHub Stars](https://img.shields.io/github/stars/val-iisc/NoisyTwins?style=social)](https://github.com/val-iisc/NoisyTwins) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://rangwani-harsh.github.io/NoisyTwins)
- [GlueGen: Plug and Play Multi‑modal Encoders for X‑to‑image Generation](http://arxiv.org/abs/2303.10056v2) [![GitHub Stars](https://img.shields.io/github/stars/salesforce/GlueGen?style=social)](https://github.com/salesforce/GlueGen)
- [MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation](http://arxiv.org/abs/2302.08113v1) [![GitHub Stars](https://img.shields.io/github/stars/omerbt/MultiDiffusion?style=social)](https://github.com/omerbt/MultiDiffusion) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://multidiffusion.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/weizmannscience/MultiDiffusion)



</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="personalized">🎨 Personalized Image Generation</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***SerialGen:*** *Personalized Image Generation by First Standardization Then Personalization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.01485) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://serialgen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/)

* **[CVPR 2025]** ***PatchDPO:*** *Patch-level DPO for Finetuning-free Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03177) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/hqhQAQ/PatchDPO) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hqhQAQ/PatchDPO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hqhQAQ/PatchDPO)

* **[CVPR 2025]** ***DreamCache:*** *Finetuning-Free Lightweight Personalized Image Generation via Feature Caching*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.17786) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://emanuele97x.github.io/DreamCache) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Emanuele97x/DreamCache)


* **[ICLR 2025]** ***MS-Diffusion:*** *Multi-Subject Zero-shot Image Personalization with Layout Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07209) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ms-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MS-Diffusion/MS-Diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/doge1516/MS-Diffusion)

* **[ICLR 2025]** ***ClassDiffusion:*** *More Aligned Personalization Tuning with Explicit Class Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=iTm4H6N4aG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://classdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Rbrq03/ClassDiffusion)

* **[ICLR 2025]** ***DreamBench++:*** *A Human-Aligned Benchmark for Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.16855) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambenchplus.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuangpeng/dreambench_plus) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/yuangpeng/dreambench_plus)

* **[ICLR 2025]** ***TweedieMix:*** *Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.05591) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/KwonGihyun/TweedieMix) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwonGihyun/TweedieMix)

* **[NeurIPS 2025]** ***RePIC:*** *Reinforced Post-Training for Personalizing Multi-Modal Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119231) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/oyt9306/RePIC) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Yeongtak/RePIC_Qwen2.5VL_7B)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [FocusDPO: Dynamic Preference Optimization for Multi-Subject Personalized Image Generation via Adaptive Focus](http://arxiv.org/abs/2509.01181v1) [![GitHub Stars](https://img.shields.io/github/stars/bytedance-fanqie-ai/FocusDPO?style=social)](https://github.com/bytedance-fanqie-ai/FocusDPO)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://bytedance-fanqie-ai.github.io/FocusDPO/)
- [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](http://arxiv.org/abs/2508.11433v2)
- [Anti-Tamper Protection for Unauthorized Individual Image Generation](http://arxiv.org/abs/2508.06325v1) [![GitHub Stars](https://img.shields.io/github/stars/Seeyn/Anti-Tamper-Perturbation?style=social)](https://github.com/Seeyn/Anti-Tamper-Perturbation)
- [Improving Personalized Image Generation through Social Context Feedback](http://arxiv.org/abs/2507.16095v1)
- [A Training-Free Style‑Personalization via Scale‑wise Autoregressive Model](http://arxiv.org/abs/2507.04482v1)
- [Personalized Image Generation from an Author Writing Style](http://arxiv.org/abs/2507.03313v1)
- [TaleForge: Interactive Multimodal System for Personalized Story Creation](http://arxiv.org/abs/2506.21832v1)
- [AlignGen: Boosting Personalized Image Generation with Cross‑Modality Prior Alignment](http://arxiv.org/abs/2505.21911v1)
- [RAGAR: Retrieval Augmented Personalized Image Generation Guided by Recommendation](http://arxiv.org/abs/2505.01657v2)
- [DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition](http://arxiv.org/abs/2504.17349v2)
- [Personalized Text‑to‑Image Generation with Auto‑Regressive Models](http://arxiv.org/abs/2504.13162v1) [![GitHub Stars](https://img.shields.io/github/stars/KaiyueSun98/T2I-Personalization-with-AR?style=social)](https://github.com/KaiyueSun98/T2I-Personalization-with-AR)
- [AC‑LoRA: Auto Component LoRA for Personalized Artistic Style Image Generation](http://arxiv.org/abs/2504.02231v1)
- [Single Image Iterative Subject‑driven Generation and Editing](http://arxiv.org/abs/2503.16025v1) [![GitHub Stars](https://img.shields.io/github/stars/yairshp/SISO?style=social)](https://github.com/yairshp/SISO)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://siso-paper.github.io/)
- [Personalize Anything for Free with Diffusion Transformer](http://arxiv.org/abs/2503.12590v1) [![GitHub Stars](https://img.shields.io/github/stars/fenghora/personalize-anything?style=social)](https://github.com/fenghora/personalize-anything)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://fenghora.github.io/Personalize-Anything-Page/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/black-forest-labs/FLUX.1-dev)
- [Towards More Accurate Personalized Image Generation: Addressing Overfitting and Evaluation Bias](http://arxiv.org/abs/2503.06632v1) [![GitHub Stars](https://img.shields.io/github/stars/Mingxiao-Li/Towards-More-Accurate-Personalized-Image-Generation?style=social)](https://github.com/Mingxiao-Li/Towards-More-Accurate-Personalized-Image-Generation)
- [Conceptrol: Concept Control of Zero‑shot Personalized Image Generation](http://arxiv.org/abs/2503.06568v1) [![GitHub Stars](https://img.shields.io/github/stars/QY-H00/Conceptrol?style=social)](https://github.com/QY-H00/Conceptrol)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://qy-h00.github.io/Conceptrol/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/qyoo/Conceptrol)
- [Personalized Image Generation with Deep Generative Models: A Decade Survey](http://arxiv.org/abs/2502.13081v1) [![GitHub Stars](https://img.shields.io/github/stars/csyxwei/Awesome-Personalized-Image-Generation?style=social)](https://github.com/csyxwei/Awesome-Personalized-Image-Generation)
- [Beyond Fine‑Tuning: A Systematic Study of Sampling Techniques in Personalized Image Generation](http://arxiv.org/abs/2502.05895v1) [![GitHub Stars](https://img.shields.io/github/stars/ControlGenAI/PersonGenSampler?style=social)](https://github.com/ControlGenAI/PersonGenSampler)
- [Enhanced Multi‑Scale Cross‑Attention for Person Image Generation](http://arxiv.org/abs/2501.08900v1)
- [SceneBooth: Diffusion‑based Framework for Subject‑preserved Text‑to‑Image Generation](http://arxiv.org/abs/2501.03490v1)


</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***Cross Initialization:*** *Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15905.pdf)

* **[CVPR 2024]** ***When StyleGAN Meets Stable Diffusion:*** *a W+ Adapter for Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17461.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://csxmli2016.github.io/projects/w-plus-adapter/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csxmli2016/w-plus-adapter)

* **[CVPR 2024]** ***Style Aligned:*** *Image Generation via Shared Attention*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.02133.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://style-aligned-gen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/style-aligned)

* **[CVPR 2024]** ***InstantBooth:*** *Personalized Text‑to‑Image Generation without Test‑Time Finetuning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2304.03411.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jshi31.github.io/InstantBooth/)

* **[CVPR 2024]** ***High Fidelity:*** *Person‑centric Subject‑to‑Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.10329.pdf)

* **[CVPR 2024]** ***RealCustom:*** *Narrowing Real Text Word for Real‑Time Open‑Domain Text‑to‑Image Customization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.00483.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://corleone-huang.github.io/realcustom/) [![🤗 Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/bytedance-research/RealCustom)

* **[CVPR 2024]** ***DisenDiff:*** *Attention Calibration for Disentangled Text‑to‑Image Personalization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.18551) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Monalissaa/DisenDiff)

* **[CVPR 2024]** ***FreeCustom:*** *Tuning‑Free Customized Image Generation for Multi‑Concept Composition*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.13870v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/FreeCustom/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/FreeCustom)

* **[CVPR 2024]** ***Personalized Residuals:*** *for Concept‑Driven Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.12978)

* **[CVPR 2024]** ***Subject‑Agnostic Guidance:*** *Improving Subject‑Driven Image Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.01356)

* **[CVPR 2024]** ***JeDi:*** *Joint‑Image Diffusion Models for Finetuning‑Free Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_JeDi_Joint-Image_Diffusion_Models_for_Finetuning-Free_Personalized_Text-to-Image_Generation_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***Influence Watermarks:*** *Countering Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***PIA:*** *Your Personalized Image Animator via Plug‑and‑Play Modules in Text‑to‑Image Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pi-animator.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/PIA)

* **[CVPR 2024]** ***SSR‑Encoder:*** *Encoding Selective Subject Representation for Subject‑Driven Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Xiaojiu-z/SSR_Encoder)

* **[CVPR 2024]** ***Cross Initialization:*** *Face Personalization of Text‑to‑Image Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00802) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lyuPang/CrossInitialization)

* **[ECCV 2024]** ***Be Yourself:*** *Bounded Attention for Multi‑Subject Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.16990) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omer11a.github.io/bounded-attention/)

* **[ECCV 2024]** ***Powerful and Flexible:*** *Personalized Text‑to‑Image Generation via Reinforcement Learning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.06642v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wfanyue/DPG-T2I-Personalization)

* **[ECCV 2024]** ***TIGC:*** *Tuning‑Free Image Customization with Image and Text Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.12658) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/TIGIC/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zrealli/TIGIC)

* **[ECCV 2024]** ***MasterWeaver:*** *Taming Editability and Face Identity for Personalized Text‑to‑Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06786.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://masterweaver.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/MasterWeaver)

* **[NeurIPS 2024]** ***RectifID:*** *Personalizing Rectified Flow with Anchored Classifier Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/afa58a5b6adc0845e0fd632132a64c39-Paper-Conference.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/feifeiobama/RectifID)

* **[NeurIPS 2024]** ***AttnDreamBooth:*** *Towards Text‑Aligned Personalized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/465a13a95741fab2e912f98adb07df1d-Paper-Conference.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attndreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lyuPang/AttnDreamBooth)

* **[ICLR 2024]** ***DisenBooth:*** *DisenBooth: Identity‑Preserving Disentangled Tuning for Subject‑Driven Text‑to‑Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=FlhjUkC7vH) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://disenbooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/forchchch/DisenBooth)

* **[AAAI 2024]** ***Decoupled Textual Embeddings:*** *for Customized Image Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.11826.pdf)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories](http://arxiv.org/abs/2412.17040v2)
- [PersonaMagic: Stage-Regulated High-Fidelity Face Customization with Tandem Equilibrium](http://arxiv.org/abs/2412.15674v1) [![GitHub Stars](https://img.shields.io/github/stars/xzhe-Vision/PersonaMagic?style=social)](https://github.com/xzhe-Vision/PersonaMagic)
- [LoRACLR: Contrastive Adaptation for Customization of Diffusion Models](http://arxiv.org/abs/2412.09622v1)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://loraclr.github.io)
- [Learning Flow Fields in Attention for Controllable Person Image Generation](http://arxiv.org/abs/2412.08486v2) [![GitHub Stars](https://img.shields.io/github/stars/franciszzj/Leffa?style=social)](https://github.com/franciszzj/Leffa) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/franciszzj/Leffa)
- [PatchDPO: Patch-level DPO for Finetuning-free Personalized Image Generation](http://arxiv.org/abs/2412.03177v2) [![GitHub Stars](https://img.shields.io/github/stars/hqhQAQ/PatchDPO?style=social)](https://github.com/hqhQAQ/PatchDPO) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/hqhQAQ/PatchDPO)
- [SerialGen: Personalized Image Generation by First Standardization Then Personalization](http://arxiv.org/abs/2412.01485v2)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://serialgen.github.io)
- [Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment](http://arxiv.org/abs/2412.00306v1)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://song630.github.io/Refine-by-Align-Project-Page/)
- [DreamBlend: Advancing Personalized Fine-tuning of Text-to-Image Diffusion Models](http://arxiv.org/abs/2411.19390v1)
- [DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching](http://arxiv.org/abs/2411.17786v1) [![GitHub Stars](https://img.shields.io/github/stars/Emanuele97x/DreamCache?style=social)](https://github.com/Emanuele97x/DreamCache) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://emanuele97x.github.io/DreamCache)
- [Personalized Image Generation with Large Multimodal Models](http://arxiv.org/abs/2410.14170v2) [![GitHub Stars](https://img.shields.io/github/stars/YiyanXu/Pigeon?style=social)](https://github.com/YiyanXu/Pigeon)
- [FaceChain-FACT: Face Adapter with Decoupled Training for Identity-preserved Personalization](http://arxiv.org/abs/2410.12312v2) [![GitHub Stars](https://img.shields.io/github/stars/modelscope/facechain?style=social)](https://github.com/modelscope/facechain) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://facechain-fact.github.io) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/modelscope/FaceChain-FACT)
- [Resolving Multi-Condition Confusion for Finetuning-Free Personalized Image Generation (MIP-Adapter)](http://arxiv.org/abs/2409.17920v2) [![GitHub Stars](https://img.shields.io/github/stars/hqhQAQ/MIP-Adapter?style=social)](https://github.com/hqhQAQ/MIP-Adapter) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/hqhQAQ/MIP-Adapter)
- [Imagine Yourself: Tuning-Free Personalized Image Generation](http://arxiv.org/abs/2409.13346v1)
- [StoryMaker: Towards Holistic Consistent Characters in Text-to-Image Generation](http://arxiv.org/abs/2409.12576v1) [![GitHub Stars](https://img.shields.io/github/stars/RedAIGC/StoryMaker?style=social)](https://github.com/RedAIGC/StoryMaker) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/RED-AIGC/StoryMaker)
- [TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder](http://arxiv.org/abs/2409.08248v1) [![GitHub Stars](https://img.shields.io/github/stars/nahyeonkaty/textboost?style=social)](https://github.com/nahyeonkaty/textboost) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://textboost.github.io)
- [EZIGen: Enhancing Zero-shot Personalized Image Generation with Precise Subject Encoding and Decoupled Guidance](http://arxiv.org/abs/2409.08091v4) [![GitHub Stars](https://img.shields.io/github/stars/ZichengDuan/EZIGen?style=social)](https://github.com/ZichengDuan/EZIGen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zichengduan.github.io/pages/EZIGen/)
- [ViPer: Visual Personalization of Generative Models via Individual Preference Learning](http://arxiv.org/abs/2407.17365v1) [![GitHub Stars](https://img.shields.io/github/stars/EPFL-VILAB/ViPer?style=social)](https://github.com/EPFL-VILAB/ViPer)
- [Layout-and-Retouch: A Dual-stage Framework for Improving Diversity in Personalized Image Generation](http://arxiv.org/abs/2407.09779v1)
- [RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance](http://arxiv.org/abs/2405.14677v4) [![GitHub Stars](https://img.shields.io/github/stars/feifeiobama/RectifID?style=social)](https://github.com/feifeiobama/RectifID)
- [FreeTuner: Any Subject in Any Style with Training-free Diffusion](http://arxiv.org/abs/2405.14201v2)
- [InstantFamily: Masked Attention for Zero-shot Multi-ID Image Generation](http://arxiv.org/abs/2404.19427v1)
- [MoA: Mixture-of-Attention for Subject-Context Disentanglement in Personalized Image Generation](http://arxiv.org/abs/2404.11565v2)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/mixture-of-attention)
- [CAT: Contrastive Adapter Training for Personalized Image Generation](http://arxiv.org/abs/2404.07554v2)
- [MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation](http://arxiv.org/abs/2404.05674v1) [![GitHub Stars](https://img.shields.io/github/stars/bytedance/MoMA?style=social)](https://github.com/bytedance/MoMA) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://moma-adapter.github.io) [![Hugging-Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/KunpengSong/MoMA_llava_7b)
- [MM-Diff: High-Fidelity Image Personalization via Multi-Modal Condition Integration](http://arxiv.org/abs/2403.15059v1) [![GitHub Stars](https://img.shields.io/github/stars/alibaba/mm-diff?style=social)](https://github.com/alibaba/mm-diff) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://mm-diff.github.io)
- [IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models](http://arxiv.org/abs/2403.13535v2)
- [Fast Personalized Text-to-Image Syntheses With Attention Injection](http://arxiv.org/abs/2403.11284v1)
- [Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition](http://arxiv.org/abs/2402.15504v1) [![GitHub Stars](https://img.shields.io/github/stars/louisYen/Gen4Gen?style=social)](https://github.com/louisYen/Gen4Gen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://danielchyeh.github.io/Gen4Gen)
- [Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity Personalized Diffusion Generation (SeFi-IDE)](http://arxiv.org/abs/2402.00631v2)  [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://com-vis.github.io/SeFi-IDE)
- [BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models](http://arxiv.org/abs/2401.13974v1)


</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/custom-diffusion-library/cat)

* **[CVPR 2023]** ***DreamBooth:*** *Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/dreambooth) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/DreamBooth-AltDiffusion)

* **[ICCV 2023]** ***ELITE:*** *Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/csyxwei/ELITE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/ELITE) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ELITE-library/ELITE)

* **[ICLR 2023]** ***Textual Inversion:*** *An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=NAQvF08TcyG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://textual-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rinongal/textual_inversion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Clyuue/textual_inversion_cat)

* **[SIGGRAPH Asia 2023]** ***Break-A-Scene:*** *Extracting Multiple Concepts from a Single Image*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16311.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/break-a-scene) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/break-a-scene)

* **[SIGGRAPH 2023]** ***Encoder‑Based Domain Tuning:*** *Encoder‑Based Domain Tuning for Fast Personalization of Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2302.12228.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tuning-encoder.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/e4t-diffusion)

* **[SIGGRAPH 2023]** ***LayerDiffusion:*** *Layered Controlled Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3610543.3626172) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/layerdiffusion/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/LayerDiffuse) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/LayerDiffusion/layerdiffusion-v1)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization](http://arxiv.org/abs/2312.06354v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://portraitbooth.github.io/)
- [Disentangled Representation Learning for Controllable Person Image Generation](http://arxiv.org/abs/2312.05798v1)
- [HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models](http://arxiv.org/abs/2312.00079v1)
- [When StyleGAN Meets Stable Diffusion: a \\mathscr{W}\_+ Adapter for Personalized Image Generation](http://arxiv.org/abs/2311.17461v1) [![GitHub Stars](https://img.shields.io/github/stars/csxmli2016/w-plus-adapter?style=social)](https://github.com/csxmli2016/w-plus-adapter)
- [CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image Personalization](http://arxiv.org/abs/2311.14631v2) [![GitHub Stars](https://img.shields.io/github/stars/RoyZhao926/CatVersion?style=social)](https://github.com/RoyZhao926/CatVersion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://royzhao926.github.io/CatVersion-page/)
- [FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content](http://arxiv.org/abs/2308.14256v2) [![GitHub Stars](https://img.shields.io/github/stars/modelscope/facechain?style=social)](https://github.com/modelscope/facechain)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://facechain-fact.github.io/) [![HuggingFace Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/modelscope/FaceChain)
- [Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning](http://arxiv.org/abs/2307.11410v2) [![GitHub Stars](https://img.shields.io/github/stars/OPPO-Mente-Lab/Subject-Diffusion?style=social)](https://github.com/OPPO-Mente-Lab/Subject-Diffusion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://oppo-mente-lab.github.io/subject_diffusion/)
- [FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention](http://arxiv.org/abs/2305.10431v2) [![GitHub Stars](https://img.shields.io/github/stars/mit-han-lab/fastcomposer?style=social)](https://github.com/mit-han-lab/fastcomposer) [![HuggingFace Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/mit-han-lab/fastcomposer)
- [UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer](http://arxiv.org/abs/2304.08870v2) [![GitHub Stars](https://img.shields.io/github/stars/soon-yau/upgpt?style=social)](https://github.com/soon-yau/upgpt) [![HuggingFace Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/soonyau/upgpt)
- [Identity Encoder for Personalized Diffusion](http://arxiv.org/abs/2304.07429v1)
- [InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning](http://arxiv.org/abs/2304.03411v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://jshi31.github.io/InstantBooth/)
- [Semantically Consistent Person Image Generation](http://arxiv.org/abs/2302.14728v2)
- [Learning Invariance from Generated Variance for Unsupervised Person Re-identification](http://arxiv.org/abs/2301.00725v1) [![GitHub Stars](https://img.shields.io/github/stars/chenhao2345/GCL-extended?style=social)](https://github.com/chenhao2345/GCL-extended)


</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)

### <span id="editing">✂️ Image Editing</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[NeurIPS 2025]** ***In-Context Edit:*** *Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.20690) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://river-zhang.github.io/ICEdit-gh-pages/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/River-Zhang/ICEdit?tab=readme-ov-file) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/RiverZ/normal-lora/tree/main)

* **[NeurIPS 2025]** ***Dual‑Conditional Inversion:*** *for Boosting Diffusion‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2506.02560)

* **[NeurIPS 2025]** ***CAMILA:*** *Context‑Aware Masking for Image Editing with Language Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119101) 

* **[NeurIPS 2025]** ***EditInfinity:*** *Image Editing with Binary‑Quantized Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/115392) 

* **[NeurIPS 2025]** ***KRIS‑Bench:*** *Benchmarking Knowledge‑Based Reasoning in Image Editing Systems*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2505.16707) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yongliang-wu.github.io/kris_bench_project_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mercurystraw/Kris_Bench) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/Yongliang-Wu/kris_bench)

* **[NeurIPS 2025]** ***LoongX:*** *Neural-Driven Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2507.05397) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://loongx1.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LanceZPF/loongx)

* **[NeurIPS 2025]** ***CREA:*** *CREA: A Collaborative Multi‑Agent Framework for Creative Image Editing and Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.05306) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://crea-diffusion.github.io)

* **[NeurIPS 2025]** ***IEAP:*** *Image Editing As Programs with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2506.04158) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujiahu1109.github.io/IEAP/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YujiaHu1109/IEAP) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/Cicici1109/IEAP)


* **[CVPR 2025]** ***FDS:*** *Frequency‑Aware Denoising Score for Text‑Guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19191) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ivrl.github.io/fds-webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/IVRL/FDS)


* **[CVPR 2025]** *Reference‑Based 3D‑Aware Image Editing with Triplanes*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.03632) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://three-bee.github.io/triplane_edit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/three-bee/triplane_edit)


* **[CVPR 2025]** ***MoEdit:*** *On Learning Quantity Perception for Multi‑object Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.10112)


* **[ICLR 2025]** *Lightning‑Fast Image Inversion and Editing for Text‑to‑Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=t9l63huPRt) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://barakmam.github.io/rnri.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvirsamuel/NewtonRaphsonInversion)


* **[ICLR 2025]** *Multi‑Reward as Condition for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=9RFocgIccP) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/Multi-Reward-Editing)


* **[ICLR 2025]** ***HQ‑Edit:*** *A High‑Quality Dataset for Instruction‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mZptYYttFj) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thefllood.github.io/HQEdit_web/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSC-VLAA/HQ-Edit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)


* **[ICLR 2025]** ***CLIPDrag:*** *Combining Text‑based and Drag‑based Instructions for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2HjRezQ1nj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HKUST-LongGroup/CLIPDrag)


* **[ICLR 2025]** *Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hu0FSOSEyS) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rf-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LituRout/RF-Inversion)


* **[ICLR 2025]** ***PostEdit:*** *Posterior Sampling for Efficient Zero‑Shot Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=J8YWCBPgx7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TFNTF/PostEdit)


* **[ICLR 2025]** ***OmniEdit:*** *Building Image Editing Generalist Models Through Specialist Supervision*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hlm0cga0sv) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-ai-lab.github.io/OmniEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TIGER-AI-Lab/OmniEdit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/TIGER-Lab/OmniEdit-Filtered-1.2M)

* **[NeurIPS 2025]** ***SplitFlow:*** *Flow Decomposition for Inversion-Free Text-to-Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/116281)
* **[NeurIPS 2025]** ***IEAP:*** *Image Editing As Programs with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/118240) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujiahu1109.github.io/IEAP/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YujiaHu1109/IEAP) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Cicici1109/IEAP)
* **[NeurIPS 2025]** ***In-Context Edit:*** *Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119860) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://river-zhang.github.io/ICEdit-gh-pages/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/River-Zhang/ICEdit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/sanaka87/ICEdit-MoE-LoRA)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](http://arxiv.org/abs/2509.04582v1) [![GitHub Stars](https://img.shields.io/github/stars/Visual-AI/Inpaint4Drag?style=social)](https://github.com/Visual-AI/Inpaint4Drag) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://visual-ai.github.io/inpaint4drag/)
- [From Editor to Dense Geometry Estimator](http://arxiv.org/abs/2509.04338v1) [![GitHub Stars](https://img.shields.io/github/stars/AMAP-ML/FE2E?style=social)](https://github.com/AMAP-ML/FE2E) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://amap-ml.github.io/FE2E/)
- [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](http://arxiv.org/abs/2509.05285v1)
- [Fidelity-preserving enhancement of ptychography with foundational text-to-image models](http://arxiv.org/abs/2509.04513v1)
- [Draw-In-Mind: Learning Precise Image Editing via Chain-of-Thought Imagination](http://arxiv.org/abs/2509.01986v1) [![GitHub Stars](https://img.shields.io/github/stars/showlab/DIM?style=social)](https://github.com/showlab/DIM)
- [Discrete Noise Inversion for Next-scale Autoregressive Text-based Image Editing](http://arxiv.org/abs/2509.01984v2)
- [Delta Velocity Rectified Flow for Text-to-Image Editing](http://arxiv.org/abs/2509.05342v1)
- [Neural Scene Designer: Self-Styled Semantic Image Manipulation](http://arxiv.org/abs/2509.01405v1)
- [LatentEdit: Adaptive Latent Control for Consistent Semantic Editing](http://arxiv.org/abs/2509.00541v1)
- [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](http://arxiv.org/abs/2508.20987v1)
- [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](http://arxiv.org/abs/2508.20505v1)
- [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](http://arxiv.org/abs/2508.19791v1)
- [SpotEdit: Evaluating Visually-Guided Image Editing Methods](http://arxiv.org/abs/2508.18159v1)
- [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing](http://arxiv.org/abs/2508.17435v1)
- [Defending Deepfake via Texture Feature Perturbation](http://arxiv.org/abs/2508.17315v1)
- [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing](http://arxiv.org/abs/2508.17302v1)
- [Visual Autoregressive Modeling for Instruction-Guided Image Editing](http://arxiv.org/abs/2508.15772v1)
- [Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing](http://arxiv.org/abs/2508.13797v1)
- [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](http://arxiv.org/abs/2508.12718v1)
- [PEdger++: Practical Edge Detection via Assembling Cross Information](http://arxiv.org/abs/2508.11961v1)
- [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](http://arxiv.org/abs/2508.11284v2)
- [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](http://arxiv.org/abs/2508.10711v2)
- [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](http://arxiv.org/abs/2508.10509v1)
- [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](http://arxiv.org/abs/2508.10498v1)
- [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](http://arxiv.org/abs/2508.08134v2) [![GitHub Stars](https://img.shields.io/github/stars/mayuelala/FollowYourShape?style=social)](https://github.com/mayuelala/FollowYourShape) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://follow-your-shape.github.io/)
- [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](http://arxiv.org/abs/2508.07981v2)
- [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](http://arxiv.org/abs/2508.07607v1)
- [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](http://arxiv.org/abs/2508.07519v1)
- [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](http://arxiv.org/abs/2508.07413v1)
- [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](http://arxiv.org/abs/2508.06937v1) [![GitHub Stars](https://img.shields.io/github/stars/vaynexie/CannyEdit?style=social)](https://github.com/vaynexie/CannyEdit) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vaynexie.github.io/CannyEdit/)
- [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](http://arxiv.org/abs/2508.06916v1)
- [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](http://arxiv.org/abs/2508.06101v1)
- [DreamVE: Unified Instruction-based Image and Video Editing](http://arxiv.org/abs/2508.06080v1)
- [NEP: Autoregressive Image Editing via Next Editing Token Prediction](http://arxiv.org/abs/2508.06044v1)
- [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](http://arxiv.org/abs/2508.06033v1)
- [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation](http://arxiv.org/abs/2508.03320v1)
- [Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation](http://arxiv.org/abs/2508.03300v1)
- [LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing](http://arxiv.org/abs/2508.03144v2) [![GitHub Stars](https://img.shields.io/github/stars/oyly16/LORE?style=social)](https://github.com/oyly16/LORE)
- [UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying](http://arxiv.org/abs/2508.03142v1)
- [Transport-Guided Rectified Flow Inversion: Improved Image Editing Using Optimal Transport Theory](http://arxiv.org/abs/2508.02363v1)
- [Qwen-Image Technical Report](http://arxiv.org/abs/2508.02324v1)
- [The Promise of RL for Autoregressive Image Editing](http://arxiv.org/abs/2508.01119v2)
- [Towards Robust Semantic Correspondence: A Benchmark and Insights](http://arxiv.org/abs/2508.00272v1)
- [Training-free Geometric Image Editing on Diffusion Models](http://arxiv.org/abs/2507.23300v2) [![GitHub Stars](https://img.shields.io/github/stars/CIawevy/FreeFine?style=social)](https://github.com/CIawevy/FreeFine)
- [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](http://arxiv.org/abs/2507.23278v1)
- [GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset](http://arxiv.org/abs/2507.21033v1) [![GitHub Stars](https://img.shields.io/github/stars/wyhlovecpp/GPT-Image-Edit?style=social)](https://github.com/wyhlovecpp/GPT-Image-Edit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ucsc-vlaa.github.io/GPT-Image-Edit/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M)
- [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](http://arxiv.org/abs/2507.17801v1) [![GitHub Stars](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-mGPT-2.0?style=social)](https://github.com/Alpha-VLLM/Lumina-mGPT-2.0) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Alpha-VLLM/Lumina-mGPT-2.0)
- [ADCD-Net: Robust Document Image Forgery Localization via Adaptive DCT Feature and Hierarchical Content Disentanglement](http://arxiv.org/abs/2507.16397v1) [![GitHub Stars](https://img.shields.io/github/stars/KAHIMWONG/ADCD-Net?style=social)](https://github.com/KAHIMWONG/ADCD-Net)
- [Scale Your Instructions: Enhance the Instruction-Following Fidelity of Unified Image Generation Model by Self-Adaptive Attention Scaling](http://arxiv.org/abs/2507.16240v1) [![GitHub Stars](https://img.shields.io/github/stars/zhouchao-ops/SaaS?style=social)](https://github.com/zhouchao-ops/SaaS) 
- [LMM4Edit: Benchmarking and Evaluating Multimodal Image Editing with LMMs](http://arxiv.org/abs/2507.16193v2) [![GitHub Stars](https://img.shields.io/github/stars/IntMeGroup/LMM4Edit?style=social)](https://github.com/IntMeGroup/LMM4Edit) 
- [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](http://arxiv.org/abs/2507.14809v1)
- [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](http://arxiv.org/abs/2507.14119v1) [![GitHub Stars](https://img.shields.io/github/stars/ai-forever/NoHumansRequired?style=social)](https://github.com/ai-forever/NoHumansRequired)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://riko0.github.io/No-Humans-Required/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/iitolstykh/NHR-Edit) 
- [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](http://arxiv.org/abs/2507.14024v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://moodify2024.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/moodify2024/moodifyCLIP) 
- [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](http://arxiv.org/abs/2507.11096v1) [![GitHub Stars](https://img.shields.io/github/stars/billsioros/EditGen?style=social)](https://github.com/billsioros/EditGen) 
- [Sparse Fine-Tuning of Transformers for Generative Tasks](http://arxiv.org/abs/2507.10855v1) [![GitHub Stars](https://img.shields.io/github/stars/liuting20/Sparse-Tuning?style=social)](https://github.com/liuting20/Sparse-Tuning)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://weichennone.github.io/myhomepage/_projects/2025iccv/index.html) 
- [LayLens: Improving Deepfake Understanding through Simplified Explanations](http://arxiv.org/abs/2507.10066v2)
- [FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields](http://arxiv.org/abs/2507.08285v1) [![GitHub Stars](https://img.shields.io/github/stars/kookie12/FlowDrag?style=social)](https://github.com/kookie12/FlowDrag)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kookie12.github.io/FlowDrag-Projecct-Page/)
- [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](http://arxiv.org/abs/2507.07317v2) [![GitHub Stars](https://img.shields.io/github/stars/SamsungLabs/ADIEE?style=social)](https://github.com/SamsungLabs/ADIEE) 
- [2D Instance Editing in 3D Space](http://arxiv.org/abs/2507.05819v1)
- [Neural-Driven Image Editing](http://arxiv.org/abs/2507.05397v2) [![GitHub Stars](https://img.shields.io/github/stars/LanceZPF/loongx?style=social)](https://github.com/LanceZPF/loongx)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://loongx1.github.io/) 
- [Beyond Simple Edits: X‑Planner for Complex Instruction-Based Image Editing](http://arxiv.org/abs/2507.05259v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://danielchyeh.github.io/x-planner/) 
- [S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial Control](http://arxiv.org/abs/2507.04584v1) [![GitHub Stars](https://img.shields.io/github/stars/JohnDreamer/DualCycleDiffusion?style=social)](https://github.com/JohnDreamer/DualCycleDiffusion) 
- [Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images](http://arxiv.org/abs/2507.03402v1) [![GitHub Stars](https://img.shields.io/github/stars/NDYBSNDY/Pose-Star?style=social)](https://github.com/NDYBSNDY/Pose-Star) 
- [LACONIC: A 3D Layout Adapter for Controllable Image Creation](http://arxiv.org/abs/2507.03257v2)
- [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](http://arxiv.org/abs/2507.01908v1) [![GitHub Stars](https://img.shields.io/github/stars/hithqd/ReasonBrain?style=social)](https://github.com/hithqd/ReasonBrain) 
- [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](http://arxiv.org/abs/2507.01496v1) [![GitHub Stars](https://img.shields.io/github/stars/wlaud1001/ReFlex?style=social)](https://github.com/wlaud1001/ReFlex) 
- [QC-OT: Optimal Transport with Quasiconformal Mapping](http://arxiv.org/abs/2507.01456v1)
- [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](http://arxiv.org/abs/2506.23676v1)
- [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](http://arxiv.org/abs/2506.23484v2) [![GitHub Stars](https://img.shields.io/github/stars/Suchenl/TAG-WM?style=social)](https://github.com/Suchenl/TAG-WM)
- [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](http://arxiv.org/abs/2506.23361v1) [![GitHub Stars](https://img.shields.io/github/stars/caiyuanhao1998/Open-OmniVCus?style=social)](https://github.com/caiyuanhao1998/Open-OmniVCus)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://caiyuanhao1998.github.io/project/OmniVCus) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/CaiYuanhao/OmniVCus)
- [Ovis-U1 Technical Report](http://arxiv.org/abs/2506.23044v2) [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis?style=social)](https://github.com/AIDC-AI/Ovis)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/AIDC-AI/Ovis-U1-3B) 
- [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](http://arxiv.org/abs/2506.22930v1) [![GitHub Stars](https://img.shields.io/github/stars/RaffeLegend/misinformation?style=social)](https://github.com/RaffeLegend/misinformation) 
- [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](http://arxiv.org/abs/2506.21839v2)
- [Controllable 3D Placement of Objects with Scene-Aware Diffusion Models](http://arxiv.org/abs/2506.21446v1)
- [Improving Diffusion-Based Image Editing Faithfulness via Guidance and Scheduling](http://arxiv.org/abs/2506.21045v1)
- [M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization](http://arxiv.org/abs/2506.20922v1)
- [FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing](http://arxiv.org/abs/2506.20911v1) [![GitHub Stars](https://img.shields.io/github/stars/tianyi-lab/FaSTAR?style=social)](https://github.com/tianyi-lab/FaSTAR) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/advaitgupta/CoSTAR)
- [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](http://arxiv.org/abs/2506.20652v1) [![GitHub Stars](https://img.shields.io/github/stars/editp23/editp23?style=social)](https://github.com/editp23/editp23)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://editp23.github.io) 
- [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](http://arxiv.org/abs/2506.20155v1)
- [SceneCrafter: Controllable Multi-View Driving Scene Editing](http://arxiv.org/abs/2506.19488v1)
- [Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency Models](http://arxiv.org/abs/2506.19103v1)
- [OmniGen2: Exploration to Advanced Multimodal Generation](http://arxiv.org/abs/2506.18871v2)
- [CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing](http://arxiv.org/abs/2506.18438v1)
- [Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction](http://arxiv.org/abs/2506.18290v1)
- [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](http://arxiv.org/abs/2506.16806v1)
- [Arch-Router: Aligning LLM Routing with Human Preferences](http://arxiv.org/abs/2506.16655v1)
- [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](http://arxiv.org/abs/2506.15903v1)
- [AttentionDrag: Exploiting Latent Correlation Knowledge in Pre-trained Diffusion Models for Image Editing](http://arxiv.org/abs/2506.13301v1)
- [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](http://arxiv.org/abs/2506.13827v1)
- [ComplexBench-Edit: Benchmarking Complex Instruction-Driven Image Editing via Compositional Dependencies](http://arxiv.org/abs/2506.12830v1)
- [SphereDrag: Spherical Geometry-Aware Panoramic Image Editing](http://arxiv.org/abs/2506.11863v1)
- [VINCIE: Unlocking In-context Image Editing from Video](http://arxiv.org/abs/2506.10941v1)
- [Edit360: 2D Image Edits to 3D Assets from Any Angle](http://arxiv.org/abs/2506.10507v2)
- [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](http://arxiv.org/abs/2506.09988v1)
- [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](http://arxiv.org/abs/2506.09740v1)
- [Ming-Omni: A Unified Multimodal Model for Perception and Generation](http://arxiv.org/abs/2506.09344v1)
- [Fine-Grained Spatially Varying Material Selection in Images](http://arxiv.org/abs/2506.09023v2)
- [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](http://arxiv.org/abs/2506.08991v1)
- [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](http://arxiv.org/abs/2506.08632v1)
- [Highly Compressed Tokenizer Can Generate Without Training](http://arxiv.org/abs/2506.08257v1)
- [PairEdit: Learning Semantic Variations for Exemplar-based Image Editing](http://arxiv.org/abs/2506.07992v1)
- [Diffusion Counterfactual Generation with Semantic Abduction](http://arxiv.org/abs/2506.07883v1)
- [DragNeXt: Rethinking Drag-Based Image Editing](http://arxiv.org/abs/2506.07611v1)
- [Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models](http://arxiv.org/abs/2506.06006v1)
- [FADE: Frequency-Aware Diffusion Model Factorization for Video Editing](http://arxiv.org/abs/2506.05934v1)
- [Towards Reliable Identification of Diffusion-based Image Manipulations](http://arxiv.org/abs/2506.05466v2)
- [SeedEdit 3.0: Fast and High-Quality Generative Image Editing](http://arxiv.org/abs/2506.05083v2)
- [Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking](http://arxiv.org/abs/2506.04879v1)

</details>

</details>

<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***InfEdit:*** *Inversion‑Free Image Editing with Natural Language*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04965.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sled-group.github.io/InfEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sled-group/InfEdit)


* **[CVPR 2024]** ***CrossSelfAttention:*** *Towards Understanding Cross and Self‑Attention in Stable Diffusion for Text‑Guided Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.03431.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing)


* **[CVPR 2024]** ***DAC:*** *Doubly Abductive Counterfactual Inference for Text‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.02981.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/xuesong39/DAC)


* **[CVPR 2024]** ***FoI:*** *Focus on Your Instruction: Fine‑grained and Multi‑instruction Image Editing by Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10113.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/guoqincode/Focus-on-Your-Instruction)


* **[CVPR 2024]** ***CDS:*** *Contrastive Denoising Score for Text‑guided Latent Diffusion Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18608.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hyelinnam.github.io/CDS/)


* **[CVPR 2024]** ***DragDiffusion:*** *Harnessing Diffusion Models for Interactive Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.14435.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujun-shi.github.io/projects/dragdiffusion.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Yujun-Shi/DragDiffusion)


* **[CVPR 2024]** ***DiffEditor:*** *Boosting Accuracy and Flexibility on Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.02583.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MC-E/DragonDiffusion)


* **[CVPR 2024]** ***FreeDrag:*** *Feature Dragging for Reliable Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.04684.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LPengYang/FreeDrag)


* **[CVPR 2024]** ***Learnable Regions:*** *Text‑Driven Image Editing via Learnable Regions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16432.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuanze-lin.me/LearnableRegions_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuanze-lin/Learnable_Regions)


* **[CVPR 2024]** ***LEDITS++:*** *Limitless Image Editing using Text‑to‑Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16711.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leditsplusplus-project.static.hf.space/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/editing-images/leditsplusplus)


* **[CVPR 2024]** ***SmartEdit:*** *Exploring Complex Instruction‑based Image Editing with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.06739.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuzhou914.github.io/SmartEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/SmartEdit)


* **[CVPR 2024]** ***Edit One for All:*** *Interactive Batch Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.10219.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thaoshibe.github.io/edit-one-for-all/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thaoshibe/edit-one-for-all)


* **[CVPR 2024]** ***DiffMorpher:*** *Unleashing the Capability of Diffusion Models for Image Morphing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07409.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kevin-thu.github.io/DiffMorpher_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Kevin-thu/DiffMorpher)


* **[CVPR 2024]** ***TiNO‑Edit:*** *Timestep and Noise Optimization for Robust Diffusion‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.11120.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SherryXTChen/TiNO-Edit)


* **[CVPR 2024]** ***Person in Place:*** *Generating Associative Skeleton‑Guidance Maps for Human‑Object Interaction Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Person_in_Place_Generating_Associative_Skeleton-Guidance_Maps_for_Human-Object_Interaction_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yangchanghee.github.io/Person-in-Place_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE)


* **[CVPR 2024]** ***Referring Image Editing:*** *Object‑level Image Editing via Referring Expressions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***Prompt Augmentation:*** *Prompt Augmentation for Self‑supervised Text‑guided Image Manipulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bodur_Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation_CVPR_2024_paper.pdf)


* **[CVPR 2024]** ***StyleFeatureEditor:*** *The Devil is in the Details — StyleFeatureEditor for Detail‑Rich StyleGAN Inversion and High Quality Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bobkov_The_Devil_is_in_the_Details_StyleFeatureEditor_for_Detail-Rich_StyleGAN_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/StyleFeatureEditor)


* **[ECCV 2024]** ***RegionDrag:*** *Fast Region‑Based Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.18247v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://visual-ai.github.io/regiondrag/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Visual-AI/RegionDrag)


* **[ECCV 2024]** ***TurboEdit:*** *Instant Text‑Based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.08332v1.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://betterze.github.io/TurboEdit/)


* **[ECCV 2024]** ***InstructGIE:*** *Towards Generalizable Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05018.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cr8br0ze.github.io/InstructGIE)


* **[ECCV 2024]** ***StableDrag:*** *Stable Dragging for Point‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04437.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://stabledrag.github.io/)


* **[ECCV 2024]** ***Eta Inversion:*** *Designing an Optimal Eta Function for Diffusion‑based Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02157.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/furiosa-ai/eta-inversion)


* **[ECCV 2024]** ***SwapAnything:*** *Enabling Arbitrary Object Swapping in Personalized Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04768.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://swap-anything.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/eric-ai-lab/swap-anything)


* **[ECCV 2024]** ***Guide‑and‑Rescale:*** *Self‑Guidance Mechanism for Effective Tuning‑Free Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08987.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/Guide-and-Rescale)


* **[ECCV 2024]** ***FreeDiff:*** *Progressive Frequency Truncation for Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00759.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Thermal-Dynamics/FreeDiff)


* **[ECCV 2024]** ***Lazy Diffusion Transformer:*** *Lazy Diffusion Transformer for Interactive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03436.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lazydiffusion.github.io/)


* **[ECCV 2024]** ***ByteEdit:*** *Boost, Comply and Accelerate Generative Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00359.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byte-edit.github.io/)


* **[ICLR 2024]** ***MGIE:*** *Guiding Instruction‑based Image Editing via Multimodal Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.17102.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mllm-ie.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/apple/ml-mgie)


* **[ICLR 2024]** ***SDE‑Drag:*** *The Blessing of Randomness — SDE Beats ODE in General Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.01410.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ml-gsai.github.io/SDE-Drag-demo/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ML-GSAI/SDE-Drag)


* **[ICLR 2024]** ***Motion Guidance:*** *Diffusion‑Based Image Editing with Differentiable Motion Estimators*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.18085.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dangeng.github.io/motion_guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dangeng/motion_guidance)


* **[ICLR 2024]** ***OIR:*** *Object‑Aware Inversion and Reassembly for Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.12149.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/OIR-Diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/OIR)


* **[ICLR 2024]** ***Noise Map Guidance:*** *Inversion with Spatial Context for Real Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.04625.pdf)


* **[AAAI 2024]** ***TIC:*** *Tuning‑Free Inversion‑Enhanced Control for Consistent Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.14611.pdf)


* **[AAAI 2024]** ***BARET:*** *Balanced Attention based Real Image Editing driven by Target‑text Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.05482.pdf)


* **[AAAI 2024]** ***CacheEdit:*** *Accelerating Text‑to‑Image Editing via Cache‑Enabled Sparse Diffusion Inference*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.17423.pdf)


* **[AAAI 2024]** ***High‑Fidelity Editing:*** *High‑Fidelity Diffusion‑based Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15707.pdf)


* **[AAAI 2024]** ***AdapEdit:*** *Spatio‑Temporal Guided Adaptive Editing Algorithm for Text‑Based Continuity‑Sensitive Image Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.08019.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AnonymousPony/adap-edit)


* **[AAAI 2024]** ***TexFit:*** *Text‑Driven Fashion Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28885)

</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [Edicho: Consistent Image Editing in the Wild](http://arxiv.org/abs/2412.21079v3) [![GitHub Stars](https://img.shields.io/github/stars/ant-research/edicho?style=social)](https://github.com/ant-research/edicho)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ant-research.github.io/edicho/)
- [Unforgettable Lessons from Forgettable Images: Intra-Class Memorability Matters in Computer Vision](http://arxiv.org/abs/2412.20761v4)
- [MADiff: Text-Guided Fashion Image Editing with Mask Prediction and Attention-Enhanced Diffusion](http://arxiv.org/abs/2412.20062v2)
- [DRDM: A Disentangled Representations Diffusion Model for Synthesizing Realistic Person Images](http://arxiv.org/abs/2412.18797v1)
- [Fashionability-Enhancing Outfit Image Editing with Conditional Diffusion Models](http://arxiv.org/abs/2412.18421v1)
- [The Superposition of Diffusion Models Using the Itô Density Estimator](http://arxiv.org/abs/2412.17762v2)
- [Mapping the Mind of an Instruction-based Image Editing using SMILE](http://arxiv.org/abs/2412.16277v1)
- [Diffusion-Based Conditional Image Editing through Optimized Inference with Guidance](http://arxiv.org/abs/2412.15798v1)
- [UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency](http://arxiv.org/abs/2412.15216v1)
- [Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion](http://arxiv.org/abs/2412.14462v2) [![GitHub Stars](https://img.shields.io/github/stars/KaKituken/affordance-aware-any?style=social)](https://github.com/KaKituken/affordance-aware-any)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kakituken.github.io/affordance-any.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/Kakituken/SAM-FB)
- [Text2Relight: Creative Portrait Relighting with Text Guidance](http://arxiv.org/abs/2412.13734v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://junukcha.github.io/project/text2relight/)
- [Prompt Augmentation for Self-supervised Text-guided Image Manipulation](http://arxiv.org/abs/2412.13081v1)
- [Unsupervised Region-Based Image Editing of Denoising Diffusion Models](http://arxiv.org/abs/2412.12912v1)
- [Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy](http://arxiv.org/abs/2412.12463v2)
- [Dual-Schedule Inversion: Training- and Tuning-Free Inversion for Real Image Editing](http://arxiv.org/abs/2412.11152v1)
- [BrushEdit: All-In-One Image Inpainting and Editing](http://arxiv.org/abs/2412.10316v3) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/BrushEdit?style=social)](https://github.com/TencentARC/BrushEdit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://liyaowei-stu.github.io/project/BrushEdit/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/TencentARC/BrushEdit)
- [Learning Complex Non-Rigid Image Edits from Multimodal Conditioning](http://arxiv.org/abs/2412.10219v1)
- [Context Canvas: Enhancing Text-to-Image Diffusion Models with Knowledge Graph-Based RAG](http://arxiv.org/abs/2412.09614v1)
- [FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers](http://arxiv.org/abs/2412.09611v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://fluxspace.github.io/)
- [PrEditor3D: Fast and Precise 3D Shape Editing](http://arxiv.org/abs/2412.06592v1)
- [MoViE: Mobile Diffusion for Video Editing](http://arxiv.org/abs/2412.06578v1) [![GitHub Stars](https://img.shields.io/github/stars/Qualcomm-AI-research/mobile-video-editing?style=social)](https://github.com/Qualcomm-AI-research/mobile-video-editing)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://qualcomm-ai-research.github.io/mobile-video-editing/)
- [GraPE: A Generate-Plan-Edit Framework for Compositional T2I Synthesis](http://arxiv.org/abs/2412.06089v2) [![GitHub Stars](https://img.shields.io/github/stars/dair-iitd/PixEdit?style=social)](https://github.com/dair-iitd/PixEdit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://dair-iitd.github.io/GraPE/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/aggr8/PixEdit-v1)
- [Text-to-3D Generation by 2D Editing](http://arxiv.org/abs/2412.05929v2)


</details>

</details>

<details>
<summary><h4>✨ 2023</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2023]** ***Diffusion Disentanglement:*** *Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wuqiuche.github.io/DiffusionDisentanglement-project-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)

* **[CVPR 2023]** ***SINE:*** *SINgle Image Editing with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhang-zx.github.io/SINE/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhang-zx/SINE)

* **[CVPR 2023]** ***Imagic:*** *Text-Based Real Image Editing with Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://imagic-editing.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/fffiloni/imagic-stable-diffusion)

* **[CVPR 2023]** ***InstructPix2Pix:*** *Learning to Follow Image Editing Instructions*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.timothybrooks.com/instruct-pix2pix/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/timothybrooks/instruct-pix2pix) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/timbrooks/instruct-pix2pix)

* **[CVPR 2023]** ***Null-text Inversion:*** *Null-text Inversion for Editing Real Images using Guided Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://null-text-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/prompt-to-prompt)

* **[ICCV 2023]** ***MasaCtrl:*** *Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ljzycmd.github.io/projects/MasaCtrl/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/MasaCtrl)

* **[ICCV 2023]** ***Local Prompt Mixing:*** *Localizing Object-level Shape Variations with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://orpatashnik.github.io/local-prompt-mixing/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/orpatashnik/local-prompt-mixing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/orpatashnik/local-prompt-mixing)

* **[ICLR 2022]** ***SDEdit:*** *Guided Image Synthesis and Editing with Stochastic Differential Equations*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2108.01073.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sde-image-editing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ermongroup/SDEdit)


</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

- [ZONE: Zero-Shot Instruction-Guided Local Editing](http://arxiv.org/abs/2312.16794v2) [![GitHub Stars](https://img.shields.io/github/stars/lsl001006/ZONE?style=social)](https://github.com/lsl001006/ZONE)
- [UniHuman: A Unified Model for Editing Human Images in the Wild](http://arxiv.org/abs/2312.14985v2) [![GitHub Stars](https://img.shields.io/github/stars/adobe-research/UniHuman?style=social)](https://github.com/adobe-research/UniHuman)
- [AppAgent: Multimodal Agents as Smartphone Users](http://arxiv.org/abs/2312.13771v2) [![GitHub Stars](https://img.shields.io/github/stars/TencentQQGYLab/AppAgent?style=social)](https://github.com/TencentQQGYLab/AppAgent)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://appagent-official.github.io/)
- [Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models](http://arxiv.org/abs/2312.12540v5) [![GitHub Stars](https://img.shields.io/github/stars/dvirsamuel/NewtonRaphsonInversion?style=social)](https://github.com/dvirsamuel/NewtonRaphsonInversion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://barakmam.github.io/rnri.github.io/)
- [MAG-Edit: Localized Image Editing in Complex Scenarios via Mask-Based Attention-Adjusted Guidance](http://arxiv.org/abs/2312.11396v2) [![GitHub Stars](https://img.shields.io/github/stars/HelenMao/MAG-Edit?style=social)](https://github.com/HelenMao/MAG-Edit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://mag-edit.github.io)
- [CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update](http://arxiv.org/abs/2312.10908v3) [![GitHub Stars](https://img.shields.io/github/stars/clova-tool/CLOVA-tool?style=social)](https://github.com/clova-tool/CLOVA-tool)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://clova-tool.github.io)
- [Latent Space Editing in Transformer-Based Flow Matching](http://arxiv.org/abs/2312.10825v1) [![GitHub Stars](https://img.shields.io/github/stars/dongzhuoyao/uspace?style=social)](https://github.com/dongzhuoyao/uspace)
- [VidToMe: Video Token Merging for Zero-Shot Video Editing](http://arxiv.org/abs/2312.10656v2) [![GitHub Stars](https://img.shields.io/github/stars/lixirui142/VidToMe?style=social)](https://github.com/lixirui142/VidToMe)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vidtome-diffusion.github.io)
- [Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation](http://arxiv.org/abs/2312.10113v1) [![GitHub Stars](https://img.shields.io/github/stars/guoqincode/Focus-on-Your-Instruction?style=social)](https://github.com/guoqincode/Focus-on-Your-Instruction)
- [SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds](http://arxiv.org/abs/2312.09246v1) [![GitHub Stars](https://img.shields.io/github/stars/silent-chen/Shap-Editor?style=social)](https://github.com/silent-chen/Shap-Editor)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://silent-chen.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/silent-chen/Shap-Editor)
- [Clockwork Diffusion: Efficient Generation With Model-Step Distillation](http://arxiv.org/abs/2312.08128v2) [![GitHub Stars](https://img.shields.io/github/stars/Qualcomm-AI-research/clockwork-diffusion?style=social)](https://github.com/Qualcomm-AI-research/clockwork-diffusion)
- [SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](http://arxiv.org/abs/2312.06739v1) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/SmartEdit?style=social)](https://github.com/TencentARC/SmartEdit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yuzhou914.github.io/projects/SmartEdit/)
- [Inversion-Free Image Editing with Natural Language](http://arxiv.org/abs/2312.04965v1) [![GitHub Stars](https://img.shields.io/github/stars/sled-group/InfEdit?style=social)](https://github.com/sled-group/InfEdit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sled-group.github.io/InfEdit/)
- [BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models](http://arxiv.org/abs/2312.02813v2) [![GitHub Stars](https://img.shields.io/github/stars/MCG-NJU/BIVDiff?style=social)](https://github.com/MCG-NJU/BIVDiff)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://bivdiff.github.io/)
- [Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training](http://arxiv.org/abs/2312.01663v1) [![GitHub Stars](https://img.shields.io/github/stars/hrz2000/CustomNeRF?style=social)](https://github.com/hrz2000/CustomNeRF)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://customnerf.github.io/)
- [Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D](http://arxiv.org/abs/2312.02190v2) [![GitHub Stars](https://img.shields.io/github/stars/adobe-research/DiffusionHandles?style=social)](https://github.com/adobe-research/DiffusionHandles)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://diffusionhandles.github.io/)
- [Adversarial Score Distillation: When score distillation meets GAN](http://arxiv.org/abs/2312.00739v2) [![GitHub Stars](https://img.shields.io/github/stars/2y7c3/ASD?style=social)](https://github.com/2y7c3/ASD)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://2y7c3.github.io/ASD/asd.html)
- [Motion-Conditioned Image Animation for Video Editing](http://arxiv.org/abs/2311.18827v1) [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/MoCA?style=social)](https://github.com/facebookresearch/MoCA)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://facebookresearch.github.io/MoCA/)
- [Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing](http://arxiv.org/abs/2311.18608v2) [![GitHub Stars](https://img.shields.io/github/stars/HyelinNAM/ContrastiveDenoisingScore?style=social)](https://github.com/HyelinNAM/ContrastiveDenoisingScore)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hyelinnam.github.io/CDS/)
- [On Exact Inversion of DPM-Solvers](http://arxiv.org/abs/2311.18387v1) [![GitHub Stars](https://img.shields.io/github/stars/smhongok/inv-dpm?style=social)](https://github.com/smhongok/inv-dpm)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://smhongok.github.io/inv-dpm.html)
- [COLE: A Hierarchical Generation Framework for Multi-Layered and Editable Graphic Design](http://arxiv.org/abs/2311.16974v2) [![GitHub Stars](https://img.shields.io/github/stars/CyberAgentAILab/OpenCOLE?style=social)](https://github.com/CyberAgentAILab/OpenCOLE)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/cyberagent/opencole)
- [LEDITS++: Limitless Image Editing using Text-to-Image Models](http://arxiv.org/abs/2311.16711v2) [![GitHub Stars](https://img.shields.io/github/stars/ml-research/ledits_pp?style=social)](https://github.com/ml-research/ledits_pp)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/leditsplusplus/project)
- [Text-Driven Image Editing via Learnable Regions](http://arxiv.org/abs/2311.16432v2) [![GitHub Stars](https://img.shields.io/github/stars/yuanze-lin/Learnable_Regions?style=social)](https://github.com/yuanze-lin/Learnable_Regions)
- [Self-correcting LLM-controlled Diffusion Models](http://arxiv.org/abs/2311.16090v1) [![GitHub Stars](https://img.shields.io/github/stars/tsunghan-wu/SLD?style=social)](https://github.com/tsunghan-wu/SLD)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://self-correcting-llm-diffusion.github.io/)
- [EditShield: Protecting Unauthorized Image Editing by Instruction-guided Diffusion Models](http://arxiv.org/abs/2311.12066v2) [![GitHub Stars](https://img.shields.io/github/stars/Allen-piexl/Editshield?style=social)](https://github.com/Allen-piexl/Editshield)
- [EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods](http://arxiv.org/abs/2310.02426v1) [![GitHub Stars](https://img.shields.io/github/stars/deep-ml-research/editval?style=social)](https://github.com/deep-ml-research/editval)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://deep-ml-research.github.io/editval/)
- [ImagenHub: Standardizing the evaluation of conditional image generation models](http://arxiv.org/abs/2310.01596v4) [![GitHub Stars](https://img.shields.io/github/stars/TIGER-AI-Lab/ImagenHub?style=social)](https://github.com/TIGER-AI-Lab/ImagenHub)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://tiger-ai-lab.github.io/ImagenHub/)
- [TokenFlow: Consistent Diffusion Features for Consistent Video Editing](http://arxiv.org/abs/2307.10373v3) [![GitHub Stars](https://img.shields.io/github/stars/omerbt/TokenFlow?style=social)](https://github.com/omerbt/TokenFlow)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://diffusion-tokenflow.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/weizmannscience/tokenflow)
- [FreeDrag: Feature Dragging for Reliable Point-based Image Editing](http://arxiv.org/abs/2307.04684v4) [![GitHub Stars](https://img.shields.io/github/stars/LPengYang/FreeDrag?style=social)](https://github.com/LPengYang/FreeDrag)
- [DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](http://arxiv.org/abs/2307.02421v2) [![GitHub Stars](https://img.shields.io/github/stars/MC-E/DragonDiffusion?style=social)](https://github.com/MC-E/DragonDiffusion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://mc-e.github.io/project/DragonDiffusion/)
- [LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance](http://arxiv.org/abs/2307.00522v1) [![GitHub Stars](https://img.shields.io/github/stars/camenduru/ledits-hf?style=social)](https://github.com/camenduru/ledits-hf)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/editing-images/project)
- [DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing](http://arxiv.org/abs/2306.14435v6) [![GitHub Stars](https://img.shields.io/github/stars/Yujun-Shi/DragDiffusion?style=social)](https://github.com/Yujun-Shi/DragDiffusion)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yujun-shi.github.io/projects/dragdiffusion.html)
- [MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing](http://arxiv.org/abs/2304.08465v1) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/MasaCtrl?style=social)](https://github.com/TencentARC/MasaCtrl)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ljzycmd.github.io/projects/MasaCtrl/)
- [MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing](http://arxiv.org/abs/2306.10012v3) [![GitHub Stars](https://img.shields.io/github/stars/OSU-NLP-Group/MagicBrush?style=social)](https://github.com/OSU-NLP-Group/MagicBrush)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://osu-nlp-group.github.io/MagicBrush/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/osunlp/MagicBrush)


</details>

</details>

[<small>⇧ Back to ToC</small>](#contents)


### <span id="unified">🔄 Unified Generation and Understanding</span>

<details>
<summary><h4>✨ 2025</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2025]** ***OmniFlow:*** Any‑to‑Any Generation with Multi‑Modal Rectified Flows<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.01169) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jacklishufan/OmniFlows)

* **[CVPR 2025]** ***TokenFlow:*** Unified image tokenizer for multimodal understanding and generation<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.03069) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://bytevisionlab.github.io/TokenFlow/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ByteVisionLab/TokenFlow) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/collections/ByteVisionLab/tokenflow-674ebd443b616dff1a634178)

* **[CVPR 2025]** ***UNIC‑Adapter:*** Unified Image‑instruction Adapter with Multi‑modal Transformer for Image Generation<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.18928) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/AIDC-AI/UNIC-Adapter) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIDC-AI/UNIC-Adapter) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/AIDC-AI/UNIC-Adapter)

* **[CVPR 2025]** ***MergeVQ:*** A Unified Framework for Visual Generation and Representation with Token Merging and Quantization<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://huggingface.co/papers/2504.00999) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/ApexGen-X/MergeVQ) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ApexGen-X/MergeVQ) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/papers/2504.00999)


* **[ICLR 2025]** ***Show‑o:*** One Single Transformer to Unify Multimodal Understanding and Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2408.12528) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/showlab/Show-o) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://huggingface.co/spaces/showlab/Show-o)

* **[ICLR 2025]** ***Transfusion:*** Predict the Next Token and Diffuse Images with One Multi‑Modal Model<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2408.11039) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/transfusion-pytorch)

* **[CVPRW 2025]** ***UniToken:*** Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.04423) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SxJyJay/UniToken)





</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

* [Selftok: Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning](https://arxiv.org/abs/2505.07538) [![GitHub Stars](https://img.shields.io/github/stars/selftok-team/SelftokTokenizer?style=social)](https://github.com/selftok-team/SelftokTokenizer)
* [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/TokLIP?style=social)](https://github.com/TencentARC/TokLIP)
* [Harmonizing Visual Representations for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2503.21979) [![GitHub Stars](https://img.shields.io/github/stars/wusize/Harmon?style=social)](https://github.com/wusize/Harmon)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/wusize/Harmon)
* [UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning](https://arxiv.org/abs/2503.21193)
* [Bifrost‑1: Bridging Multimodal LLMs and Diffusion Models with Patch‑level CLIP Latents](https://arxiv.org/abs/2508.05954) [![GitHub Stars](https://img.shields.io/github/stars/HL-hanlin/Bifrost-1?style=social)](https://github.com/HL-hanlin/Bifrost-1)
* [Qwen‑Image Technical Report](https://arxiv.org/abs/2508.02324) [![GitHub Stars](https://img.shields.io/github/stars/QwenLM/Qwen-Image?style=social)](https://github.com/QwenLM/Qwen-Image)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Qwen/Qwen-Image)
* [X‑Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again](https://arxiv.org/abs/2507.22058) [![GitHub Stars](https://img.shields.io/github/stars/X-Omni-Team/X-Omni?style=social)](https://github.com/X-Omni-Team/X-Omni)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/collections/X-Omni/x-omni-spaces-6888c64f38446f1efc402de7)
* [Ovis‑U1 Technical Report](https://arxiv.org/abs/2506.23044) [![GitHub Stars](https://img.shields.io/github/stars/AIDC-AI/Ovis-U1?style=social)](https://github.com/AIDC-AI/Ovis-U1)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B)
* [UniCode\$^2\$: Cascaded Large‑scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
* [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871) [![GitHub Stars](https://img.shields.io/github/stars/VectorSpaceLab/OmniGen2?style=social)](https://github.com/VectorSpaceLab/OmniGen2)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/OmniGen2/OmniGen2)
* [Vision as a Dialect: Unifying Visual Understanding and Generation via Text‑Aligned Representations](https://arxiv.org/abs/2506.18898) [![GitHub Stars](https://img.shields.io/github/stars/csuhan/Tar?style=social)](https://github.com/csuhan/Tar)
* [UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.17202) [![GitHub Stars](https://img.shields.io/github/stars/tliby/UniFork?style=social)](https://github.com/tliby/UniFork)
* [UniWorld: High‑Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/UniWorld-V1?style=social)](https://github.com/PKU-YuanGroup/UniWorld-V1)
* [Pisces: An Auto‑regressive Foundation Model for Image Understanding and Generation](https://arxiv.org/abs/2506.10395) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/UniWorld-V1?style=social)](https://github.com/PKU-YuanGroup/UniWorld-V1)
* [DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies](https://arxiv.org/abs/2503.14324) [![GitHub Stars](https://img.shields.io/github/stars/songweii/dualtoken?style=social)](https://github.com/songweii/dualtoken)
* [UniTok: A Unified Tokenizer for Visual Generation and Understanding](https://arxiv.org/abs/2502.20321) [![GitHub Stars](https://img.shields.io/github/stars/FoundationVision/UniTok?style=social)](https://github.com/FoundationVision/UniTok)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/FoundationVision/UniTok)
* [QLIP: Text‑Aligned Visual Tokenization Unifies Auto‑Regressive Multimodal Understanding and Generation](https://arxiv.org/abs/2502.05178) [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/QLIP?style=social)](https://github.com/NVlabs/QLIP)
* [TBAC‑UniImage: Unified Understanding and Generation by Ladder‑Side Diffusion Tuning](https://arxiv.org/abs/2508.08098) [![GitHub Stars](https://img.shields.io/github/stars/DruryXu/TBAC-UniImage?style=social)](https://github.com/DruryXu/TBAC-UniImage)
* [UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing](https://arxiv.org/abs/2507.23278)
* [Ming‑Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
* [OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.23661) [![GitHub Stars](https://img.shields.io/github/stars/wusize/OpenUni?style=social)](https://github.com/wusize/OpenUni)
* [BLIP3‑o: A Family of Fully Open Unified Multimodal Models—Architecture, Training and Dataset](https://arxiv.org/abs/2505.09568) [![GitHub Stars](https://img.shields.io/github/stars/JiuhaiChen/BLIP3o?style=social)](https://github.com/JiuhaiChen/BLIP3o)
* [Ming‑Lite‑Uni: Advancements in Unified Architecture for Natural Multimodal Interaction](https://arxiv.org/abs/2505.02471) [![GitHub Stars](https://img.shields.io/github/stars/inclusionAI/Ming?style=social)](https://github.com/inclusionAI/Ming)
* [Nexus‑Gen: A Unified Model for Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/abs/2504.21356) [![GitHub Stars](https://img.shields.io/github/stars/modelscope/Nexus-Gen?style=social)](https://github.com/modelscope/Nexus-Gen)
* [MetaQueries: Transfer between Modalities with MetaQueries](https://arxiv.org/abs/2504.06256)
* [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation](https://arxiv.org/abs/2506.15564) [![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/UniPic?style=social)](https://github.com/SkyworkAI/UniPic)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Skywork/UniPic)
* [MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO](https://arxiv.org/abs/2505.13031) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/MindOmni?style=social)](https://github.com/TencentARC/MindOmni)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/stevengrove/MindOmni)
* [UniFluid: Unified Autoregressive Visual Generation and Understanding with Continuous Tokens](https://arxiv.org/abs/2503.13436)
* [OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models](https://arxiv.org/abs/2503.08686) [![GitHub Stars](https://img.shields.io/github/stars/hustvl/OmniMamba?style=social)](https://github.com/hustvl/OmniMamba)
* [Janus‑Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)](https://github.com/deepseek-ai/Janus)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B)
* [VARGPT‑v1.1: Improve Visual Autoregressive Large Unified Model via Iterative Instruction Tuning and Reinforcement Learning](https://arxiv.org/abs/2504.02949) [![GitHub Stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT-v1.1?style=social)](https://github.com/VARGPT-family/VARGPT-v1.1)
* [ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement](https://arxiv.org/abs/2504.01934) [![GitHub Stars](https://img.shields.io/github/stars/illume-unified-mllm/ILLUME_plus?style=social)](https://github.com/illume-unified-mllm/ILLUME_plus)
* [SemHiTok: A Unified Image Tokenizer via Semantic‑Guided Hierarchical Codebook for Multimodal Understanding and Generation](https://arxiv.org/abs/2503.06764)
* [VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model](https://arxiv.org/abs/2501.12327) [![GitHub Stars](https://img.shields.io/github/stars/VARGPT-family/VARGPT?style=social)](https://github.com/VARGPT-family/VARGPT)
* [BAGEL: Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683) [![GitHub Stars](https://img.shields.io/github/stars/bytedance-seed/BAGEL?style=social)](https://github.com/bytedance-seed/BAGEL)
* [Mogao: An Omni Foundation Model for Interleaved Multi‑Modal Generation](https://arxiv.org/abs/2505.05472)
* [M2‑omni: Advancing Omni‑MLLM for Comprehensive Modality Support with Competitive Performance](https://arxiv.org/abs/2502.18778)


</details>

</details>


<details>
<summary><h4>✨ 2024</h4></summary>

<details>
<summary><h4>✅ Published Papers</h4></summary>

* **[CVPR 2024]** ***TokenFlow:*** Unified Image Tokenizer for Multimodal Understanding and Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.03069) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ByteFlow-AI/TokenFlow)

* **[CVPR 2024]** ***Unified‑IO 2:*** Scaling Autoregressive Multimodal Models with Vision, Language, Audio and Action<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2312.17172) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/allenai/unified-io-2)

* **[CVPR 2024]** ***Emu2:*** Generative Multimodal Models are In‑Context Learners<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2312.13286) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://baaivision.github.io/emu2) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/baaivision/Emu) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/Emu2)

* **[ICLR 2024]** ***LWM:*** World Model on Million‑Length Video And Language With Blockwise RingAttention<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2402.08268) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://largeworldmodel.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LargeWorldModel/LWM) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/LargeWorldModel/LWM-Text-1M)

* **[ICLR 2024]** ***VILA‑U:*** a Unified Foundation Model Integrating Visual Understanding and Generation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2409.04429) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hanlab.mit.edu/projects/vila-u) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mit-han-lab/vila-u) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/mit-han-lab/vila-u-7b-256)

* **[ICLR 2024]** ***DreamLLM:*** Synergistic Multimodal Comprehension and Creation<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2309.11499) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreamllm.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RunpeiDong/DreamLLM) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/RunpeiDong/dreamllm-7b-chat-v1.0)

* **[ICLR 2024]** ***LaVIT:*** Unified Language‑Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2309.04669) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/LaVIT-7B-v2)

* **[ICLR 2024]** ***Emu:*** Generative Pretraining in Multimodality<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2307.05222) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/baaivision/Emu) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](http://218.91.113.230:9002/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/Emu)

* **[ICLR 2024]** ***SEED‑LLaMA:*** Making LLaMA SEE and Draw with SEED Tokenizer<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2310.01218) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AILab-CVC/SEED) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ailab-cvc.github.io/seed) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AILab-CVC/SEED-LLaMA)

* **[ICML 2024]** ***Video‑LaVIT:*** Unified Video‑Language Pre‑training with Decoupled Visual‑Motional Tokenization<br>[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2402.03161) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-lavit.github.io) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/Video-LaVIT-v1)




</details>

<details>
<summary><h4>💡 Pre-Print Papers</h4></summary>

* [SynerGen‑VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding](https://arxiv.org/abs/2412.09604)
* [Liquid: Language Models are Scalable and Unified Multi‑modal Generators](https://arxiv.org/abs/2412.04332) [![GitHub Stars](https://img.shields.io/github/stars/FoundationVision/Liquid?style=social)](https://github.com/FoundationVision/Liquid)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Junfeng5/Liquid_demo)
* [Orthus: Autoregressive Interleaved Image‑Text Generation with Modality‑Specific Heads](https://arxiv.org/abs/2412.00127) [![GitHub Stars](https://img.shields.io/github/stars/zhijie-group/Orthus?style=social)](https://github.com/zhijie-group/Orthus)
* [MMAR: Towards Lossless Multi‑Modal Auto‑Regressive Probabilistic Modeling](httpsxiv.org/abs/2410.10798)
* [Emu3: Next‑Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) [![GitHub Stars](https://img.shields.io/github/stars/baaivision/Emu3?style=social)](https://github.com/baaivision/Emu3)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/BAAI/Emu3)
* [ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image‑Text Generation](https://arxiv.org/abs/2407.06135) [![GitHub Stars](https://img.shields.io/github/stars/GAIR-NLP/anole?style=social)](https://github.com/GAIR-NLP/anole)
* [Chameleon: Mixed‑Modal Early‑Fusion Foundation Models](https://arxiv.org/abs/2405.09818) [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/chameleon?style=social)](https://github.com/facebookresearch/chameleon)
* [MetaMorph: Multimodal Understanding and Generation via Instruction Tuning](https://arxiv.org/abs/2412.14164) [![GitHub Stars](https://img.shields.io/github/stars/facebookresearch/metamorph?style=social)](https://github.com/facebookresearch/metamorph)
* [ILLUME: Illuminating Your LLMs to See, Draw, and Self‑Enhance](https://arxiv.org/abs/2412.06673)
* [PUMA: Empowering Unified MLLM with Multi‑granular Visual Generation](https://arxiv.org/abs/2410.13861) [![GitHub Stars](https://img.shields.io/github/stars/rongyaofang/PUMA?style=social)](https://github.com/rongyaofang/PUMA)
* [Mini‑Gemini: Mining the Potential of Multi‑modality Vision Language Models](https://arxiv.org/abs/2403.18814) [![GitHub Stars](https://img.shields.io/github/stars/dvlab-research/MGM?style=social)](https://github.com/dvlab-research/MGM)
* [MM‑Interleaved: Interleaved Image‑Text Generative Modeling via Multi‑modal Feature Synchronizer](https://arxiv.org/abs/2401.10208) [![GitHub Stars](https://img.shields.io/github/stars/OpenGVLab/MM-Interleaved?style=social)](https://github.com/OpenGVLab/MM-Interleaved)
* [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)](https://github.com/deepseek-ai/Janus)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/deepseek-ai/Janus-1.3B)
* [MUSE‑VL: Modeling Unified VLM through Semantic Discrete Encoding](https://arxiv.org/abs/2411.17762)
* [LMFusion: Adapting Pretrained Language Models for Multimodal Generation](https://arxiv.org/abs/2412.15188)
* [MonoFormer: One Transformer for Both Diffusion and Autoregression](https://arxiv.org/abs/2409.16280) [![GitHub Stars](https://img.shields.io/github/stars/MonoFormer/MonoFormer?style=social)](https://github.com/MonoFormer/MonoFormer)
* [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) [![GitHub Stars](https://img.shields.io/github/stars/deepseek-ai/Janus?style=social)](https://github.com/deepseek-ai/Janus)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B)
* [Spider: Any‑to‑Many Multimodal LLM](https://arxiv.org/abs/2411.09439) [![GitHub Stars](https://img.shields.io/github/stars/Layjins/Spider?style=social)](https://github.com/Layjins/Spider)
* [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) [![GitHub Stars](https://img.shields.io/github/stars/MIO-Team/MIO?style=social)](https://github.com/MIO-Team/MIO)
* [X‑VILA: Cross‑Modality Alignment for Large Language Model](https://arxiv.org/abs/2405.19335)
* [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) [![GitHub Stars](https://img.shields.io/github/stars/OpenMOSS/AnyGPT?style=social)](https://github.com/OpenMOSS/AnyGPT)



</details>

</details>

---

## <span id="datasets">🗂️ Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **Oxford-120 Flowers**| 2008 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) |
| **CUB-200-2011** | 2011 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://resolver.caltech.edu/CaltechCSTR:2010.001) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](http://www.vision.caltech.edu/datasets/cub_200_2011/) |
| **MS COCO** | 2014 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1405.0312) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://cocodataset.org/#home) |
| **LAION-5B** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.08402) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://laion.ai/blog/laion-5b/) |
| **DiffusionDB** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.14896) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://poloclub.github.io/diffusiondb/) |
| **T2I‑FactualBench** | 2024 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.04300v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/Safeoffellow/T2I-FactualBench) |
| **EvalMuse‑40K** | 2024 | Text, Image, Rating | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.18150v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://shh-han.github.io/EvalMuse-project/) |
| **T2I‑CompBench++** | 2025 | Text | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2307.06350v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://karine-h.github.io/T2I-CompBench/) |
| **Gecko Evaluation** | 2025 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openreview.net/forum?id=Im2neAMlre) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/google-deepmind/gecko_benchmark_t2i) |
| **T2I‑ReasonBench** | 2025 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2508.17472v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/KaiyueSun98/T2I-ReasonBench) |
| **ImageNet** | 2009 | Image, Class Label       | Class-Conditional Generation       |                    [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf)                    |          [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.image-net.org/)         |
| **CIFAR-10** | 2009 | Image, Class Label       | Conditional Image Generation (Class-Conditional)       |                      [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)                     | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.cs.toronto.edu/~kriz/cifar.html) |
| **LSUN** | 2015 | Image, Class/Scene Label | Conditional Image Generation (Class-Conditional) |                                     [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1506.03365)                                    |      [![Website](https://img.shields.io/badge/Info-Link-orange?style=for-the-badge)](https://complexity.cecs.ucf.edu/lsun/)     |
| **7Bench** | 2025 | Text, Image, Bounding Box | Conditional Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2508.12919v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/Elizzo/7Bench) |
| **EditInspector** | 2024 | Text, Image, Human-Annotated Brush | Conditional Image Generation, Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.09988v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/editinspector/EditInspector) |
| **Cityscapes** | 2016 | Image, Segmentation Map  | Conditional Image Generation (Segmentation-based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html) |     [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.cityscapes-dataset.com/)     |
| **ADE20K** | 2017 | Image, Segmentation Map  | Conditional Image Generation (Segmentation-based) |               [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://people.csail.mit.edu/bzhou/publication/scene-parse-camera-ready.pdf)               |        [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://ade20k.csail.mit.edu/)        |
| **COCO-Stuff** | 2017 | Image, Segmentation Map | Conditional Image Generation (Segmentation-based) |                                     [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1612.03716)                                    |    [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/nightrome/cocostuff)   |
| **EditVal** | 2023 | Text, Image | Image Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2310.02426v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://deep-ml-research.github.io/editval/) |
| **MagicBrush** | 2023 | Text, Image          | Image Editing |                                                        [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2306.10012)                                                        |              [![Website](https://img.shields.io/badge/Project-Link-orange?style=for-the-badge)](https://osu-nlp-group.github.io/MagicBrush/)              |
| **ImgEdit** | 2025 | Text, Image | Image Editing       |                                                       [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/html/2505.20275v1)                                                      |                [![Website](https://img.shields.io/badge/GitHub-Link-orange?style=for-the-badge)](https://github.com/PKU-YuanGroup/ImgEdit)                |
| **Six‑CD** | 2025 | Text, Image | Image Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.html) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/Artanisax/Six-CD) |
| **LMM4Edit (EBench‑18K)** | 2025 | Text, Question-Answer Pair, Image | Image Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2507.16193v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/IntMeGroup/LMM4Edit) |
| **InstructPix2Pix Dataset** | 2022 | Text, Image    | Image Editing (Instruction-Based)            |                                                 [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.timothybrooks.com/instruct-pix2pix)                                                 | [![Website](https://img.shields.io/badge/Dataset-HF-orange?style=for-the-badge)](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered) |
| **HIVE** | 2024 | Text, Image, Human Feedback    | Image Editing (Instruction-Based)            |                                                        [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2303.09618)                                                        |                   [![Website](https://img.shields.io/badge/GitHub-Link-orange?style=for-the-badge)](https://github.com/salesforce/HIVE)                   |
| **HQ-Edit** | 2024 | Text, Image          |   Image Editing (Instruction-Based)        |                                                        [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2404.09990)                                                        |                [![Website](https://img.shields.io/badge/Project-Link-orange?style=for-the-badge)](https://thefllood.github.io/HQEdit_web/)                |
| **AnyEdit** | 2025 | Text, Image      | Image Editing (Instruction-Based)  | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.pdf) |                   [![Website](https://img.shields.io/badge/GitHub-Link-orange?style=for-the-badge)](https://github.com/DCDmllm/AnyEdit)                   |
| **HQ‑Edit** | 2025 | Text, Image | Image Editing (Instruction-Based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openreview.net/forum?id=mZptYYttFj) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://thefllood.github.io/HQEdit_web/) |
| **OmniEdit** | 2025 | Text, Image | Image Editing (Instruction-Based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openreview.net/forum?id=Hlm0cga0sv) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://tiger-ai-lab.github.io/OmniEdit/) |
| **VectorEdits** | 2025 | Text, SVG Image | Image Editing (Instruction-Based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.15903v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/mikronai/VectorEdits) |
| **ComplexBench‑Edit** | 2025 | Text (Multi-Step Instruction), Image | Image Editing (Instruction-Based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.12830v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/llllly26/ComplexBench-Edit) |
| **GPT‑IMAGE‑EDIT‑1.5M** | 2025 | Text, Image | Image Editing (Instruction-Based) | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2507.21033v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://ucsc-vlaa.github.io/GPT-Image-Edit/) |
| **CustomConcept-101** | 2022 | Text, Image      | Personalized Image Generation (Multi-Subject)           | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2212.04488) | [![Website](https://img.shields.io/badge/Dataset-Link-orange?style=for-the-badge)](https://www.cs.cmu.edu/~custom-diffusion/dataset.html) |
| **DreamEditBench** | 2023 | Text, Image          | Personalized Image Generation             | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2306.12624) |         [![Website](https://img.shields.io/badge/Project-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/tianleliphoebe/DreamEditBench)         |
| **DreamBench++** | 2024 | Text, Image | Personalized Image Generation    | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2406.16855) |           [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://dreambenchplus.github.io/)           |



[<small>⇧ Back to ToC</small>](#contents)

---

## <span id="about-us">🎓 About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Text-to-Image Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

➡️ **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

「应达学术」(QuenithAI) 是一家由顶尖研究者组成，致力于为全球高校学生提供高质量1V1科研辅导的专业机构。我们的使命是帮助学生培养出色卓越的科研技能，在顶级会议和期刊上发表自己的成果。

维护一个GitHub调研仓库需要巨大的精力，正如完成一篇高质量的论文一样，离不开专注的投入和专业的指导。如果您希望在自己的研究项目中，获得来自顶尖学者的一对一支持，我们诚邀您与我们取得联系。

➡️ **欢迎通过 [微信](assets/wechat.jpg) 或 [邮件](mailto:your.email@example.com) 联系我们，开启您的科研之旅。**


[<small>⇧ Back to ToC</small>](#contents)

---



## <span id="contributing">🤝 Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
