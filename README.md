<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Text-to-Image Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Text-to-Image Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/T2I-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (Â∫îËææÂ≠¶ÊúØ)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of Text-to-Image generation.
>
> Your contributions are also vital‚Äîfeel free to [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> Êú¨‰ªìÂ∫ìÁî± **„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI)** ÁöÑ‰∏ÄÁ∫øÁßëÁ†îÂØºÂ∏àÂõ¢ÈòüÂÄæÂäõÊâìÈÄ†Âπ∂ÊåÅÁª≠Áª¥Êä§ÔºåÊó®Âú®‰∏∫ÊÇ®ÂëàÁé∞ÊñáÁîüÂõæÈ¢ÜÂüüÊúÄÂÖ®Èù¢„ÄÅÊúÄÂâçÊ≤øÁöÑËÆ∫Êñá„ÄÇ
>
> ÊÇ®ÁöÑË¥°ÁåÆÂØπÊàë‰ª¨ÂíåÁ§æÂå∫Êù•ËØ¥Ëá≥ÂÖ≥ÈáçË¶Å‚Äî‚ÄîÊàë‰ª¨ËØöÈÇÄÊúâÂøó‰πãÂ£´ÈÄöËøá [open an issue](https://github.com/QuenithAI/T2I-Generation-Paper-List/issues) Êàñ [submit a pull request](https://github.com/QuenithAI/T2I-Generation-Paper-List/pulls) Êù•Êàê‰∏∫Ëøô‰∏™È°πÁõÆÁöÑÂêà‰ΩúËÄÖ‰πã‰∏ÄÔºåÊúüÂæÖÊÇ®ÁöÑÂä†ÂÖ•ÔºÅ
> 
> Â¶ÇÊûúÊÇ®Âú®ÂÜ≤Âà∫ÁßëÁ†îÈ°∂‰ºöÁöÑÈÅìË∑Ø‰∏äÈúÄË¶Å‰∏ì‰∏öÁöÑ1V1ÊåáÂØºÔºåÊ¨¢Ëøé**ÈÄöËøá[ÂæÆ‰ø°](assets/wechat.jpg)Êàñ[ÈÇÆ‰ª∂](mailto:christzhaung@gmail.com)ËÅîÁ≥ªÊàë‰ª¨**„ÄÇ


<details>
<summary><strong>‚ö° Latest Updates</strong></summary>

- **(Aug 21th, 2025)**: Add a new direction: [üé® Personalized Image Generation](#personalized).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

## <span id="contents">üìö Table of Contents</span>
- [üìö Table of Contents](#-table-of-contents)
- [üìú Papers \& Models](#-papers--models)
  - [‚úçÔ∏è Survey Papers](#Ô∏è-survey-papers)
  - [üñºÔ∏è Text-to-Image Generation](#Ô∏è-text-to-image-generation)
  - [üïπÔ∏è Conditional Image Generation](#Ô∏è-conditional-image-generation)
  - [üé® Personalized Image Generation](#-personalized-image-generation)
  - [‚úÇÔ∏è Image Editing](#Ô∏è-image-editing)
- [üóÇÔ∏è Datasets](#Ô∏è-datasets)
- [üéì About Us](#-about-us)
- [ü§ù Contributing](#-contributing)

---

## <span id="papers">üìú Papers & Models</span>

### <span id="survey">‚úçÔ∏è Survey Papers</span>



[<small>‚áß Back to ToC</small>](#contents)

### <span id="t2i">üñºÔ∏è Text-to-Image Generation</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

* **[CVPR 2025]** ***PreciseCam:*** *Precise Camera Control for Text-to-Image Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.12910)
  [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://graphics.unizar.es/projects/PreciseCam2024/)
  [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/edurnebernal/PreciseCam)
  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/edurnebb/PreciseCam)

* **[CVPR 2025]** ***Type‚ÄëR:*** *Automatically Retouching Typos for Text‚Äëto‚ÄëImage Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18159) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CyberAgentAILab/Type-R) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cyberagent/type-r)

* **[CVPR 2025]** ***Compass‚ÄØControl:*** *Multi Object Orientation Control for Text‚Äëto‚ÄëImage Generation*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.06752) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/CompassControl/)

* **[CVPR 2025]** ***Generative¬†Photography:*** *Scene‚ÄëConsistent Camera Control for Realistic Text‚Äëto‚ÄëImage Synthesis*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://generative-photography.github.io/project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pandayuanyu/generative-photography) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/pandaphd/generative_photography)

* **[CVPR 2025]** ***One‚ÄëWay¬†Ticket:*** *Time‚ÄëIndependent Unified Encoder for Distilling Text‚Äëto‚ÄëImage Diffusion Models*<br>
   [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/32579) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sen-mao/Loopfree) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/senmaonk/loopfree-sd2.1-base)

* **[CVPR 2025]** ***Text‚ÄØEmbedding is Not All You Need:*** *Attention Control for Text‚Äëto‚ÄëImage Semantic Alignment with Text Self‚ÄëAttention Maps*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.15236) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://t-sam-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/t-sam-diffusion/code)

* **[CVPR 2025]** ***Towards¬†Uncertainty:*** *Understanding and Quantifying Uncertainty for Text‚Äëto‚ÄëImage Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03178) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ENSTA-U2IS-AI/Uncertainty_diffusion)

* **[CVPR 2025]** ***Responsible¬†Diffusion:*** *Plug‚Äëand‚ÄëPlay Interpretable Responsible Text‚Äëto‚ÄëImage Generation via Dual‚ÄëSpace Multi‚Äëfaceted Concept Control*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.18324) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://basim-azam.github.io/responsiblediffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/basim-azam/responsiblediffusion)

* **[CVPR 2025]** ***Make¬†It¬†Count:*** *Text‚Äëto‚ÄëImage Generation with an Accurate Number of Objects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.10210) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://make-it-count-paper.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Litalby1/make-it-count)

* **[CVPR 2025]** ***MCCD:*** *Multi‚ÄëAgent Collaboration‚Äëbased Compositional Diffusion for Complex Text‚Äëto‚ÄëImage Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2505.02648)

* **[CVPR 2025]** ***Debias‚ÄëSD:*** *Rethinking Training for De‚Äëbiasing Text‚Äëto‚ÄëImage Generation: Unlocking the Potential of Stable Diffusion*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.12692)

* **[CVPR 2025]** ***ShapeWords:*** *Guiding Text‚Äëto‚ÄëImage Synthesis with 3D Shape‚ÄëAware Prompts*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.02912) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lodurality.github.io/shapewords/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lodurality/shapewords_paper_code) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/dmpetrov/shapewords)

* **[CVPR 2025]** ***SnapGen:*** *Taming High‚ÄëResolution Text‚Äëto‚ÄëImage Models for Mobile Devices with Efficient Architectures and Training*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.09619) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapgen/)

* **[CVPR 2025]** ***STORM:*** *Spatial Transport Optimization by Repositioning Attention Map for Training‚ÄëFree Text‚Äëto‚ÄëImage Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.22168) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://micv-yonsei.github.io/storm2025/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MICV-yonsei/STORM)

* **[CVPR 2025]** ***Focus‚ÄëN‚ÄëFix:*** *Region‚ÄëAware Fine‚ÄëTuning for Text‚Äëto‚ÄëImage Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2501.06481)

* **[CVPR 2025]** ***SILMM:*** *Self‚ÄëImproving Large Multimodal Models for Compositional Text‚Äëto‚ÄëImage Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05818) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silmm.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/SILMM)

* **[CVPR 2025]** ***GLoCE:*** *Localized Concept Erasure for Text‚Äëto‚ÄëImage Diffusion Models Using Training‚ÄëFree Gated Low‚ÄëRank Adaptation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.12356) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gl-oce.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/GLoCE)

* **[CVPR 2025]** ***Self‚ÄëCross Guidance:*** *Self‚ÄëCross Diffusion Guidance for Text‚Äëto‚ÄëImage Synthesis of Similar Subjects*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.18936) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://selfcross-guidance.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mengtang-lab/selfcross-guidance)

* **[CVPR 2025]** ***Noise¬†Diffusion:*** *Enhancing Semantic Faithfulness in Text‚Äëto‚ÄëImage Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bomingmiao/NoiseDiffusion)

* **[CVPR 2025]** ***PromptSampler:*** *Learning to Sample Effective and Diverse Prompts for Text‚Äëto‚ÄëImage Generation*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.07838) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dbsxodud-11/PAG)

* **[CVPR 2025]** ***STEREO:*** *A Two‚ÄëStage Framework for Adversarially Robust Concept Erasing from Text‚Äëto‚ÄëImage Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.16807) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/koushiksrivats/robust-concept-erasing)

* **[CVPR 2025]** ***MinorityPrompt:*** *Minority‚ÄëFocused Text‚Äëto‚ÄëImage Generation via Prompt Optimization*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16503) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/soobin-um/MinorityPrompt)

* **[CVPR 2025]** ***DistillT5:*** *Scaling Down Text Encoders of Text‚Äëto‚ÄëImage Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19897) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LifuWang-66/DistillT5)

* **[CVPR 2025]** ***TIU:*** *The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text‚Äëto‚ÄëImage Diffusion Models*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NGK2110/TIU)

* **[CVPR 2025]** ***Fuse‚ÄëDiT:*** *Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text‚Äëto‚ÄëImage Synthesis*<br>  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tang-bd/fuse-dit) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ooutlierr/fuse-dit)

* **[CVPR 2025]** **Detect‚Äëand‚ÄëGuide:** *Self‚Äëregulation of Diffusion Models for Safe Text‚Äëto‚ÄëImage Generation via Guideline Token Optimization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.15197)

* **[CVPR 2025]** **Multi‚ÄëGroup¬†T2I:** *Multi‚ÄëGroup Proportional Representations for Text‚Äëto‚ÄëImage Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sangwon-jung94/mpr-t2i)

* **[CVPR 2025]** **VODiff:** *Controlling Object Visibility Order in Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/???) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dliang293/VODiff)

* **[ICLR 2025]** **Improving¬†Long‚ÄëText Alignment:** *Improving Long‚ÄëText Alignment for Text‚Äëto‚ÄëImage Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2ZK8zyIt7o) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/luping-liu/LongAlign)

* **[ICLR 2025]** **ITTA:** *Information¬†Theoretic Text‚Äëto‚ÄëImage Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Ugs2W5XFFo) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Chao0511/mitune)

* **[ICLR 2025]** **Meissonic:** *Revitalizing Masked Generative Transformers for Efficient High‚ÄëResolution Text‚Äëto‚ÄëImage Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=GJsuYHhAga) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/meissonic/home) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/viiika/Meissonic) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/MeissonFlow/Meissonic)

* **[ICLR 2025]** **PaRa:** *Personalizing Text‚Äëto‚ÄëImage Diffusion via Parameter Rank Reduction*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=KZgo2YQbhc)

* **[ICLR 2025]** **Fluid:** *Scaling Autoregressive Text‚Äëto‚Äëimage Generative Models with Continuous Tokens*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=jQP5o1VAVc)

* **[ICLR 2025]** **Prompt‚ÄëPruning:** *Not All Prompts Are Made Equal ‚Äì Prompt‚Äëbased Pruning of Text‚Äëto‚ÄëImage Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=3BhZCfJ73Y) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rezashkv/diffusion_pruning) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rezashkv/diffusion_pruning)

* **[ICLR 2025]** **Denoising¬†AR¬†Transformers:** *Denoising Autoregressive Transformers for Scalable Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=amDkNPVWcn)

* **[ICLR 2025]** **Progressive¬†Compositionality:** *Progressive Compositionality in Text‚Äëto‚ÄëImage Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S85PP4xjFD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evansh666.github.io/EvoGen_Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/evansh666/EvoGen)

* **[ICLR 2025]** **Classifier¬†Scores:** *Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text‚Äëto‚ÄëImage Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hUdLs6TqZL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://srvcodes.github.io/continual_personalization)

* **[ICLR 2025]** **Engagement:** *Measuring and Improving Engagement of Text‚Äëto‚ÄëImage Generation Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=TmCcNuo03f) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://behavior-in-the-wild.github.io/image-engagement)

* **[ICLR 2025]** **Residual¬†Gate¬†Eraser:** *Concept Pinpoint Eraser for Text‚Äëto-image Diffusion Models via Residual Attention Gate*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZRDhBwKs7l) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Hyun1A/CPE)

* **[ICLR 2025]** **Random¬†Seeds:** *Enhancing Compositional Text‚Äëto‚ÄëImage Generation with Reliable Random Seeds*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=5BSlakturs) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/doub7e/Reliable-Random-Seeds)

* **[ICLR 2025]** **One‚ÄëPrompt‚ÄëOne‚ÄëStory:** *Free‚ÄëLunch Consistent Text‚Äëto‚ÄëImage Generation Using a Single Prompt*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=cD1kl2QKv1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byliutao.github.io/1Prompt1Story.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/byliutao/1Prompt1Story)

* **[ICLR 2025]** **You¬†Only¬†Sample¬†Once:** *Taming One‚ÄëStep Text‚Äëto‚ÄëImage Synthesis by Self‚ÄëCooperative Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=T7bmHkwzS6) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yoso-t2i.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Luo-Yihong/YOSO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Luo-Yihong/yoso_pixart512)

* **[ICLR 2025]** **Copyright¬†Revisiting:** *Rethinking Artistic Copyright Infringements in the Era of Text‚Äëto‚ÄëImage Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=0OTVNEm9N4)

* **[ICLR 2025]** **Concept¬†Combination¬†Erasing:** *Erasing Concept Combination from Text‚Äëto‚ÄëImage Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=OBjF5I4PWg)

* **[ICLR 2025]** **Cross‚ÄëAttention¬†Patterns:** *Cross‚ÄëAttention Head Position Patterns Can Align with Human Visual Concepts in Text‚Äëto‚ÄëImage Generative Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=1vggIT5vvj) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SNU-DRL/HRV)

* **[ICLR 2025]** **TIGeR:** *Unifying Text‚Äëto‚ÄëImage Generation and Retrieval with Large Multimodal Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mr2icR6dpD) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LgQu/TIGeR)

* **[ICLR 2025]** **DGQ:** *Distribution‚ÄëAware Group Quantization for Text‚Äëto‚ÄëImage Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ZyNEr7Xw5L) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ugonfor/DGQ)

* **[ICLR 2025]** **Jacobi¬†Decoding:** *Accelerating Auto‚Äëregressive Text‚Äëto‚ÄëImage Generation with Training‚Äëfree Speculative Jacobi Decoding*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LZfjxvqw0N) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tyshiwo1/Accelerating-T2I-AR-with-SJD)

* **[ICLR 2025]** **PT‚ÄëT2I/V:** *An Efficient Proxy‚ÄëTokenized Diffusion Transformer for Text‚Äëto‚ÄëImage/Video Task*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=lTrrnNdkOX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://360cvgroup.github.io/Qihoo-T2X/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/360CVGroup/Qihoo-T2X)

* **[ICLR 2025]** **Gecko¬†Evaluation:** *Revisiting Text‚Äëto‚ÄëImage Evaluation with Gecko: on Metrics, Prompts, and Human Rating*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Im2neAMlre) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google-deepmind/gecko_benchmark_t2i)

* **[ICLR 2025]** **SANA:** *Efficient High‚ÄëResolution Text‚Äëto‚ÄëImage Synthesis with Linear Diffusion Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=N8Oj1XhtYZ) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/Sana) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NVlabs/Sana) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_diffusers)

* **[ICLR 2025]** **Rectified¬†Flow:** *Text‚Äëto‚ÄëImage Rectified Flow as Plug‚Äëand‚ÄëPlay Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=SzPZK856iI) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yangxiaofeng/rectified_flow_prior)

* **[ICLR 2025]** **Human¬†Feedback¬†Filtering:** *Automated Filtering of Human Feedback Data for Aligning Text‚Äëto‚ÄëImage Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=8jvVNPHtVJ)

* **[ICLR 2025]** **SAFREE:** *Training‚ÄëFree and Adaptive Guard for Safe Text‚Äëto‚ÄëImage and Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=hgTFotBRKl) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://safree-safe-t2i-t2v.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jaehong31/SAFREE)

* **[ICLR 2025]** **IterComp:** *Iterative Composition‚ÄëAware Feedback Learning from Model Gallery for Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=4w99NAikOE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangLing0818/IterComp) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/comin/IterComp)

* **[ICLR 2025]** **ScImage:** *How good are multimodal large language models at scientific text‚Äëto‚Äëimage generation?*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ugyqNEOjoU) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/casszhao/ScImage)

* **[ICLR 2025]** **Score¬†Distillation:** *Guided Score Identity Distillation for Data‚ÄëFree One‚ÄëStep Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=HMVDiaWMwM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mingyuanzhou/SiD-LSG) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/UT-Austin-PML/SiD-LSG)

* **[ICLR 2025]** **Causal¬†Variation:** *Evaluating Semantic Variation in Text‚Äëto‚ÄëImage Synthesis: A Causal Perspective*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=NWb128pSCb) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhuxiangru/SemVarBench)


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2024]** ***DistriFusion:*** *Distributed Parallel Inference for High-Resolution Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.19481.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mit-han-lab/distrifuser)

* **[CVPR 2024]** ***InstanceDiffusion:*** *Instance-level Control for Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.03290.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/frank-xwang/InstanceDiffusion)

* **[CVPR 2024]** ***ECLIPSE:*** *A Resource-Efficient Text-to-Image Prior for Image Generations*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04655.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/eclipse-t2i/eclipse-inference) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://eclipse-t2i.vercel.app/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2)

* **[CVPR 2024]** ***Instruct-Imagen:*** *Image Generation with Multi-modal Instruction*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.01952.pdf)

* **[CVPR 2024]** ***Continuous 3D Words:*** *Learning Continuous 3D Words for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.08654.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ttchengab/continuous_3d_words_code/)

* **[CVPR 2024]** ***HanDiffuser:*** *Text-to-Image Generation With Realistic Hand Appearances*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01693.pdf)

* **[CVPR 2024]** ***Rich Human Feedback:*** *Rich Human Feedback for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10240.pdf)

* **[CVPR 2024]** ***MarkovGen:*** *Structured Prediction for Efficient Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10997.pdf)

* **[CVPR 2024]** ***Customization Assistant:*** *Customization Assistant for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.03045.pdf)

* **[CVPR 2024]** ***ADI:*** *Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.15841.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adi-t2i.github.io/ADI/)

* **[CVPR 2024]** ***UFOGen:*** *You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.09257.pdf)

* **[CVPR 2024]** ***Interpret Diffusion:*** *Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17216.pdf)

* **[CVPR 2024]** ***Tailored Visions:*** *Enhancing Text-to-Image Generation with Personalized Prompt Rewriting*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.08129.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zzjchen/Tailored-Visions)

* **[CVPR 2024]** ***CoDi:*** *Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.01407.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fast-codi.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fast-codi/CoDi) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/MKFMIKU/CoDi)

* **[CVPR 2024]** ***Arbitrary‚ÄëScale Diffusion:*** *Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.10255.pdf)

* **[CVPR 2024]** ***Human-Centric Priors:*** *Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05239)

* **[CVPR 2024]** ***ElasticDiffusion:*** *Training-free Arbitrary Size Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18822) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://elasticdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MoayedHajiAli/ElasticDiffusion-official)

* **[CVPR 2024]** ***CosmicMan:*** *A Text-to-Image Foundation Model for Humans*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01294) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cosmicman-cvpr2024.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cosmicman-cvpr2024/CosmicMan)

* **[CVPR 2024]** ***PanFusion:*** *Taming Stable Diffusion for Text to 360¬∞ Panorama Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07949) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chengzhag.github.io/publication/panfusion) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chengzhag/PanFusion)

* **[CVPR 2024]** ***Intelligent Grimm:*** *Open-ended Visual Storytelling via Latent Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.00973) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://haoningwu3639.github.io/StoryGen_Webpage/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/haoningwu3639/StoryGen)

* **[CVPR 2024]** ***Scalability:*** *On the Scalability of Diffusion-based Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02883)

* **[CVPR 2024]** ***MuLAn:*** *A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.02790) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mulan-dataset.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/mulan-dataset/v1.0)

* **[CVPR 2024]** ***Multi-dimensional Preferences:*** *Learning Multi-dimensional Human Preference for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.14705)

* **[CVPR 2024]** ***Dynamic Prompts:*** *Dynamic Prompt Optimizing for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.04095)

* **[CVPR 2024]** ***Reinforcement Diversification:*** *Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***HypercGAN:*** *Adversarial Text to Continuous Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Haydarov_Adversarial_Text_to_Continuous_Image_Generation_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kilichbek.github.io/webpage/hypercgan/)

* **[CVPR 2024]** ***EmoGen:*** *Emotional Image Content Generation with Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_EmoGen_Emotional_Image_Content_Generation_with_Text-to-Image_Diffusion_Models_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/JingyuanYY/EmoGen)

* **[ECCV 2024]** ***LaVi‚ÄëBridge:*** *Bridging Different Language Models and Generative Vision Models for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.07860) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/LaVi-Bridge/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/LaVi-Bridge)

* **[ECCV 2024]** ***DiffPNG:*** *Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.05352v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nini0919/DiffPNG)

* **[ECCV 2024]** ***SPRIGHT:*** *Getting it Right: Improving Spatial Consistency in Text-to-Image Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01197) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://spright-t2i.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SPRIGHT-T2I/SPRIGHT)

* **[ECCV 2024]** ***IndicTTI:*** *Navigating Text-to-Image Generative Bias across Indic Languages*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.00283v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://iab-rubric.org/resources/other-databases/indictti)

* **[ECCV 2024]** ***Safeguard T2I:*** *Safeguard Text-to-Image Diffusion Models with Human Feedback Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.21032)

* **[ECCV 2024]** ***Reality-and-Fantasy:*** *The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12579) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leo81005.github.io/Reality-and-Fantasy/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://leo81005.github.io/Reality-and-Fantasy/)

* **[ECCV 2024]** ***RECE:*** *Reliable and Efficient Concept Erasure of Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2407.12383v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CharlesGong12/RECE)

* **[ECCV 2024]** ***StyleTokenizer:*** *Defining Image Style by a Single Instance for Controlling Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2409.02543) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alipay/style-tokenizer)

* **[ECCV 2024]** ***PEA-Diffusion:*** *Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08492.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/PEA-Diffusion)

* **[ECCV 2024]** ***Skewed Relations T2I:*** *Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11936.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zdxdsw/skewed_relations_T2I)

* **[ECCV 2024]** ***Parrot:*** *Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05562.pdf)

* **[ECCV 2024]** ***MobileDiffusion:*** *Instant Text-to-Image Generation on Mobile Devices*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07923.pdf)

* **[ECCV 2024]** ***PixArt-Œ£:*** *Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04692) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/PixArt-sigma-project/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-sigma)

* **[ECCV 2024]** ***CogView3:*** *Finer and Faster Text-to-Image Generation via Relay Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05121) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogView)

* **[ICLR 2024]** ***Patched Diffusion Models:*** *Patched Denoising Diffusion Models For High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.01316.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mlpc-ucsd/patch-dm)

* **[ICLR 2024]** ***Relay Diffusion:*** *Unifying diffusion process across resolutions for image synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.03350.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/RelayDiffusion)

* **[ICLR 2024]** ***SDXL:*** *Improving Latent Diffusion Models for High-Resolution Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.01952.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Stability-AI/generative-models)

* **[ICLR 2024]** ***Compose and Conquer:*** *Diffusion-Based 3D Depth Aware Composable Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.09048.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tomtom1103/compose-and-conquer)

* **[ICLR 2024]** ***PixArt-Œ±:*** *Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.00426.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pixart-alpha.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PixArt-alpha/PixArt-alpha) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha)

* **[SIGGRAPH 2024]** ***RGB‚ÜîX:*** *Image Decomposition and Synthesis Using Material- and Lighting-aware Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://zheng95z.github.io/assets/files/sig24-rgbx.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zheng95z.github.io/publications/rgbx24)

* **[AAAI 2024]** ***Semantic-aware Augmentation:*** *Semantic-aware Data Augmentation for Text-to-image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07951.pdf)

* **[AAAI 2024]** ***Abstract Concepts:*** *Text-to-Image Generation for Abstract Concepts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28122)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2023]** ***GigaGAN:*** *Scaling Up GANs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mingukkang.github.io/GigaGAN/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/gigagan-pytorch)

* **[CVPR 2023]** ***ERNIE-ViLG¬†2.0:*** *Improving Text-to-Image Diffusion Model With Knowledge-Enhanced Mixture-of-Denoising-Experts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***Shifted¬†Diffusion:*** *Shifted Diffusion for Text-to-image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Shifted_Diffusion_for_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/drboog/Shifted_Diffusion)

* **[CVPR 2023]** ***GALIP:*** *Generative Adversarial CLIPs for Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tobran/GALIP)

* **[CVPR 2023]** ***Specialist¬†Diffusion:*** *Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Specialist-Diffusion)

* **[CVPR 2023]** ***Verifiable Evaluation:*** *Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf)

* **[CVPR 2023]** ***RIATIG:*** *Reliable and Imperceptible Adversarial Text-to-Image Generation with Natural Prompts*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WUSTL-CSPL/RIATIG)

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion)

* **[ICCV 2023]** ***DiffFit:*** *Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/DiffFit-pytorch)

* **[NeurIPS 2023]** ***ImageReward:*** *Learning and Evaluating Human Preferences for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=JVzeOYEx6d) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/ImageReward)

* **[NeurIPS 2023]** ***RAPHAEL:*** *Text-to-Image Generation via Large Mixture of Diffusion Paths*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.18295) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://raphael-painter.github.io/)

* **[NeurIPS 2023]** ***Linguistic Binding:*** *Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=AOKU4nRw1W) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RoyiRa/Linguistic-Binding-in-Diffusion-Models)

* **[NeurIPS 2023]** ***DenseDiffusion:*** *Dense Text-to-Image Generation with Attention Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/naver-ai/densediffusion)

* **[ICLR 2023]** ***Structured Diffusion Guidance:*** *Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=PUIqjT4rzq7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/weixi-feng/Structured-Diffusion-Guidance)

* **[ICML 2023]** ***StyleGAN-T:*** *Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/sauer23a/sauer23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sites.google.com/view/stylegan-t/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/autonomousvision/stylegan-t)

* **[ICML 2023]** ***Muse:*** *Text-To-Image Generation via Masked Generative Transformers*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/chang23b/chang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://muse-icml.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lucidrains/muse-maskgit-pytorch)

* **[ICML 2023]** ***UniDiffusers:*** *One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2303.06555) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/unidiffuser)

* **[ACM¬†MM 2023]** ***SUR-adapter:*** *Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.05189.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qrange-group/SUR-adapter)

* **[ACM¬†MM 2023]** ***ControlStyle:*** *Text-Driven Stylized Image Generation Using Diffusion Priors*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.05463.pdf)

* **[SIGGRAPH 2023]** ***Attend-and-Excite:*** *Attention-Based Semantic Guidance for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2301.13826.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuval-alaluf.github.io/Attend-and-Excite/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuval-alaluf/Attend-and-Excite) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AttendAndExcite/Attend-and-Excite)



</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="conditional">üïπÔ∏è Conditional Image Generation</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‚ÄëSemantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‚ÄëShot Structure‚ÄëAware Stylized Image Synthesis:*** *One‚ÄëShot Structure‚ÄëAware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‚Äëto‚ÄëImage Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‚Äëto‚ÄëFine Latent Diffusion for Pose‚ÄëGuided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304)

* **[CVPR 2024]** ***CAN:*** *Condition‚ÄëAware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‚ÄëPainter:*** *Training‚ÄëFree Layout Control for Text‚Äëto‚ÄëImage Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) 

* **[CVPR 2024]** ***MIGC:*** *Multi‚ÄëInstance Generation Controller for Text‚Äëto‚ÄëImage Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‚ÄëFree Spatial Control of Any Text‚Äëto‚ÄëImage Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‚ÄëTo‚ÄëImage Diffusion Models with Fine‚ÄëGrained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl)

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl)

* **[NeurIPS 2024]** ***Ctrl‚ÄëX:*** *Controlling Structure and Appearance for Text‚ÄëTo‚ÄëImage Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x)

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‚ÄëGuided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs)

* **[WACV 2024]** ***Layout Control with Cross‚ÄëAttention Guidance:*** *Training‚ÄëFree Layout Control with Cross‚ÄëAttention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‚ÄëSemantic Map Guided Diffusion Model for Free‚Äëform Layout‚Äëto‚Äëimage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‚Äëto‚ÄëImage Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2024]** ***PLACE:*** *Adaptive Layout‚ÄëSemantic Fusion for Semantic Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.01852.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cszy98/PLACE)

* **[CVPR 2024]** ***One‚ÄëShot Structure‚ÄëAware Stylized Image Synthesis:*** *One‚ÄëShot Structure‚ÄëAware Stylized Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.17275.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hansam95/OSASIS)

* **[CVPR 2024]** ***Attention Refocusing:*** *Grounded Text‚Äëto‚ÄëImage Synthesis with Attention Refocusing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.05427.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attention-refocusing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Attention-Refocusing/attention-refocusing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/attention-refocusing/Attention-refocusing)

* **[CVPR 2024]** ***CFLD:*** *Coarse‚Äëto‚ÄëFine Latent Diffusion for Pose‚ÄëGuided Person Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.18078.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YanzuoLu/CFLD)

* **[CVPR 2024]** ***DetDiffusion:*** *Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.13304) 

* **[CVPR 2024]** ***CAN:*** *Condition‚ÄëAware Neural Network for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.01143.pdf)

* **[CVPR 2024]** ***SceneDiffusion:*** *Move Anything with Layered Scene Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.07178)

* **[CVPR 2024]** ***Zero‚ÄëPainter:*** *Training‚ÄëFree Layout Control for Text‚Äëto‚ÄëImage Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Ohanyan_Zero-Painter_Training-Free_Layout_Control_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Zero-Painter)

* **[CVPR 2024]** ***MIGC:*** *Multi‚ÄëInstance Generation Controller for Text‚Äëto‚ÄëImage Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_MIGC_Multi-Instance_Generation_Controller_for_Text-to-Image_Synthesis_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://migcproject.github.io/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/limuloo/MIGC)

* **[CVPR 2024]** ***FreeControl:*** *Training‚ÄëFree Spatial Control of Any Text‚Äëto‚ÄëImage Diffusion Model with Any Condition*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Mo_FreeControl_Training-Free_Spatial_Control_of_Any_Text-to-Image_Diffusion_Model_with_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/freecontrol)

* **[ECCV 2024]** ***PreciseControl:*** *Enhancing Text‚ÄëTo‚ÄëImage Diffusion Models with Fine‚ÄëGrained Attribute Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.05083) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rishubhpar.github.io/PreciseControl.home/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rishubhpar/PreciseControl) 

* **[ECCV 2024]** ***AnyControl:*** *Create Your Artwork with Versatile Control on Text‚Äëto‚ÄëImage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01706.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/AnyControl) 

* **[NeurIPS 2024]** ***Ctrl‚ÄëX:*** *Controlling Structure and Appearance for Text‚ÄëTo‚ÄëImage Generation Without Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07540) [![Project Page](https://img.shields.io-badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://genforce.github.io/ctrl-x/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/genforce/ctrl-x) 

* **[ICLR 2024]** ***PCDMs:*** *Advancing Pose‚ÄëGuided Image Synthesis with Progressive Conditional Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.06313.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/muzishen/PCDMs) 

* **[WACV 2024]** ***Layout Control with Cross‚ÄëAttention Guidance:*** *Training‚ÄëFree Layout Control with Cross‚ÄëAttention Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2024/papers/Chen_Training-Free_Layout_Control_With_Cross-Attention_Guidance_WACV_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://silent-chen.github.io/layout-guidance/) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/silent-chen/layout-guidance) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/silentchen/layout-guidance)

* **[AAAI 2024]** ***SSMG:*** *Spatial‚ÄëSemantic Map Guided Diffusion Model for Free‚Äëform Layout‚Äëto‚Äëimage Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.10156.pdf)

* **[AAAI 2024]** ***Attention Map Control:*** *Compositional Text‚Äëto‚ÄëImage Synthesis with Attention Map Control of Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.13921.pdf) [![GitHub](https://img.shields.io-badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/OPPO-Mente-Lab/attention-mask-control)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2023]** ***GLIGEN:*** *Open-Set Grounded Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gligen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/gligen/GLIGEN) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/gligen/demo)

* **[CVPR 2022]** ***Autoregressive Image Generation:*** *Using Residual Quantization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/kakaobrain/rq-vae-transformer)

* **[CVPR 2023]** ***SpaText:*** *Spatio-Textual Representation for Controllable Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/spatext/)

* **[CVPR 2022]** ***Text to Image Generation with Semantic-Spatial Aware GAN:*** *Text to Image Generation with Semantic-Spatial Aware GAN*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf)

* **[CVPR 2023]** ***ReCo:*** *Region-Controlled Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/ReCo)

* **[CVPR 2023]** ***LayoutDiffusion:*** *Controllable Diffusion Model for Layout-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/LayoutDiffusion)

* **[ICLR 2023]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grenoble-zhang.github.io/Ctrl-U-Page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/grenoble-zhang/Ctrl-U)

* **[ICCV 2023]** ***ControlNet:*** *Adding Conditional Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/ControlNet) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/lllyasviel/ControlNet)

* **[ICCV 2023]** ***SceneGenie:*** *Scene Graph Guided Diffusion Models for Image Synthesis*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023W/SG2RL/papers/Farshad_SceneGenie_Scene_Graph_Guided_Diffusion_Models_for_Image_Synthesis_ICCVW_2023_paper.pdf)

* **[ICCV 2023]** ***ZestGuide:*** *Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf)

* **[ICML 2023]** ***Composer:*** *Creative and Controllable Image Synthesis with Composable Conditions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/huang23b/huang23b.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/composer-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/composer)

* **[ICML 2023]** ***MultiDiffusion:*** *Fusing Diffusion Paths for Controlled Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.mlr.press/v202/bar-tal23a/bar-tal23a.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://multidiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/omerbt/MultiDiffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/weizmannscience/MultiDiffusion)

* **[SIGGRAPH 2023]** ***Sketch-Guided Text-to-Image Diffusion Models:*** *Sketch-Guided Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3588432.3591560) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sketch-guided-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ogkalu2/Sketch-Guided-Stable-Diffusion)

* **[NeurIPS 2023]** ***Uni-ControlNet:*** *All-in-One Control to Text-to-Image Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16322.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shihaozhaozsh.github.io/unicontrolnet/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ShihaoZhaoZSH/Uni-ControlNet)

* **[NeurIPS 2023]** ***Prompt Diffusion:*** *In-Context Learning Unlocked for Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=6BZS2EAkns) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhendong-wang.github.io/prompt-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Zhendong-Wang/Prompt-Diffusion)

* **[WACV 2023]** ***More Control for Free!:*** *Image Synthesis with Semantic Diffusion Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf)

* **[ACM MM 2023]** ***LayoutLLM-T2I:*** *Eliciting Layout Guidance from LLM for Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2308.05095.pdf)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="personalized">üé® Personalized Image Generation</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2025]** ***SerialGen:*** *Personalized Image Generation by First Standardization Then Personalization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.01485) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://serialgen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/)

* **[CVPR 2025]** ***PatchDPO:*** *Patch-level DPO for Finetuning-free Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03177) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/hqhQAQ/PatchDPO) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hqhQAQ/PatchDPO) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hqhQAQ/PatchDPO)

* **[CVPR 2025]** ***DreamCache:*** *Finetuning-Free Lightweight Personalized Image Generation via Feature Caching*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.17786) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://emanuele97x.github.io/DreamCache) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Emanuele97x/DreamCache)

* **[NeurIPS 2025]** ***MS-Diffusion:*** *Multi-Subject Zero-shot Image Personalization with Layout Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.07209) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ms-diffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MS-Diffusion/MS-Diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/doge1516/MS-Diffusion)

* **[NeurIPS 2025]** ***ClassDiffusion:*** *More Aligned Personalization Tuning with Explicit Class Guidance*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=iTm4H6N4aG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://classdiffusion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Rbrq03/ClassDiffusion)

* **[NeurIPS 2025]** ***DreamBench++:*** *A Human-Aligned Benchmark for Personalized Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2406.16855) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambenchplus.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuangpeng/dreambench_plus) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/yuangpeng/dreambench_plus)

* **[NeurIPS 2025]** ***TweedieMix:*** *Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.05591) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/KwonGihyun/TweedieMix) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwonGihyun/TweedieMix)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2024]** ***Cross¬†Initialization:*** *Personalized¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15905.pdf)

* **[CVPR 2024]** ***When¬†StyleGAN¬†Meets¬†Stable¬†Diffusion:*** *a¬†W+¬†Adapter¬†for¬†Personalized¬†Image¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.17461.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://csxmli2016.github.io/projects/w-plus-adapter/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csxmli2016/w-plus-adapter)

* **[CVPR 2024]** ***Style¬†Aligned:*** *Image¬†Generation¬†via¬†Shared¬†Attention*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.02133.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://style-aligned-gen.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/style-aligned)

* **[CVPR 2024]** ***InstantBooth:*** *Personalized¬†Text‚Äëto‚ÄëImage¬†Generation¬†without¬†Test‚ÄëTime¬†Finetuning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2304.03411.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jshi31.github.io/InstantBooth/)

* **[CVPR 2024]** ***High¬†Fidelity:*** *Person‚Äëcentric¬†Subject‚Äëto‚ÄëImage¬†Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.10329.pdf)

* **[CVPR 2024]** ***RealCustom:*** *Narrowing¬†Real¬†Text¬†Word¬†for¬†Real‚ÄëTime¬†Open‚ÄëDomain¬†Text‚Äëto‚ÄëImage¬†Customization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.00483.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://corleone-huang.github.io/realcustom/) [![ü§ó¬†Hugging¬†Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/bytedance-research/RealCustom)

* **[CVPR 2024]** ***DisenDiff:*** *Attention¬†Calibration¬†for¬†Disentangled¬†Text‚Äëto‚ÄëImage¬†Personalization*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.18551) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Monalissaa/DisenDiff)

* **[CVPR 2024]** ***FreeCustom:*** *Tuning‚ÄëFree¬†Customized¬†Image¬†Generation¬†for¬†Multi‚ÄëConcept¬†Composition*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.13870v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/FreeCustom/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/FreeCustom)

* **[CVPR 2024]** ***Personalized¬†Residuals:*** *for¬†Concept‚ÄëDriven¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.12978)

* **[CVPR 2024]** ***Subject‚ÄëAgnostic¬†Guidance:*** *Improving¬†Subject‚ÄëDriven¬†Image¬†Synthesis*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2405.01356)

* **[CVPR 2024]** ***JeDi:*** *Joint‚ÄëImage¬†Diffusion¬†Models¬†for¬†Finetuning‚ÄëFree¬†Personalized¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zeng_JeDi_Joint-Image_Diffusion_Models_for_Finetuning-Free_Personalized_Text-to-Image_Generation_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***Influence¬†Watermarks:*** *Countering¬†Personalized¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.pdf)

* **[CVPR 2024]** ***PIA:*** *Your¬†Personalized¬†Image¬†Animator¬†via¬†Plug‚Äëand‚ÄëPlay¬†Modules¬†in¬†Text‚Äëto‚ÄëImage¬†Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_PIA_Your_Personalized_Image_Animator_via_Plug-and-Play_Modules_in_Text-to-Image_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pi-animator.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/open-mmlab/PIA)

* **[CVPR 2024]** ***SSR‚ÄëEncoder:*** *Encoding¬†Selective¬†Subject¬†Representation¬†for¬†Subject‚ÄëDriven¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_SSR-Encoder_Encoding_Selective_Subject_Representation_for_Subject-Driven_Generation_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Xiaojiu-z/SSR_Encoder)

* **[ECCV 2024]** ***Be¬†Yourself:*** *Bounded¬†Attention¬†for¬†Multi‚ÄëSubject¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.16990) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omer11a.github.io/bounded-attention/)

* **[ECCV 2024]** ***Powerful¬†and¬†Flexible:*** *Personalized¬†Text‚Äëto‚ÄëImage¬†Generation¬†via¬†Reinforcement¬†Learning*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.06642v1) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wfanyue/DPG-T2I-Personalization)

* **[ECCV 2024]** ***TIGC:*** *Tuning‚ÄëFree¬†Image¬†Customization¬†with¬†Image¬†and¬†Text¬†Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.12658) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/TIGIC/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zrealli/TIGIC)

* **[ECCV 2024]** ***MasterWeaver:*** *Taming¬†Editability¬†and¬†Face¬†Identity¬†for¬†Personalized¬†Text‚Äëto‚ÄëImage¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06786.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://masterweaver.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/MasterWeaver)

* **[NeurIPS 2024]** ***RectifID:*** *Personalizing¬†Rectified¬†Flow¬†with¬†Anchored¬†Classifier¬†Guidance*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/afa58a5b6adc0845e0fd632132a64c39-Paper-Conference.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/feifeiobama/RectifID)

* **[NeurIPS 2024]** ***AttnDreamBooth:*** *Towards¬†Text‚ÄëAligned¬†Personalized¬†Image¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/465a13a95741fab2e912f98adb07df1d-Paper-Conference.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://attndreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lyuPang/AttnDreamBooth)

* **[AAAI 2024]** ***Decoupled¬†Textual¬†Embeddings:*** *for¬†Customized¬†Image¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.11826.pdf)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2023]** ***Custom Diffusion:*** *Multi-Concept Customization of Text-to-Image Diffusion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.cs.cmu.edu/~custom-diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/adobe-research/custom-diffusion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/custom-diffusion-library/cat)

* **[CVPR 2023]** ***DreamBooth:*** *Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dreambooth.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/dreambooth) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/DreamBooth-AltDiffusion)

* **[ICCV 2023]** ***ELITE:*** *Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/csyxwei/ELITE) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/csyxwei/ELITE) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/ELITE-library/ELITE)

* **[ICLR 2023]** ***Textual Inversion:*** *An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/pdf?id=NAQvF08TcyG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://textual-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/rinongal/textual_inversion) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/Clyuue/textual_inversion_cat)

* **[SIGGRAPH Asia 2023]** ***Break-A-Scene:*** *Extracting Multiple Concepts from a Single Image*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.16311.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omriavrahami.com/break-a-scene) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/break-a-scene)

* **[SIGGRAPH 2023]** ***Encoder‚ÄëBased Domain Tuning:*** *Encoder‚ÄëBased Domain Tuning for Fast Personalization of Text‚Äëto‚ÄëImage Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2302.12228.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tuning-encoder.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mkshing/e4t-diffusion)

* **[SIGGRAPH 2023]** ***LayerDiffusion:*** *Layered Controlled Image Editing with Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://dl.acm.org/doi/pdf/10.1145/3610543.3626172) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zrealli.github.io/layerdiffusion/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lllyasviel/LayerDiffuse) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/LayerDiffusion/layerdiffusion-v1)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="editing">‚úÇÔ∏è Image Editing</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR¬†2025]** ***FDS:*** *Frequency‚ÄëAware¬†Denoising¬†Score¬†for¬†Text‚ÄëGuided¬†Latent¬†Diffusion¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.19191)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ivrl.github.io/fds-webpage/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/IVRL/FDS)


* **[CVPR¬†2025]** *Reference‚ÄëBased¬†3D‚ÄëAware¬†Image¬†Editing¬†with¬†Triplanes*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.03632)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://three-bee.github.io/triplane_edit/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/three-bee/triplane_edit)


* **[CVPR¬†2025]** ***MoEdit:*** *On¬†Learning¬†Quantity¬†Perception¬†for¬†Multi‚Äëobject¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2503.10112)


* **[ICLR¬†2025]** *Lightning‚ÄëFast¬†Image¬†Inversion¬†and¬†Editing¬†for¬†Text‚Äëto‚ÄëImage¬†Diffusion¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=t9l63huPRt)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://barakmam.github.io/rnri.github.io/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvirsamuel/NewtonRaphsonInversion)


* **[ICLR¬†2025]** *Multi‚ÄëReward¬†as¬†Condition¬†for¬†Instruction‚Äëbased¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=9RFocgIccP)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/Multi-Reward-Editing)


* **[ICLR¬†2025]** ***HQ‚ÄëEdit:*** *A¬†High‚ÄëQuality¬†Dataset¬†for¬†Instruction‚Äëbased¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=mZptYYttFj)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thefllood.github.io/HQEdit_web/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSC-VLAA/HQ-Edit)¬†[![Hugging¬†Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit)


* **[ICLR¬†2025]** ***CLIPDrag:*** *Combining¬†Text‚Äëbased¬†and¬†Drag‚Äëbased¬†Instructions¬†for¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=2HjRezQ1nj)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HKUST-LongGroup/CLIPDrag)


* **[ICLR¬†2025]** *Semantic¬†Image¬†Inversion¬†and¬†Editing¬†using¬†Rectified¬†Stochastic¬†Differential¬†Equations*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hu0FSOSEyS)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://rf-inversion.github.io/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LituRout/RF-Inversion)


* **[ICLR¬†2025]** ***PostEdit:*** *Posterior¬†Sampling¬†for¬†Efficient¬†Zero‚ÄëShot¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=J8YWCBPgx7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TFNTF/PostEdit)


* **[ICLR¬†2025]** ***OmniEdit:*** *Building¬†Image¬†Editing¬†Generalist¬†Models¬†Through¬†Specialist¬†Supervision*<br>
[![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=Hlm0cga0sv)¬†[![Project¬†Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tiger-ai-lab.github.io/OmniEdit/)¬†[![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TIGER-AI-Lab/OmniEdit)¬†[![Hugging¬†Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/TIGER-Lab/OmniEdit-Filtered-1.2M)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR¬†2024]** ***InfEdit:*** *Inversion‚ÄëFree Image Editing with Natural Language*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.04965.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sled-group.github.io/InfEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sled-group/InfEdit)


* **[CVPR¬†2024]** ***CrossSelfAttention:*** *Towards Understanding Cross¬†and¬†Self‚ÄëAttention in¬†Stable¬†Diffusion for Text‚ÄëGuided Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.03431.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing)


* **[CVPR¬†2024]** ***DAC:*** *Doubly¬†Abductive Counterfactual Inference for Text‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.02981.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/xuesong39/DAC)


* **[CVPR¬†2024]** ***FoI:*** *Focus¬†on¬†Your¬†Instruction:¬†Fine‚Äëgrained and Multi‚Äëinstruction Image¬†Editing by Attention¬†Modulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.10113.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/guoqincode/Focus-on-Your-Instruction)


* **[CVPR¬†2024]** ***CDS:*** *Contrastive¬†Denoising¬†Score for Text‚Äëguided Latent Diffusion Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.18608.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hyelinnam.github.io/CDS/)


* **[CVPR¬†2024]** ***DragDiffusion:*** *Harnessing Diffusion¬†Models for Interactive Point‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2306.14435.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yujun-shi.github.io/projects/dragdiffusion.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Yujun-Shi/DragDiffusion)


* **[CVPR¬†2024]** ***DiffEditor:*** *Boosting Accuracy and Flexibility on Diffusion‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.02583.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MC-E/DragonDiffusion)


* **[CVPR¬†2024]** ***FreeDrag:*** *Feature¬†Dragging for Reliable Point‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2307.04684.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/LPengYang/FreeDrag)


* **[CVPR¬†2024]** ***Learnable¬†Regions:*** *Text‚ÄëDriven Image¬†Editing via¬†Learnable¬†Regions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16432.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuanze-lin.me/LearnableRegions_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yuanze-lin/Learnable_Regions)


* **[CVPR¬†2024]** ***LEDITS++:*** *Limitless Image¬†Editing using Text‚Äëto‚ÄëImage¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.16711.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://leditsplusplus-project.static.hf.space/index.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://huggingface.co/spaces/editing-images/leditsplusplus/tree/main) [![Hugging¬†Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/editing-images/leditsplusplus)


* **[CVPR¬†2024]** ***SmartEdit:*** *Exploring¬†Complex Instruction‚Äëbased Image¬†Editing with Large¬†Language¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.06739.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yuzhou914.github.io/SmartEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/SmartEdit)


* **[CVPR¬†2024]** ***Edit¬†One¬†for¬†All:*** *Interactive¬†Batch Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.10219.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://thaoshibe.github.io/edit-one-for-all/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thaoshibe/edit-one-for-all)


* **[CVPR¬†2024]** ***DiffMorpher:*** *Unleashing the Capability of Diffusion¬†Models for Image¬†Morphing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.07409.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kevin-thu.github.io/DiffMorpher_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Kevin-thu/DiffMorpher)


* **[CVPR¬†2024]** ***TiNO‚ÄëEdit:*** *Timestep and Noise¬†Optimization for Robust Diffusion‚ÄëBased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2404.11120.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SherryXTChen/TiNO-Edit)


* **[CVPR¬†2024]** ***Person¬†in¬†Place:*** *Generating Associative Skeleton‚ÄëGuidance Maps for Human‚ÄëObject¬†Interaction Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Person_in_Place_Generating_Associative_Skeleton-Guidance_Maps_for_Human-Object_Interaction_CVPR_2024_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yangchanghee.github.io/Person-in-Place_page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/YangChangHee/CVPR2024_Person-In-Place_RELEASE)


* **[CVPR¬†2024]** ***Referring¬†Image¬†Editing:*** *Object‚Äëlevel Image¬†Editing via Referring¬†Expressions*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.pdf)


* **[CVPR¬†2024]** ***Prompt¬†Augmentation:*** *Prompt¬†Augmentation for Self‚Äësupervised Text‚Äëguided Image¬†Manipulation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bodur_Prompt_Augmentation_for_Self-supervised_Text-guided_Image_Manipulation_CVPR_2024_paper.pdf)


* **[CVPR¬†2024]** ***StyleFeatureEditor:*** *The Devil is in the Details¬†‚Äî StyleFeatureEditor for Detail‚ÄëRich StyleGAN¬†Inversion and High¬†Quality Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Bobkov_The_Devil_is_in_the_Details_StyleFeatureEditor_for_Detail-Rich_StyleGAN_CVPR_2024_paper.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/StyleFeatureEditor)


* **[ECCV¬†2024]** ***RegionDrag:*** *Fast Region‚ÄëBased Image¬†Editing with Diffusion¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2407.18247v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://visual-ai.github.io/regiondrag/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Visual-AI/RegionDrag)


* **[ECCV¬†2024]** ***TurboEdit:*** *Instant Text‚ÄëBased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.08332v1.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://betterze.github.io/TurboEdit/)


* **[ECCV¬†2024]** ***InstructGIE:*** *Towards¬†Generalizable Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.05018.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cr8br0ze.github.io/InstructGIE)


* **[ECCV¬†2024]** ***StableDrag:*** *Stable¬†Dragging for Point‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2403.04437.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://stabledrag.github.io/)


* **[ECCV¬†2024]** ***Eta¬†Inversion:*** *Designing an Optimal Eta¬†Function for Diffusion‚Äëbased Real Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02157.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/furiosa-ai/eta-inversion)


* **[ECCV¬†2024]** ***SwapAnything:*** *Enabling¬†Arbitrary Object¬†Swapping in Personalized Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04768.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://swap-anything.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/eric-ai-lab/swap-anything)


* **[ECCV¬†2024]** ***Guide‚Äëand‚ÄëRescale:*** *Self‚ÄëGuidance¬†Mechanism for¬†Effective Tuning‚ÄëFree Real Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08987.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AIRI-Institute/Guide-and-Rescale)


* **[ECCV¬†2024]** ***FreeDiff:*** *Progressive Frequency¬†Truncation for Image¬†Editing with Diffusion¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00759.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Thermal-Dynamics/FreeDiff)


* **[ECCV¬†2024]** ***Lazy¬†Diffusion¬†Transformer:*** *Lazy Diffusion¬†Transformer for Interactive Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03436.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lazydiffusion.github.io/)


* **[ECCV¬†2024]** ***ByteEdit:*** *Boost, Comply and Accelerate Generative Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00359.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://byte-edit.github.io/)


* **[ICLR¬†2024]** ***MGIE:*** *Guiding Instruction‚Äëbased Image¬†Editing via¬†Multimodal¬†Large¬†Language¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2309.17102.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://mllm-ie.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/apple/ml-mgie)


* **[ICLR¬†2024]** ***SDE‚ÄëDrag:*** *The Blessing of Randomness¬†‚Äî SDE¬†Beats¬†ODE in General¬†Diffusion‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2311.01410.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ml-gsai.github.io/SDE-Drag-demo/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ML-GSAI/SDE-Drag)


* **[ICLR¬†2024]** ***Motion¬†Guidance:*** *Diffusion‚ÄëBased Image¬†Editing with Differentiable Motion¬†Estimators*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2401.18085.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dangeng.github.io/motion_guidance/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dangeng/motion_guidance)


* **[ICLR¬†2024]** ***OIR:*** *Object‚ÄëAware¬†Inversion and¬†Reassembly for Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2310.12149.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://aim-uofa.github.io/OIR-Diffusion/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aim-uofa/OIR)


* **[ICLR¬†2024]** ***Noise¬†Map¬†Guidance:*** *Inversion¬†with¬†Spatial Context for¬†Real¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2402.04625.pdf)


* **[AAAI¬†2024]** ***TIC:*** *Tuning‚ÄëFree Inversion‚ÄëEnhanced¬†Control for Consistent¬†Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.14611.pdf)


* **[AAAI¬†2024]** ***BARET:*** *Balanced Attention¬†based Real Image¬†Editing driven by Target‚Äëtext¬†Inversion*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.05482.pdf)


* **[AAAI¬†2024]** ***CacheEdit:*** *Accelerating Text‚Äëto‚ÄëImage¬†Editing via Cache‚ÄëEnabled Sparse¬†Diffusion¬†Inference*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2305.17423.pdf)


* **[AAAI¬†2024]** ***High‚ÄëFidelity¬†Editing:*** *High‚ÄëFidelity Diffusion‚Äëbased Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.15707.pdf)


* **[AAAI¬†2024]** ***AdapEdit:*** *Spatio‚ÄëTemporal Guided Adaptive¬†Editing Algorithm for Text‚ÄëBased Continuity‚ÄëSensitive Image¬†Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2312.08019.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AnonymousPony/adap-edit)


* **[AAAI¬†2024]** ***TexFit:*** *Text‚ÄëDriven Fashion Image¬†Editing with Diffusion¬†Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://ojs.aaai.org/index.php/AAAI/article/view/28885)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[CVPR 2023]** ***Diffusion Disentanglement:*** *Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wuqiuche.github.io/DiffusionDisentanglement-project-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement)

* **[CVPR 2023]** ***SINE:*** *SINgle Image Editing with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zhang-zx.github.io/SINE/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhang-zx/SINE)

* **[CVPR 2023]** ***Imagic:*** *Text-Based Real Image Editing with Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://imagic-editing.github.io/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/fffiloni/imagic-stable-diffusion)

* **[CVPR 2023]** ***InstructPix2Pix:*** *Learning to Follow Image Editing Instructions*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.timothybrooks.com/instruct-pix2pix/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/timothybrooks/instruct-pix2pix) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/timbrooks/instruct-pix2pix)

* **[CVPR 2023]** ***Null-text Inversion:*** *Null-text Inversion for Editing Real Images using Guided Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://null-text-inversion.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/google/prompt-to-prompt)

* **[ICCV 2023]** ***MasaCtrl:*** *Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ljzycmd.github.io/projects/MasaCtrl/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/TencentARC/MasaCtrl)

* **[ICCV 2023]** ***Local Prompt Mixing:*** *Localizing Object-level Shape Variations with Text-to-Image Diffusion Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://orpatashnik.github.io/local-prompt-mixing/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/orpatashnik/local-prompt-mixing) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/orpatashnik/local-prompt-mixing)

* **[ICLR 2022]** ***SDEdit:*** *Guided Image Synthesis and Editing with Stochastic Differential Equations*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2108.01073.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sde-image-editing.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ermongroup/SDEdit)


</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)



---

## <span id="datasets">üóÇÔ∏è Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **MS COCO** | 2014 | Text, Image | Text-to-Image Generation, Image Captioning | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/1405.0312) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://cocodataset.org/#home) |
| **Oxford-120 Flowers**| 2008 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/publications/2008/Nilsback08/nilsback08.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/) |
| **CUB-200-2011** | 2011 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://resolver.caltech.edu/CaltechCSTR:2010.001) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](http://www.vision.caltech.edu/datasets/cub_200_2011/) |
| **LAION-5B** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.08402) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://laion.ai/blog/laion-5b/) |
| **DiffusionDB** | 2022 | Text, Image | Text-to-Image Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2210.14896) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://poloclub.github.io/diffusiondb/) |

[<small>‚áß Back to ToC</small>](#contents)

---

## <span id="about-us">üéì About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Text-to-Image Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

‚û°Ô∏è **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI) ÊòØ‰∏ÄÂÆ∂Áî±È°∂Â∞ñÁ†îÁ©∂ËÄÖÁªÑÊàêÔºåËá¥Âäõ‰∫é‰∏∫ÂÖ®ÁêÉÈ´òÊ†°Â≠¶ÁîüÊèê‰æõÈ´òË¥®Èáè1V1ÁßëÁ†îËæÖÂØºÁöÑ‰∏ì‰∏öÊú∫ÊûÑ„ÄÇÊàë‰ª¨ÁöÑ‰ΩøÂëΩÊòØÂ∏ÆÂä©Â≠¶ÁîüÂüπÂÖªÂá∫Ëâ≤ÂçìË∂äÁöÑÁßëÁ†îÊäÄËÉΩÔºåÂú®È°∂Á∫ß‰ºöËÆÆÂíåÊúüÂàä‰∏äÂèëË°®Ëá™Â∑±ÁöÑÊàêÊûú„ÄÇ

Áª¥Êä§‰∏Ä‰∏™GitHubË∞ÉÁ†î‰ªìÂ∫ìÈúÄË¶ÅÂ∑®Â§ßÁöÑÁ≤æÂäõÔºåÊ≠£Â¶ÇÂÆåÊàê‰∏ÄÁØáÈ´òË¥®ÈáèÁöÑËÆ∫Êñá‰∏ÄÊ†∑ÔºåÁ¶ª‰∏çÂºÄ‰∏ìÊ≥®ÁöÑÊäïÂÖ•Âíå‰∏ì‰∏öÁöÑÊåáÂØº„ÄÇÂ¶ÇÊûúÊÇ®Â∏åÊúõÂú®Ëá™Â∑±ÁöÑÁ†îÁ©∂È°πÁõÆ‰∏≠ÔºåËé∑ÂæóÊù•Ëá™È°∂Â∞ñÂ≠¶ËÄÖÁöÑ‰∏ÄÂØπ‰∏ÄÊîØÊåÅÔºåÊàë‰ª¨ËØöÈÇÄÊÇ®‰∏éÊàë‰ª¨ÂèñÂæóËÅîÁ≥ª„ÄÇ

‚û°Ô∏è **Ê¨¢ËøéÈÄöËøá [ÂæÆ‰ø°](assets/wechat.jpg) Êàñ [ÈÇÆ‰ª∂](mailto:your.email@example.com) ËÅîÁ≥ªÊàë‰ª¨ÔºåÂºÄÂêØÊÇ®ÁöÑÁßëÁ†î‰πãÊóÖ„ÄÇ**


[<small>‚áß Back to ToC</small>](#contents)

---



## <span id="contributing">ü§ù Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
